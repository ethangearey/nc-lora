{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkAD/UHdkJ2/j4ED8AGu2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3: LoRA and Frozen Backbone Probes\n",
        "- Take a pretrained classifier (e.g., trained ResNet).\n",
        "- Fine-tune only the final layer using LoRA, varying the rank (e.g., 1–16).\n",
        "- Evaluate:\n",
        " - Whether NC geometry persists or evolves under low-rank adaptation.\n",
        " - If LoRA directions align with NC class mean directions (cosine similarity, projection overlap)."
      ],
      "metadata": {
        "id": "3V6JtNCJOzDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO\n",
        "1. rank=0 (compare against no LoRA fine tuning) ((to what purpose?))\n"
      ],
      "metadata": {
        "id": "ZiJGkQOuVcut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620YzzMPOvDJ",
        "outputId": "13bf8673-e1bf-4e40-ee98-19e0c0857039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mount Drive for checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Reuse existing imports and configurations from Experiment 1+2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "debug = True\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "lr = 0.1\n",
        "batch_size          = 128\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4 # too high?\n",
        "\n",
        "epochs  = 20\n",
        "C       = 10\n",
        "ranks   = [0,1,2,4,8,16]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [torch.zeros(model.fc.in_features, device=device) for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Cov']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "                elif str(criterion_summed) == 'MSELoss()':\n",
        "                  loss += criterion_summed(output, F.one_hot(target, num_classes=num_classes).float()).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Cov':\n",
        "                    # update within-class cov\n",
        "\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "                graphs.mean = mean\n",
        "            loss /= sum(N)\n",
        "        elif computation == 'Cov':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    if hasattr(classifier, 'original'):\n",
        "        W = classifier.original.weight  # LoRA case\n",
        "    else:\n",
        "        W = classifier.weight # rank=0 baseline\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.cpu().double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.svd_lowrank(Sb, q=C-1)\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))\n",
        "\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    self.mean         = []\n",
        "    self.feature_rank = []\n"
      ],
      "metadata": {
        "id": "i4R4RmRuPfmY"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_lora_model(pretrained_model, rank=4):\n",
        "    config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=16,  # Scaling factor (alpha = 16 is common)\n",
        "        target_modules=[\"fc\"],  # Apply LoRA to final layer\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "    model = get_peft_model(pretrained_model, config)\n",
        "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def compute_alignment_metrics(model, class_means):\n",
        "    \"\"\"Calculate alignment between LoRA directions and NC class means.\"\"\"\n",
        "    lora_A = model.base_model.model.fc.lora_A['default'].weight  # [rank, in_dim]\n",
        "    lora_B = model.base_model.model.fc.lora_B['default'].weight  # [out_dim, rank]\n",
        "    W_lora = lora_B @ lora_A  # Combined LoRA direction [out_dim, in_dim]\n",
        "\n",
        "    # Project class means onto LoRA subspace\n",
        "    M = class_means.T.cpu().numpy()  # [in_dim, C]\n",
        "    U, _, _ = randomized_svd(W_lora.detach().cpu().numpy(), n_components=2)\n",
        "\n",
        "    # Cosine similarity between LoRA directions and class means\n",
        "    cos_sims = []\n",
        "    for c in range(M.shape[1]):\n",
        "        v = M[:, c]\n",
        "        for i in range(U.shape[1]):\n",
        "            u = U[:, i]\n",
        "            cos_sim = np.dot(u, v) / (np.linalg.norm(u)*np.linalg.norm(v)+1e-8)\n",
        "            cos_sims.append(cos_sim)\n",
        "\n",
        "    # Subspace projection score\n",
        "    proj = U.T @ M\n",
        "    proj_score = np.linalg.norm(proj)**2 / np.linalg.norm(M)**2\n",
        "\n",
        "    return np.mean(cos_sims), proj_score\n"
      ],
      "metadata": {
        "id": "2SHzUkNIQq32"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lora(model, criterion, device, train_loader, optimizer, epoch, rank):\n",
        "    model.train()\n",
        "    # model.fc.original.requires_grad_(False)  # redundant as peft freezes weights automatically\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'LoRA Rank {rank} Epoch {epoch}')\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'Loss': loss.item()})\n",
        "        pbar.update(1)\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "            break\n",
        "    pbar.close()\n",
        "\n",
        "def run_experiment_3(pretrained_model, ranks=ranks):\n",
        "    results = []\n",
        "    original_class_means = None  # To store initial NC geometry\n",
        "\n",
        "    for rank in ranks:\n",
        "        # Load fresh pretrained model for each run\n",
        "        model = prepare_lora_model(pretrained_model, rank=rank)\n",
        "        optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)  # Only optimize LoRA params\n",
        "\n",
        "        # Initial NC state\n",
        "        graphs = Graphs()  # Reuse Graph class from Experiment 1\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch=0)\n",
        "        original_class_means = torch.stack(graphs.mean).T if original_class_means is None else original_class_means # todo fix?\n",
        "\n",
        "        # Fine-tune for 20 epochs\n",
        "        for epoch in range(1, 21):\n",
        "            train_lora(model, criterion, device, train_loader, optimizer, epoch, rank)\n",
        "            analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "            # Compute alignment metrics\n",
        "            current_class_means = torch.stack(graphs.mean).T\n",
        "            cos_sim, proj_score = compute_alignment_metrics(model, current_class_means)\n",
        "\n",
        "            results.append({\n",
        "                'rank': rank,\n",
        "                'epoch': epoch,\n",
        "                'NC1': graphs.Sw_invSb[-1],\n",
        "                # 'feature_rank': graphs.feature_rank[-1],\n",
        "                'cos_sim': cos_sim,\n",
        "                'proj_score': proj_score\n",
        "                # 'test_acc': graphs.accuracy[-1]  # Assuming test loader available\n",
        "            })\n",
        "\n",
        "        # Save results per rank\n",
        "        pd.DataFrame(results).to_csv(f'lora_rank_{rank}_results.csv', index=False)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "4jZ0_e2fQ5bJ"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================== EXECUTION ======================\n",
        "# Load pretrained model from Experiment 1\n",
        "pretrained_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # Match Experiment 1\n",
        "pretrained_model.maxpool = nn.Identity() # CIFAR-10\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = pretrained_model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "# load from Experiment 1+2\n",
        "checkpoint = torch.load('/content/drive/MyDrive/checkpoint.pth', weights_only=False)\n",
        "pretrained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "pretrained_model.to(device)\n",
        "\n",
        "# dataset, optimizer setup from Experiment 1+2\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD(pretrained_model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "# Run Experiment 3\n",
        "results = run_experiment_3(pretrained_model, ranks=[1,2,4,8,16])\n",
        "\n",
        "# Visualization\n",
        "df = pd.DataFrame(results)\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['cos_sim'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Cosine Similarity')\n",
        "plt.title('LoRA Direction Alignment with Class Means')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['NC1'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NC1 (Tr{Sw Sb^-1})')\n",
        "plt.title('Neural Collapse Persistence During LoRA Tuning')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yKTl8zXRCdf",
        "outputId": "b7815f20-0347-408e-d959-f54111290c14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:13<21:37,  3.52s/it]\n",
            "Analysis Cov\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:13<21:35,  3.51s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 1/390 [00:03<20:12,  3.12s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 1/390 [00:06<20:12,  3.12s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 2/390 [00:06<20:12,  3.12s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 2/390 [00:09<20:12,  3.12s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 3/390 [00:09<20:05,  3.12s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 3/390 [00:13<20:05,  3.12s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 4/390 [00:13<23:37,  3.67s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 4/390 [00:17<23:37,  3.67s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|▏         | 5/390 [00:17<22:23,  3.49s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|▏         | 5/390 [00:20<22:23,  3.49s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 6/390 [00:20<21:35,  3.37s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 6/390 [00:23<21:35,  3.37s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 7/390 [00:23<21:02,  3.30s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 7/390 [00:28<21:02,  3.30s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 8/390 [00:28<23:52,  3.75s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 8/390 [00:31<23:52,  3.75s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 9/390 [00:31<23:09,  3.65s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 9/390 [00:34<23:09,  3.65s/it, Loss=2.05]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 10/390 [00:34<22:27,  3.55s/it, Loss=2.05]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 10/390 [00:38<22:27,  3.55s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 11/390 [00:38<21:57,  3.48s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 11/390 [00:42<21:57,  3.48s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 12/390 [00:42<24:08,  3.83s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 12/390 [00:45<24:08,  3.83s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 13/390 [00:45<22:47,  3.63s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 13/390 [00:49<22:47,  3.63s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▎         | 14/390 [00:49<21:54,  3.50s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▎         | 14/390 [00:52<21:54,  3.50s/it, Loss=2.12]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 15/390 [00:52<21:14,  3.40s/it, Loss=2.12]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 15/390 [00:56<21:14,  3.40s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 16/390 [00:56<23:35,  3.79s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 16/390 [01:00<23:35,  3.79s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 17/390 [01:00<22:27,  3.61s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 17/390 [01:03<22:27,  3.61s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 18/390 [01:03<21:36,  3.48s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 18/390 [01:06<21:36,  3.48s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 19/390 [01:06<21:01,  3.40s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 19/390 [01:11<21:01,  3.40s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 20/390 [01:11<23:24,  3.80s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 20/390 [01:14<23:24,  3.80s/it, Loss=2.1] \u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 21/390 [01:14<22:17,  3.62s/it, Loss=2.1]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 21/390 [01:17<22:17,  3.62s/it, Loss=1.85]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   6%|▌         | 22/390 [01:17<21:39,  3.53s/it, Loss=1.85]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:11<20:51,  3.39s/it]\n",
            "Analysis Cov\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:09<20:26,  3.32s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 1/390 [00:03<19:59,  3.08s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 1/390 [00:06<19:59,  3.08s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 2/390 [00:06<19:57,  3.09s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 2/390 [00:09<19:57,  3.09s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 3/390 [00:09<21:33,  3.34s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 3/390 [00:14<21:33,  3.34s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 4/390 [00:14<23:43,  3.69s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 4/390 [00:17<23:43,  3.69s/it, Loss=2.11]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|▏         | 5/390 [00:17<22:30,  3.51s/it, Loss=2.11]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|▏         | 5/390 [00:20<22:30,  3.51s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 6/390 [00:20<21:46,  3.40s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 6/390 [00:24<21:46,  3.40s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 7/390 [00:24<22:05,  3.46s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 7/390 [00:28<22:05,  3.46s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 8/390 [00:28<23:42,  3.72s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 8/390 [00:31<23:42,  3.72s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 9/390 [00:31<22:34,  3.56s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 9/390 [00:34<22:34,  3.56s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 10/390 [00:34<21:49,  3.45s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 10/390 [00:38<21:49,  3.45s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 11/390 [00:38<21:50,  3.46s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 11/390 [00:42<21:50,  3.46s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 12/390 [00:42<23:23,  3.71s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 12/390 [00:45<23:23,  3.71s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 13/390 [00:45<22:16,  3.55s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 13/390 [00:48<22:16,  3.55s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▎         | 14/390 [00:48<21:30,  3.43s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▎         | 14/390 [00:52<21:30,  3.43s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 15/390 [00:52<21:35,  3.46s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 15/390 [00:56<21:35,  3.46s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 16/390 [00:56<23:13,  3.73s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 16/390 [00:59<23:13,  3.73s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 17/390 [00:59<22:11,  3.57s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 17/390 [01:03<22:11,  3.57s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 18/390 [01:03<21:24,  3.45s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 18/390 [01:06<21:24,  3.45s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 19/390 [01:06<21:27,  3.47s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 19/390 [01:11<21:27,  3.47s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 20/390 [01:11<23:14,  3.77s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 20/390 [01:14<23:14,  3.77s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 21/390 [01:14<22:09,  3.60s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 21/390 [01:17<22:09,  3.60s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   6%|▌         | 22/390 [01:17<21:35,  3.52s/it, Loss=1.92]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:09<20:25,  3.32s/it]\n",
            "Analysis Cov\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:11<20:59,  3.41s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 3:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   0%|          | 1/390 [00:03<20:53,  3.22s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   0%|          | 1/390 [00:07<20:53,  3.22s/it, Loss=2.08]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 2/390 [00:07<26:04,  4.03s/it, Loss=2.08]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 2/390 [00:10<26:04,  4.03s/it, Loss=2.06]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 3/390 [00:10<23:25,  3.63s/it, Loss=2.06]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 3/390 [00:14<23:25,  3.63s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 4/390 [00:14<22:05,  3.43s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|          | 4/390 [00:17<22:05,  3.43s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|▏         | 5/390 [00:17<21:19,  3.32s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   1%|▏         | 5/390 [00:21<21:19,  3.32s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 6/390 [00:21<24:04,  3.76s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 6/390 [00:24<24:04,  3.76s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 7/390 [00:24<22:40,  3.55s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 7/390 [00:28<22:40,  3.55s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 8/390 [00:28<21:45,  3.42s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 8/390 [00:31<21:45,  3.42s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 9/390 [00:31<21:05,  3.32s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   2%|▏         | 9/390 [00:35<21:05,  3.32s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 10/390 [00:35<23:39,  3.74s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 10/390 [00:39<23:39,  3.74s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 11/390 [00:39<22:28,  3.56s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 11/390 [00:42<22:28,  3.56s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 12/390 [00:42<21:35,  3.43s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 12/390 [00:45<21:35,  3.43s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 13/390 [00:45<21:03,  3.35s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   3%|▎         | 13/390 [00:49<21:03,  3.35s/it, Loss=1.9] \u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▎         | 14/390 [00:49<23:09,  3.69s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▎         | 14/390 [00:53<23:09,  3.69s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 15/390 [00:53<22:10,  3.55s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 15/390 [00:56<22:10,  3.55s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 16/390 [00:56<21:22,  3.43s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 16/390 [00:59<21:22,  3.43s/it, Loss=2]   \u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 17/390 [00:59<20:46,  3.34s/it, Loss=2]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   4%|▍         | 17/390 [01:03<20:46,  3.34s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▍         | 18/390 [01:03<22:45,  3.67s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▍         | 18/390 [01:07<22:45,  3.67s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▍         | 19/390 [01:07<22:11,  3.59s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▍         | 19/390 [01:10<22:11,  3.59s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▌         | 20/390 [01:10<21:15,  3.45s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▌         | 20/390 [01:13<21:15,  3.45s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▌         | 21/390 [01:13<20:34,  3.34s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 3:   5%|▌         | 21/390 [01:17<20:34,  3.34s/it, Loss=1.9] \u001b[A\n",
            "LoRA Rank 1 Epoch 3:   6%|▌         | 22/390 [01:17<21:38,  3.53s/it, Loss=1.9]\n",
            "Analysis Mean\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:10<20:41,  3.37s/it]\n",
            "Analysis Cov\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:09<20:18,  3.30s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 4:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   0%|          | 0/390 [00:04<?, ?it/s, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   0%|          | 1/390 [00:04<27:03,  4.17s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   0%|          | 1/390 [00:07<27:03,  4.17s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 2/390 [00:07<23:08,  3.58s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 2/390 [00:10<23:08,  3.58s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 3/390 [00:10<21:47,  3.38s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 3/390 [00:13<21:47,  3.38s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 4/390 [00:13<21:53,  3.40s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|          | 4/390 [00:18<21:53,  3.40s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|▏         | 5/390 [00:18<23:42,  3.70s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   1%|▏         | 5/390 [00:21<23:42,  3.70s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 6/390 [00:21<22:20,  3.49s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 6/390 [00:24<22:20,  3.49s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 7/390 [00:24<21:26,  3.36s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 7/390 [00:27<21:26,  3.36s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 8/390 [00:27<21:18,  3.35s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 8/390 [00:31<21:18,  3.35s/it, Loss=2.1] \u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 9/390 [00:31<23:11,  3.65s/it, Loss=2.1]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   2%|▏         | 9/390 [00:35<23:11,  3.65s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 10/390 [00:35<21:59,  3.47s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 10/390 [00:38<21:59,  3.47s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 11/390 [00:38<21:09,  3.35s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 11/390 [00:41<21:09,  3.35s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 12/390 [00:41<20:37,  3.27s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 12/390 [00:45<20:37,  3.27s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 13/390 [00:45<22:47,  3.63s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   3%|▎         | 13/390 [00:48<22:47,  3.63s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▎         | 14/390 [00:48<21:34,  3.44s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▎         | 14/390 [00:51<21:34,  3.44s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 15/390 [00:51<20:49,  3.33s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 15/390 [00:54<20:49,  3.33s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 16/390 [00:54<20:16,  3.25s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 16/390 [00:59<20:16,  3.25s/it, Loss=2.13]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 17/390 [00:59<22:31,  3.62s/it, Loss=2.13]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   4%|▍         | 17/390 [01:02<22:31,  3.62s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▍         | 18/390 [01:02<21:35,  3.48s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▍         | 18/390 [01:05<21:35,  3.48s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▍         | 19/390 [01:05<20:50,  3.37s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▍         | 19/390 [01:08<20:50,  3.37s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▌         | 20/390 [01:08<20:24,  3.31s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▌         | 20/390 [01:13<20:24,  3.31s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▌         | 21/390 [01:13<22:22,  3.64s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   5%|▌         | 21/390 [01:16<22:22,  3.64s/it, Loss=1.86]\u001b[A\n",
            "LoRA Rank 1 Epoch 4:   6%|▌         | 22/390 [01:16<21:19,  3.48s/it, Loss=1.86]\n",
            "Analysis Mean\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:10<20:47,  3.38s/it]\n",
            "Analysis Cov\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:14<21:55,  3.56s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 5:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   0%|          | 1/390 [00:03<20:08,  3.11s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   0%|          | 1/390 [00:06<20:08,  3.11s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 2/390 [00:06<20:22,  3.15s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 2/390 [00:10<20:22,  3.15s/it, Loss=2.05]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 3/390 [00:10<22:55,  3.55s/it, Loss=2.05]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 3/390 [00:14<22:55,  3.55s/it, Loss=1.87]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 4/390 [00:14<23:27,  3.65s/it, Loss=1.87]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|          | 4/390 [00:17<23:27,  3.65s/it, Loss=2.14]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|▏         | 5/390 [00:17<22:23,  3.49s/it, Loss=2.14]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   1%|▏         | 5/390 [00:20<22:23,  3.49s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 6/390 [00:20<21:42,  3.39s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 6/390 [00:24<21:42,  3.39s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 7/390 [00:24<23:04,  3.62s/it, Loss=1.89]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 7/390 [00:28<23:04,  3.62s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 8/390 [00:28<23:26,  3.68s/it, Loss=2.01]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 8/390 [00:31<23:26,  3.68s/it, Loss=2]   \u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 9/390 [00:31<22:23,  3.53s/it, Loss=2]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   2%|▏         | 9/390 [00:34<22:23,  3.53s/it, Loss=1.85]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 10/390 [00:34<21:39,  3.42s/it, Loss=1.85]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 10/390 [00:38<21:39,  3.42s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 11/390 [00:38<22:45,  3.60s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 11/390 [00:42<22:45,  3.60s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 12/390 [00:42<23:03,  3.66s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 12/390 [00:45<23:03,  3.66s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 13/390 [00:45<22:06,  3.52s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   3%|▎         | 13/390 [00:48<22:06,  3.52s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▎         | 14/390 [00:48<21:26,  3.42s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▎         | 14/390 [00:53<21:26,  3.42s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 15/390 [00:53<22:30,  3.60s/it, Loss=1.99]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 15/390 [00:56<22:30,  3.60s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 16/390 [00:56<22:57,  3.68s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 16/390 [01:00<22:57,  3.68s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 17/390 [01:00<22:00,  3.54s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   4%|▍         | 17/390 [01:03<22:00,  3.54s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▍         | 18/390 [01:03<21:21,  3.44s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▍         | 18/390 [01:07<21:21,  3.44s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▍         | 19/390 [01:07<22:26,  3.63s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▍         | 19/390 [01:11<22:26,  3.63s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▌         | 20/390 [01:11<22:48,  3.70s/it, Loss=1.93]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▌         | 20/390 [01:14<22:48,  3.70s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▌         | 21/390 [01:14<21:48,  3.54s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   5%|▌         | 21/390 [01:17<21:48,  3.54s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 5:   6%|▌         | 22/390 [01:17<21:39,  3.53s/it, Loss=1.96]\n",
            "Analysis Mean\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:10<20:31,  3.34s/it]\n",
            "Analysis Cov\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:11<20:59,  3.41s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 6:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   0%|          | 1/390 [00:03<25:49,  3.98s/it, Loss=1.91]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   0%|          | 1/390 [00:08<25:49,  3.98s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 2/390 [00:08<25:55,  4.01s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 2/390 [00:11<25:55,  4.01s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 3/390 [00:11<23:22,  3.62s/it, Loss=2.04]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 3/390 [00:14<23:22,  3.62s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 4/390 [00:14<22:02,  3.43s/it, Loss=1.95]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|          | 4/390 [00:17<22:02,  3.43s/it, Loss=1.84]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|▏         | 5/390 [00:17<22:24,  3.49s/it, Loss=1.84]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   1%|▏         | 5/390 [00:21<22:24,  3.49s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 6/390 [00:21<23:38,  3.69s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 6/390 [00:25<23:38,  3.69s/it, Loss=1.9] \u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 7/390 [00:25<22:23,  3.51s/it, Loss=1.9]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 7/390 [00:28<22:23,  3.51s/it, Loss=2.06]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 8/390 [00:28<21:34,  3.39s/it, Loss=2.06]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 8/390 [00:31<21:34,  3.39s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 9/390 [00:31<21:44,  3.42s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   2%|▏         | 9/390 [00:36<21:44,  3.42s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 10/390 [00:36<23:17,  3.68s/it, Loss=1.88]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 10/390 [00:39<23:17,  3.68s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 11/390 [00:39<22:14,  3.52s/it, Loss=2.03]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 11/390 [00:42<22:14,  3.52s/it, Loss=2]   \u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 12/390 [00:42<21:27,  3.41s/it, Loss=2]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 12/390 [00:45<21:27,  3.41s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 13/390 [00:45<21:24,  3.41s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   3%|▎         | 13/390 [00:50<21:24,  3.41s/it, Loss=2]   \u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▎         | 14/390 [00:50<23:04,  3.68s/it, Loss=2]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▎         | 14/390 [00:53<23:04,  3.68s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 15/390 [00:53<21:54,  3.50s/it, Loss=1.92]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 15/390 [00:56<21:54,  3.50s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 16/390 [00:56<21:14,  3.41s/it, Loss=1.98]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 16/390 [00:59<21:14,  3.41s/it, Loss=1.87]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 17/390 [00:59<20:54,  3.36s/it, Loss=1.87]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   4%|▍         | 17/390 [01:03<20:54,  3.36s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▍         | 18/390 [01:03<22:48,  3.68s/it, Loss=1.94]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▍         | 18/390 [01:07<22:48,  3.68s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▍         | 19/390 [01:07<21:44,  3.52s/it, Loss=2.02]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▍         | 19/390 [01:10<21:44,  3.52s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▌         | 20/390 [01:10<20:56,  3.40s/it, Loss=1.96]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▌         | 20/390 [01:13<20:56,  3.40s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▌         | 21/390 [01:13<20:26,  3.32s/it, Loss=1.97]\u001b[A\n",
            "LoRA Rank 1 Epoch 6:   5%|▌         | 21/390 [01:17<20:26,  3.32s/it, Loss=1.9] \u001b[A\n",
            "LoRA Rank 1 Epoch 6:   6%|▌         | 22/390 [01:17<21:44,  3.54s/it, Loss=1.9]\n",
            "Analysis Mean\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:07<19:44,  3.21s/it]\n",
            "Analysis Cov\tEpoch: 6 [7/390 (2%)]:   2%|▏         | 7/390 [00:23<21:23,  3.35s/it]"
          ]
        }
      ]
    }
  ]
}