{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V6JtNCJOzDu"
      },
      "source": [
        "# Experiment 3: LoRA and Frozen Backbone Probes\n",
        "- Take a pretrained classifier (e.g., trained ResNet).\n",
        "- Fine-tune only the final layer using LoRA, varying the rank (e.g., 1–16).\n",
        "- Evaluate:\n",
        " - Whether NC geometry persists or evolves under low-rank adaptation.\n",
        " - If LoRA directions align with NC class mean directions (cosine similarity, projection overlap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiJGkQOuVcut"
      },
      "source": [
        "### TODO\n",
        "1. baseline: full fine-tuning (necessary?) (think: MPU)\n",
        "2. validate data collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620YzzMPOvDJ",
        "outputId": "25176bea-63e4-4fc1-df89-e08170355589"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mount Drive for checkpoints\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Reuse existing imports and configurations from Experiment 1+2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "debug = True\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "lr                  = 0.01 # verify with ablation\n",
        "batch_size          = 128\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4 # too high?\n",
        "\n",
        "epochs              = 20\n",
        "ranks               = [1,2,4,8,16]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4R4RmRuPfmY"
      },
      "outputs": [],
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [torch.zeros(model.fc.in_features, device=device) for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Cov']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Cov':\n",
        "                    # update within-class cov\n",
        "\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "                graphs.mean = mean\n",
        "            loss /= sum(N)\n",
        "\n",
        "            # Feature rank analysis\n",
        "\n",
        "\n",
        "\n",
        "        elif computation == 'Cov':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    if hasattr(classifier, 'original'):\n",
        "        W = classifier.original.weight  # LoRA case #?\n",
        "    else:\n",
        "        W = classifier.weight # rank=0 baseline\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.cpu().double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.svd_lowrank(Sb, q=C-1)\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))\n",
        "\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    self.mean         = []\n",
        "    self.feature_rank = [] # not used\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SHzUkNIQq32"
      },
      "outputs": [],
      "source": [
        "def prepare_lora_model(pretrained_model, rank=4):\n",
        "    config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=16,  # Scaling factor (alpha = 16 is common)\n",
        "        target_modules=[\"fc\"],  # Apply LoRA to final layer\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "    model = get_peft_model(pretrained_model, config)\n",
        "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def compute_alignment_metrics(model, class_means, rank):\n",
        "    \"\"\"Calculate alignment between LoRA directions and NC class means.\"\"\"\n",
        "    lora_A = model.base_model.model.fc.lora_A['default'].weight  # [rank, in_dim]\n",
        "    lora_B = model.base_model.model.fc.lora_B['default'].weight  # [out_dim, rank]\n",
        "    W_lora = lora_B @ lora_A  # Combined LoRA direction [out_dim, in_dim]\n",
        "\n",
        "    # Project class means onto LoRA subspace\n",
        "    M = class_means.T.cpu().numpy()  # [in_dim, C]\n",
        "    U, _, _ = randomized_svd(W_lora.detach().cpu().numpy(), n_components=rank)\n",
        "\n",
        "    # Cosine similarity between LoRA directions and class means\n",
        "    cos_sims = []\n",
        "    for c in range(M.shape[1]):\n",
        "        v = M[:, c]\n",
        "        for i in range(U.shape[1]):\n",
        "            u = U[:, i]\n",
        "            cos_sim = np.dot(u, v) / (np.linalg.norm(u)*np.linalg.norm(v)+1e-8)\n",
        "            cos_sims.append(cos_sim)\n",
        "\n",
        "    # Subspace projection score\n",
        "    proj = U.T @ M\n",
        "    proj_score = np.linalg.norm(proj)**2 / np.linalg.norm(M)**2\n",
        "\n",
        "    return np.mean(cos_sims), proj_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jZ0_e2fQ5bJ"
      },
      "outputs": [],
      "source": [
        "def train_lora(model, criterion, device, train_loader, optimizer, epoch, rank):\n",
        "    model.train()\n",
        "    # model.fc.original.requires_grad_(False)  # redundant as peft freezes weights automatically\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'LoRA Rank {rank} Epoch {epoch}')\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'Loss': loss.item()})\n",
        "        pbar.update(1)\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "            break\n",
        "    pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yKTl8zXRCdf",
        "outputId": "d6fe1beb-23b9-49eb-935c-9781997defd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 170M/170M [00:04<00:00, 38.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:22,  2.99s/it]\n",
            "Analysis Cov\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:30,  3.01s/it]\n",
            "LoRA Rank 1 Epoch 1:   6%|▌         | 22/390 [01:10<19:41,  3.21s/it, Loss=1.28]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:35,  2.86s/it]\n",
            "Analysis Cov\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:50,  3.06s/it]\n",
            "LoRA Rank 1 Epoch 2:   6%|▌         | 22/390 [01:10<19:39,  3.21s/it, Loss=1.18]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:22,  2.99s/it]\n",
            "Analysis Cov\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:06<19:22,  3.15s/it]\n",
            "LoRA Rank 1 Epoch 3:   6%|▌         | 22/390 [01:13<20:24,  3.33s/it, Loss=1.19]\n",
            "Analysis Mean\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:57,  3.08s/it]\n",
            "Analysis Cov\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:37,  3.03s/it]\n",
            "LoRA Rank 1 Epoch 4:   6%|▌         | 22/390 [01:10<19:46,  3.22s/it, Loss=1.35]\n",
            "Analysis Mean\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:06<19:27,  3.16s/it]\n",
            "Analysis Cov\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:15,  2.97s/it]\n",
            "LoRA Rank 1 Epoch 5:   6%|▌         | 22/390 [01:11<19:49,  3.23s/it, Loss=1.33]\n",
            "Analysis Mean\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:59,  2.93s/it]\n",
            "Analysis Cov\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:07<19:50,  3.23s/it]\n",
            "LoRA Rank 1 Epoch 6:   6%|▌         | 22/390 [01:11<19:54,  3.24s/it, Loss=1.35]\n",
            "Analysis Mean\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:40,  2.87s/it]\n",
            "Analysis Cov\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:22,  2.99s/it]\n",
            "LoRA Rank 1 Epoch 7:   6%|▌         | 22/390 [01:11<19:58,  3.26s/it, Loss=1.33]\n",
            "Analysis Mean\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:52,  2.91s/it]\n",
            "Analysis Cov\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:29,  3.01s/it]\n",
            "LoRA Rank 1 Epoch 8:   6%|▌         | 22/390 [01:10<19:43,  3.22s/it, Loss=1.41]\n",
            "Analysis Mean\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:50,  2.90s/it]\n",
            "Analysis Cov\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [01:06<19:28,  3.17s/it]\n",
            "LoRA Rank 1 Epoch 9:   6%|▌         | 22/390 [01:10<19:44,  3.22s/it, Loss=1.39]\n",
            "Analysis Mean\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:53,  2.91s/it]\n",
            "Analysis Cov\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:12,  2.96s/it]\n",
            "LoRA Rank 1 Epoch 10:   6%|▌         | 22/390 [01:11<19:54,  3.25s/it, Loss=1.48]\n",
            "Analysis Mean\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:34,  2.86s/it]\n",
            "Analysis Cov\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:10,  2.96s/it]\n",
            "LoRA Rank 1 Epoch 11:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.3]\n",
            "Analysis Mean\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:00,  2.93s/it]\n",
            "Analysis Cov\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:54,  2.91s/it]\n",
            "LoRA Rank 1 Epoch 12:   6%|▌         | 22/390 [01:14<20:39,  3.37s/it, Loss=1.27]\n",
            "Analysis Mean\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:50,  2.90s/it]\n",
            "Analysis Cov\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:10,  2.96s/it]\n",
            "LoRA Rank 1 Epoch 13:   6%|▌         | 22/390 [01:11<19:50,  3.24s/it, Loss=1.47]\n",
            "Analysis Mean\tEpoch: 13 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:16,  2.97s/it]\n",
            "Analysis Cov\tEpoch: 13 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:07,  2.95s/it]\n",
            "LoRA Rank 1 Epoch 14:   6%|▌         | 22/390 [01:10<19:47,  3.23s/it, Loss=1.26]\n",
            "Analysis Mean\tEpoch: 14 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:02,  2.93s/it]\n",
            "Analysis Cov\tEpoch: 14 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:22,  2.99s/it]\n",
            "LoRA Rank 1 Epoch 15:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.38]\n",
            "Analysis Mean\tEpoch: 15 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:00,  2.93s/it]\n",
            "Analysis Cov\tEpoch: 15 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:11,  2.96s/it]\n",
            "LoRA Rank 1 Epoch 16:   6%|▌         | 22/390 [01:09<19:27,  3.17s/it, Loss=1.28]\n",
            "Analysis Mean\tEpoch: 16 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:55,  2.91s/it]\n",
            "Analysis Cov\tEpoch: 16 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:53,  2.91s/it]\n",
            "LoRA Rank 1 Epoch 17:   6%|▌         | 22/390 [01:12<20:09,  3.29s/it, Loss=1.22]\n",
            "Analysis Mean\tEpoch: 17 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:25,  2.99s/it]\n",
            "Analysis Cov\tEpoch: 17 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:26,  3.00s/it]\n",
            "LoRA Rank 1 Epoch 18:   6%|▌         | 22/390 [01:10<19:33,  3.19s/it, Loss=1.36]\n",
            "Analysis Mean\tEpoch: 18 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:24,  2.99s/it]\n",
            "Analysis Cov\tEpoch: 18 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:13,  2.96s/it]\n",
            "LoRA Rank 1 Epoch 19:   6%|▌         | 22/390 [01:10<19:37,  3.20s/it, Loss=1.42]\n",
            "Analysis Mean\tEpoch: 19 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:35,  2.86s/it]\n",
            "Analysis Cov\tEpoch: 19 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:22,  2.99s/it]\n",
            "LoRA Rank 1 Epoch 20:   6%|▌         | 22/390 [01:11<19:55,  3.25s/it, Loss=1.31]\n",
            "Analysis Mean\tEpoch: 20 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:58,  2.92s/it]\n",
            "Analysis Cov\tEpoch: 20 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:01,  2.93s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 1044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:05<19:06,  3.11s/it]\n",
            "Analysis Cov\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<19:01,  3.09s/it]\n",
            "LoRA Rank 2 Epoch 1:   6%|▌         | 22/390 [01:09<19:30,  3.18s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:44,  3.05s/it]\n",
            "Analysis Cov\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:05<19:12,  3.12s/it]\n",
            "LoRA Rank 2 Epoch 2:   6%|▌         | 22/390 [01:11<19:56,  3.25s/it, Loss=1.54]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:59,  3.09s/it]\n",
            "Analysis Cov\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:21,  2.98s/it]\n",
            "LoRA Rank 2 Epoch 3:   6%|▌         | 22/390 [01:11<19:54,  3.25s/it, Loss=1.59]\n",
            "Analysis Mean\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:13,  2.96s/it]\n",
            "Analysis Cov\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:30,  3.01s/it]\n",
            "LoRA Rank 2 Epoch 4:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.44]\n",
            "Analysis Mean\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:05,  2.94s/it]\n",
            "Analysis Cov\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:40,  3.04s/it]\n",
            "LoRA Rank 2 Epoch 5:   6%|▌         | 22/390 [01:10<19:36,  3.20s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:55,  2.92s/it]\n",
            "Analysis Cov\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:28,  3.00s/it]\n",
            "LoRA Rank 2 Epoch 6:   6%|▌         | 22/390 [01:10<19:39,  3.20s/it, Loss=1.28]\n",
            "Analysis Mean\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:05<19:03,  3.10s/it]\n",
            "Analysis Cov\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:36,  3.03s/it]\n",
            "LoRA Rank 2 Epoch 7:   6%|▌         | 22/390 [01:10<19:37,  3.20s/it, Loss=1.47]\n",
            "Analysis Mean\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:51,  2.90s/it]\n",
            "Analysis Cov\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:53,  3.07s/it]\n",
            "LoRA Rank 2 Epoch 8:   6%|▌         | 22/390 [01:09<19:30,  3.18s/it, Loss=1.41]\n",
            "Analysis Mean\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [01:05<19:07,  3.11s/it]\n",
            "Analysis Cov\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:47,  3.06s/it]\n",
            "LoRA Rank 2 Epoch 9:   6%|▌         | 22/390 [01:11<19:51,  3.24s/it, Loss=1.42]\n",
            "Analysis Mean\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:01,  2.93s/it]\n",
            "Analysis Cov\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:18,  2.98s/it]\n",
            "LoRA Rank 2 Epoch 10:   6%|▌         | 22/390 [01:11<19:49,  3.23s/it, Loss=1.29]\n",
            "Analysis Mean\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:06,  2.94s/it]\n",
            "Analysis Cov\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:24,  2.99s/it]\n",
            "LoRA Rank 2 Epoch 11:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:08,  2.95s/it]\n",
            "Analysis Cov\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:59,  2.93s/it]\n",
            "LoRA Rank 2 Epoch 12:   6%|▌         | 22/390 [01:11<20:03,  3.27s/it, Loss=1.29]\n",
            "Analysis Mean\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:53,  3.07s/it]\n",
            "Analysis Cov\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:35,  3.02s/it]\n",
            "LoRA Rank 2 Epoch 13:   6%|▌         | 22/390 [01:10<19:34,  3.19s/it, Loss=1.25]\n",
            "Analysis Mean\tEpoch: 13 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:43,  3.05s/it]\n",
            "Analysis Cov\tEpoch: 13 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:58,  2.92s/it]\n",
            "LoRA Rank 2 Epoch 14:   6%|▌         | 22/390 [01:11<19:50,  3.23s/it, Loss=1.46]\n",
            "Analysis Mean\tEpoch: 14 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:53,  2.91s/it]\n",
            "Analysis Cov\tEpoch: 14 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:56,  3.08s/it]\n",
            "LoRA Rank 2 Epoch 15:   6%|▌         | 22/390 [01:09<19:28,  3.18s/it, Loss=1.38]\n",
            "Analysis Mean\tEpoch: 15 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:25,  3.00s/it]\n",
            "Analysis Cov\tEpoch: 15 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:32,  3.01s/it]\n",
            "LoRA Rank 2 Epoch 16:   6%|▌         | 22/390 [01:12<20:05,  3.27s/it, Loss=1.22]\n",
            "Analysis Mean\tEpoch: 16 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:04,  2.94s/it]\n",
            "Analysis Cov\tEpoch: 16 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<19:00,  3.09s/it]\n",
            "LoRA Rank 2 Epoch 17:   6%|▌         | 22/390 [01:10<19:41,  3.21s/it, Loss=1.34]\n",
            "Analysis Mean\tEpoch: 17 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:13,  2.96s/it]\n",
            "Analysis Cov\tEpoch: 17 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:52,  3.07s/it]\n",
            "LoRA Rank 2 Epoch 18:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.2]\n",
            "Analysis Mean\tEpoch: 18 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:28,  3.00s/it]\n",
            "Analysis Cov\tEpoch: 18 [21/390 (5%)]:   5%|▌         | 21/390 [01:06<19:31,  3.17s/it]\n",
            "LoRA Rank 2 Epoch 19:   6%|▌         | 22/390 [01:12<20:17,  3.31s/it, Loss=1.18]\n",
            "Analysis Mean\tEpoch: 19 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:33,  3.02s/it]\n",
            "Analysis Cov\tEpoch: 19 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:18,  2.98s/it]\n",
            "LoRA Rank 2 Epoch 20:   6%|▌         | 22/390 [01:10<19:32,  3.19s/it, Loss=1.38]\n",
            "Analysis Mean\tEpoch: 20 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:13,  2.96s/it]\n",
            "Analysis Cov\tEpoch: 20 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:50,  3.06s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable parameters: 2088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:49,  2.90s/it]\n",
            "Analysis Cov\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:28,  3.00s/it]\n",
            "LoRA Rank 4 Epoch 1:   6%|▌         | 22/390 [01:09<19:30,  3.18s/it, Loss=1.27]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:06<19:27,  3.16s/it]\n",
            "Analysis Cov\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:19,  2.98s/it]\n",
            "LoRA Rank 4 Epoch 2:   6%|▌         | 22/390 [01:10<19:39,  3.21s/it, Loss=1.3]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:39,  2.87s/it]\n",
            "Analysis Cov\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:36,  3.03s/it]\n",
            "LoRA Rank 4 Epoch 3:   6%|▌         | 22/390 [01:10<19:35,  3.19s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:56,  2.92s/it]\n",
            "Analysis Cov\tEpoch: 3 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:53,  2.91s/it]\n",
            "LoRA Rank 4 Epoch 4:   6%|▌         | 22/390 [01:10<19:46,  3.22s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:05,  2.94s/it]\n",
            "Analysis Cov\tEpoch: 4 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:17,  2.97s/it]\n",
            "LoRA Rank 4 Epoch 5:   6%|▌         | 22/390 [01:10<19:32,  3.19s/it, Loss=1.41]\n",
            "Analysis Mean\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:43,  3.04s/it]\n",
            "Analysis Cov\tEpoch: 5 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:14,  2.97s/it]\n",
            "LoRA Rank 4 Epoch 6:   6%|▌         | 22/390 [01:12<20:12,  3.30s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:09,  2.95s/it]\n",
            "Analysis Cov\tEpoch: 6 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:33,  3.02s/it]\n",
            "LoRA Rank 4 Epoch 7:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.26]\n",
            "Analysis Mean\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:20,  2.98s/it]\n",
            "Analysis Cov\tEpoch: 7 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:09,  2.95s/it]\n",
            "LoRA Rank 4 Epoch 8:   6%|▌         | 22/390 [01:10<19:36,  3.20s/it, Loss=1.28]\n",
            "Analysis Mean\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [00:59<17:34,  2.86s/it]\n",
            "Analysis Cov\tEpoch: 8 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:14,  2.97s/it]\n",
            "LoRA Rank 4 Epoch 9:   6%|▌         | 22/390 [01:11<19:54,  3.25s/it, Loss=1.56]\n",
            "Analysis Mean\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:04<18:50,  3.06s/it]\n",
            "Analysis Cov\tEpoch: 9 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:32,  3.02s/it]\n",
            "LoRA Rank 4 Epoch 10:   6%|▌         | 22/390 [01:09<19:30,  3.18s/it, Loss=1.42]\n",
            "Analysis Mean\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:03<18:39,  3.03s/it]\n",
            "Analysis Cov\tEpoch: 10 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:17,  2.97s/it]\n",
            "LoRA Rank 4 Epoch 11:   6%|▌         | 22/390 [01:10<19:31,  3.18s/it, Loss=1.37]\n",
            "Analysis Mean\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:20,  2.98s/it]\n",
            "Analysis Cov\tEpoch: 11 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:10,  2.96s/it]\n",
            "LoRA Rank 4 Epoch 12:   6%|▌         | 22/390 [01:11<19:59,  3.26s/it, Loss=1.21]\n",
            "Analysis Mean\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:02<18:23,  2.99s/it]\n",
            "Analysis Cov\tEpoch: 12 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:01,  2.93s/it]\n",
            "LoRA Rank 4 Epoch 13:   6%|▌         | 22/390 [01:10<19:32,  3.19s/it, Loss=1.3]\n",
            "Analysis Mean\tEpoch: 13 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<18:07,  2.95s/it]\n",
            "Analysis Cov\tEpoch: 13 [19/390 (5%)]:   5%|▍         | 19/390 [00:58<18:50,  3.05s/it]"
          ]
        }
      ],
      "source": [
        "# ====================== EXECUTION ======================\n",
        "# checkpoint from Experiment 1+2\n",
        "checkpoint = torch.load('/content/drive/MyDrive/checkpoint.pth', weights_only=False)\n",
        "\n",
        "# dataset, optimizer setup from Experiment 1+2\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "## Run Experiment 3\n",
        "original_class_means = None  # To store initial NC geometry\n",
        "for rank in ranks:\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Load fresh pretrained model for each run\n",
        "    pretrained_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "    pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    pretrained_model.maxpool = nn.Identity()\n",
        "    # Register feature hook\n",
        "    class features:\n",
        "        pass\n",
        "    def hook(self, input, output):\n",
        "        features.value = input[0].clone()\n",
        "    pretrained_model.fc.register_forward_hook(hook)\n",
        "    pretrained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    pretrained_model.to(device)\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = prepare_lora_model(pretrained_model, rank=rank)\n",
        "    # optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "    optimizer = optim.SGD(pretrained_model.parameters(),\n",
        "                          lr=lr,\n",
        "                          momentum=momentum,\n",
        "                          weight_decay=weight_decay) # SGD, not Adam, for consistency with Exp1+2\n",
        "\n",
        "    # Initial NC state, from Exp1+2\n",
        "    graphs = Graphs()\n",
        "    analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch=0)\n",
        "    original_class_means = torch.stack(graphs.mean).T if original_class_means is None else original_class_means\n",
        "\n",
        "    # Experiment 3: Fine-tune pretrained model for 20 epochs under LoRA\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_lora(model, criterion, device, train_loader, optimizer, epoch, rank)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "        # Compute alignment metrics\n",
        "        current_class_means = torch.stack(graphs.mean).T\n",
        "        cos_sim, proj_score = compute_alignment_metrics(model, current_class_means, rank)\n",
        "\n",
        "        results.append({\n",
        "            'rank': rank,\n",
        "            'epoch': epoch,\n",
        "            'Sw_invSb': graphs.Sw_invSb[-1],\n",
        "            'W_M_dist': graphs.W_M_dist,\n",
        "            'NCC_mismatch': graphs.NCC_mismatch,\n",
        "            'norm_M_CoV': graphs.norm_M_CoV,\n",
        "            'norm_W_CoV': graphs.norm_W_CoV,\n",
        "            'cos_M': graphs.cos_M,\n",
        "            'cos_W': graphs.cos_W,\n",
        "            # 'feature_rank': graphs.feature_rank[-1],\n",
        "            'cos_sim': cos_sim,\n",
        "            'proj_score': proj_score\n",
        "            # 'test_acc': graphs.accuracy[-1]  # Assuming test loader available\n",
        "        })\n",
        "\n",
        "    # Save results per rank\n",
        "    pd.DataFrame(results).to_csv(f'lora_rank_{rank}_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDtYgy7VHUIp"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "df = pd.DataFrame(results)\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['cos_sim'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Cosine Similarity')\n",
        "plt.title('LoRA Direction Alignment with Class Means')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['NC1'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NC1 (Tr{Sw Sb^-1})')\n",
        "plt.title('Neural Collapse Persistence During LoRA Tuning')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline: Full Fine-Tuning\n",
        "\n",
        "\n",
        "From Sonnet; not debugged"
      ],
      "metadata": {
        "id": "HkK4hWwxPohr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After your LoRA experiments, add:\n",
        "\n",
        "# Full fine-tuning baseline\n",
        "full_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "full_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "full_model.maxpool = nn.Identity()\n",
        "full_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "full_model.to(device)\n",
        "\n",
        "# Register hook\n",
        "full_model.fc.register_forward_hook(hook)\n",
        "\n",
        "# Save original weights for later comparison\n",
        "original_fc_weights = full_model.fc.weight.clone().detach()\n",
        "\n",
        "# For full fine-tuning, only train the classifier layer for fair comparison with LoRA\n",
        "for param in full_model.parameters():\n",
        "    param.requires_grad = False\n",
        "full_model.fc.weight.requires_grad = True\n",
        "full_model.fc.bias.requires_grad = True\n",
        "\n",
        "# Use same optimizer as LoRA experiments\n",
        "optimizer = optim.SGD(full_model.fc.parameters(), lr=0.01, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# Train and collect same metrics\n",
        "full_results = []\n",
        "graphs = Graphs()\n",
        "for epoch in range(1, 21):\n",
        "    train_full_ft(full_model, criterion, device, train_loader, optimizer, epoch)\n",
        "    analysis(graphs, full_model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "    # Compute weight update and its rank\n",
        "    current_weights = full_model.fc.weight.clone().detach()\n",
        "    weight_delta = current_weights - original_fc_weights\n",
        "    weight_delta_rank = compute_effective_rank(weight_delta)\n",
        "\n",
        "    # Current class means\n",
        "    current_class_means = torch.stack(graphs.mean).T\n",
        "\n",
        "    # Store results with same metrics as LoRA\n",
        "    full_results.append({\n",
        "        'method': 'full_ft',\n",
        "        'epoch': epoch,\n",
        "        'NC1': graphs.Sw_invSb[-1],\n",
        "        'NC2': graphs.norm_M_CoV[-1],\n",
        "        'NC3': graphs.W_M_dist[-1],\n",
        "        'NC4': graphs.NCC_mismatch[-1],\n",
        "        'weight_update_rank': weight_delta_rank,\n",
        "        'class_mean_change': torch.norm(current_class_means - original_class_means).item(),\n",
        "    })\n",
        "\n",
        "pd.DataFrame(full_results).to_csv('full_ft_results.csv', index=False)"
      ],
      "metadata": {
        "id": "3r9U7NEePyuB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQKcsOLrK4xqHVic6mGB4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}