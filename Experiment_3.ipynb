{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V6JtNCJOzDu"
      },
      "source": [
        "# Experiment 3: LoRA and Frozen Backbone Probes\n",
        "- Take a pretrained classifier (e.g., trained ResNet).\n",
        "- Fine-tune only the final layer using LoRA, varying the rank (e.g., 1–16).\n",
        "- Evaluate:\n",
        " - Whether NC geometry persists or evolves under low-rank adaptation.\n",
        " - If LoRA directions align with NC class mean directions (cosine similarity, projection overlap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiJGkQOuVcut"
      },
      "source": [
        "### TODO\n",
        "1. validate data collection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "620YzzMPOvDJ",
        "outputId": "754679bd-97a8-4f75-f742-ac1c80aa00c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "import psutil\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/experiment_data/' # save and retrieve data from here\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "# Reuse existing imports and configurations from Experiment 1+2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "debug = True # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "lr                  = 0.01 # verify with ablation\n",
        "batch_size          = 128\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4 # too high?\n",
        "\n",
        "# analysis parameters\n",
        "epochs              = 2\n",
        "ranks               = [1,2,4,8,16]\n",
        "lora_alpha          = 16\n",
        "lora_dropout        = 0.05\n",
        "RANK_THRESHOLDS     = [0.95, 0.99]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "i4R4RmRuPfmY"
      },
      "outputs": [],
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    all_features = []\n",
        "    class_features = [[] for _ in range(C)]\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Metrics']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # Collect all features for rank analysis\n",
        "            if computation == 'Mean':\n",
        "              all_features.append(h.detach())\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                # Collect class-specific features for SVD analysis\n",
        "                if computation == 'Mean':\n",
        "                    class_features[c].append(h_c.detach())\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Metrics':\n",
        "                    ## COV\n",
        "                    # update within-class cov\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "                graphs.mean = mean\n",
        "            loss /= sum(N)\n",
        "\n",
        "            # Feature rank analysis\n",
        "            all_features_tensor = torch.cat(all_features, dim=0)\n",
        "\n",
        "            # Compute feature rank using *torch SVD*\n",
        "            with torch.no_grad():\n",
        "                _, S, _ = torch.linalg.svd(all_features_tensor, full_matrices=False)\n",
        "                S = S[:100]  # Only keep top 100 components\n",
        "\n",
        "            # Calculate effective rank\n",
        "            normalized_sv = S / torch.sum(S)\n",
        "            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "            effective_ranks = {}\n",
        "            for thresh in RANK_THRESHOLDS:\n",
        "                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item() # convert tensor to scalar\n",
        "            graphs.feature_rank.append(effective_ranks)\n",
        "            graphs.singular_values.append(S.cpu().numpy())\n",
        "\n",
        "            # Class means SVD\n",
        "            U_M, S_M, V_M = torch.svd(M, some=True)\n",
        "            graphs.mean_singular_values.append(S_M.cpu().numpy())\n",
        "\n",
        "            # Class-wise SVD analysis\n",
        "            class_sv_lists = []\n",
        "            for c in range(C):\n",
        "                if len(class_features[c]) > 0:\n",
        "                    class_feat = torch.cat(class_features[c], dim=0).to(device)\n",
        "                    # Center the features\n",
        "                    class_feat = class_feat - mean[c].unsqueeze(0)\n",
        "                    # Compute SVD\n",
        "                    try:\n",
        "                        _, S_c, _ = torch.svd(class_feat, some=True)\n",
        "                        class_sv_lists.append(S_c.cpu().numpy())\n",
        "                    except:\n",
        "                        # Handle potential numerical issues\n",
        "                        class_sv_lists.append(np.zeros(min(class_feat.shape)))\n",
        "\n",
        "            graphs.class_singular_values.append(class_sv_lists)\n",
        "        elif computation == 'Metrics':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm, with LoRA weights\n",
        "    if hasattr(model.fc, 'lora_A'):\n",
        "        lora_A = model.fc.lora_A['default'].weight\n",
        "        lora_B = model.fc.lora_B['default'].weight\n",
        "        W_effective = model.fc.weight + (lora_B @ lora_A) # W_effective replaces W\n",
        "    else:\n",
        "        W_effective = model.fc.weight\n",
        "\n",
        "    M_norms = torch.norm(M_, dim=0)\n",
        "    W_norms = torch.norm(W_effective.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.linalg.svd(Sb, full_matrices=False)\n",
        "    eigvec = eigvec[:, :C-1]\n",
        "    eigval = eigval[:C-1]\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W_effective.T / torch.norm(W_effective.T, 'fro')\n",
        "    graphs.W_M_dist.append(torch.norm(normalized_W - normalized_M)**2) # .item()?\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W_effective.T/W_norms))\n",
        "\n",
        "\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    self.mean         = []\n",
        "    self.feature_rank = [] # stores dict [{'0.95': rank1, '0.99': rank2}\n",
        "    self.singular_values = []\n",
        "    self.mean_singular_values = []\n",
        "    self.class_singular_values = []\n",
        "\n",
        "    # Experiment 3 data\n",
        "    self.cos_sim = []\n",
        "    self.proj_score = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2SHzUkNIQq32"
      },
      "outputs": [],
      "source": [
        "def prepare_lora_model(pretrained_model, rank=4):\n",
        "    config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"fc\"],  # Apply LoRA to final layer\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "    model = get_peft_model(pretrained_model, config)\n",
        "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def compute_alignment_metrics(model, graphs, original_class_means, rank):\n",
        "    \"\"\"Calculate alignment between LoRA directions and original NC class means.\"\"\"\n",
        "    lora_A = model.base_model.model.fc.lora_A['default'].weight  # [rank, in_dim]\n",
        "    lora_B = model.base_model.model.fc.lora_B['default'].weight  # [out_dim, rank]\n",
        "    W_lora = lora_B @ lora_A  # Combined LoRA direction [out_dim, in_dim]\n",
        "\n",
        "    # current_class_means = torch.stack(graphs.mean).T\n",
        "\n",
        "    # Project original class means onto LoRA subspace\n",
        "    M = original_class_means.T.cpu().numpy()  # [in_dim, C]\n",
        "    U, _, _ = randomized_svd(W_lora.detach().cpu().numpy(), n_components=rank)\n",
        "\n",
        "    # Cosine similarity between LoRA directions and current class means\n",
        "    cos_sims = []\n",
        "    for c in range(M.shape[1]):\n",
        "        v = M[:, c]\n",
        "        for i in range(U.shape[1]):\n",
        "            u = U[:, i]\n",
        "            cos_sim = np.dot(u, v) / (np.linalg.norm(u)*np.linalg.norm(v)+1e-8)\n",
        "            cos_sims.append(cos_sim)\n",
        "\n",
        "    # Subspace projection score\n",
        "    proj = U.T @ M\n",
        "    proj_score = np.linalg.norm(proj)**2 / np.linalg.norm(M)**2\n",
        "\n",
        "    graphs.cos_sim.append(np.mean(cos_sims))\n",
        "    graphs.proj_score.append(proj_score)\n",
        "\n",
        "    return np.mean(cos_sims), proj_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "4jZ0_e2fQ5bJ"
      },
      "outputs": [],
      "source": [
        "def train_lora(model, criterion, device, train_loader, optimizer, epoch, rank):\n",
        "    model.train()\n",
        "    # model.fc.original.requires_grad_(False)  # redundant as peft freezes weights automatically\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'LoRA Rank {rank} Epoch {epoch}')\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'Loss': loss.item()})\n",
        "        pbar.update(1)\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "            break\n",
        "    pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yKTl8zXRCdf",
        "outputId": "0091c19c-7cbb-46bf-e82d-e43f40e8f353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:01<17:59,  2.92s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:00<17:45,  2.89s/it]\n",
            "LoRA Rank 1 Epoch 1:   6%|▌         | 22/390 [01:07<18:51,  3.08s/it, Loss=2.29]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [00:58<17:09,  2.79s/it]\n",
            "Analysis Metrics\tEpoch: 1 [16/390 (4%)]:   4%|▍         | 16/390 [00:45<17:17,  2.77s/it]"
          ]
        }
      ],
      "source": [
        "# ====================== EXECUTION ======================\n",
        "# checkpoint from Experiment 1+2\n",
        "checkpoint = torch.load('/content/drive/MyDrive/checkpoint.pth', weights_only=False)\n",
        "\n",
        "# dataset, optimizer setup from Experiment 1+2\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "## Run Experiment 3\n",
        "original_class_means = None  # To store initial NC geometry\n",
        "for rank in ranks:\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Load fresh pretrained model for each run\n",
        "    pretrained_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "    pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    pretrained_model.maxpool = nn.Identity()\n",
        "    # Register feature hook\n",
        "    class features:\n",
        "        pass\n",
        "    def hook(self, input, output):\n",
        "        features.value = input[0].clone()\n",
        "    pretrained_model.fc.register_forward_hook(hook)\n",
        "    pretrained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    pretrained_model.to(device)\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = prepare_lora_model(pretrained_model, rank=rank)\n",
        "    # optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=lr,\n",
        "                          momentum=momentum,\n",
        "                          weight_decay=weight_decay) # SGD, not Adam, for consistency with Exp1+2\n",
        "\n",
        "    # Initial NC state, from Exp1+2\n",
        "    graphs = Graphs()\n",
        "    analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch=0)\n",
        "    original_class_means = torch.stack(graphs.mean).T if original_class_means is None else original_class_means\n",
        "\n",
        "    # Experiment 3: Fine-tune pretrained model for 20 epochs under LoRA\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_lora(model, criterion, device, train_loader, optimizer, epoch, rank)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "        # Compute alignment metrics\n",
        "        compute_alignment_metrics(model, graphs, original_class_means, rank)\n",
        "\n",
        "        # results.append({\n",
        "        #     'rank': rank,\n",
        "        #     'epoch': epoch,\n",
        "        #     'Sw_invSb': graphs.Sw_invSb[-1],\n",
        "        #     'W_M_dist': graphs.W_M_dist[-1],\n",
        "        #     'NCC_mismatch': graphs.NCC_mismatch[-1],\n",
        "        #     'norm_M_CoV': graphs.norm_M_CoV[-1],\n",
        "        #     'norm_W_CoV': graphs.norm_W_CoV[-1],\n",
        "        #     'cos_M': graphs.cos_M[-1],\n",
        "        #     'cos_W': graphs.cos_W[-1],\n",
        "        #     'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "        #     'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "        #     'cos_sim': cos_sim,\n",
        "        #     'proj_score': proj_score\n",
        "        #     # 'test_acc': graphs.accuracy[-1]  # Assuming test loader available\n",
        "        # })\n",
        "\n",
        "    # Data save\n",
        "    df = pd.DataFrame({\n",
        "            'rank': [rank] * epochs,\n",
        "            'epoch': list(range(1, epochs + 1)),\n",
        "            # 'relative_epoch': 350+, 300+, etc.\n",
        "            'Sw_invSb': graphs.Sw_invSb,\n",
        "            'W_M_dist': graphs.W_M_dist,\n",
        "            'NCC_mismatch': graphs.NCC_mismatch,\n",
        "            'norm_M_CoV': graphs.norm_M_CoV,\n",
        "            'norm_W_CoV': graphs.norm_W_CoV,\n",
        "            'cos_M': graphs.cos_M,\n",
        "            'cos_W': graphs.cos_W,\n",
        "            'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "            'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "            'cos_sim': graphs.cos_sim,\n",
        "            'proj_score': graphs.proj_score\n",
        "            # 'test_acc': graphs.accuracy[-1]  # Assuming test loader available\n",
        "    })\n",
        "\n",
        "    # Save results per rank\n",
        "    df.to_csv(data_path + f'lora_rank_{rank}_results.csv', index=False)\n",
        "\n",
        "    # # Save singular values (optional)\n",
        "    # for rank in ranks:\n",
        "    #     np.save(data_path + f'rank_{rank}_feature_svs.npy', graphs.singular_values)\n",
        "    #     np.save(data_path + f'rank_{rank}_mean_svs.npy', graphs.mean_singular_values)\n",
        "    #     np.save(data_path + f'rank_{rank}_class_svs.npy', graphs.class_singular_values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDtYgy7VHUIp"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "df = pd.DataFrame(results)\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['cos_sim'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Mean Cosine Similarity')\n",
        "plt.title('LoRA Direction Alignment with Class Means')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "for rank in df['rank'].unique():\n",
        "    subset = df[df['rank'] == rank]\n",
        "    plt.plot(subset['epoch'], subset['NC1'], label=f'Rank {rank}')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NC1 (Tr{Sw Sb^-1})')\n",
        "plt.title('Neural Collapse Persistence During LoRA Tuning')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3b: LoRA Before NC\n",
        "Does the implementation of LoRA (restriction of parameter updates to low-rank) accelerate or disrupt NC behavior when implemented before collapse?"
      ],
      "metadata": {
        "id": "EnSfsYQ3esw4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RRJXSw-fBhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkK4hWwxPohr"
      },
      "source": [
        "## Baseline: Full Fine-Tuning\n",
        "\n",
        "\n",
        "From Sonnet; not debugged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r9U7NEePyuB"
      },
      "outputs": [],
      "source": [
        "# After your LoRA experiments, add:\n",
        "\n",
        "# Full fine-tuning baseline\n",
        "full_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "full_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "full_model.maxpool = nn.Identity()\n",
        "full_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "full_model.to(device)\n",
        "\n",
        "# Register hook\n",
        "full_model.fc.register_forward_hook(hook)\n",
        "\n",
        "# Save original weights for later comparison\n",
        "original_fc_weights = full_model.fc.weight.clone().detach()\n",
        "\n",
        "# For full fine-tuning, only train the classifier layer for fair comparison with LoRA\n",
        "for param in full_model.parameters():\n",
        "    param.requires_grad = False\n",
        "full_model.fc.weight.requires_grad = True\n",
        "full_model.fc.bias.requires_grad = True\n",
        "\n",
        "# Use same optimizer as LoRA experiments\n",
        "optimizer = optim.SGD(full_model.fc.parameters(), lr=0.01, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# Train and collect same metrics\n",
        "full_results = []\n",
        "graphs = Graphs()\n",
        "for epoch in range(1, 21):\n",
        "    train_full_ft(full_model, criterion, device, train_loader, optimizer, epoch)\n",
        "    analysis(graphs, full_model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "    # Compute weight update and its rank\n",
        "    current_weights = full_model.fc.weight.clone().detach()\n",
        "    weight_delta = current_weights - original_fc_weights\n",
        "    weight_delta_rank = compute_effective_rank(weight_delta)\n",
        "\n",
        "    # Current class means\n",
        "    current_class_means = torch.stack(graphs.mean).T\n",
        "\n",
        "    # Store results with same metrics as LoRA\n",
        "    full_results.append({\n",
        "        'method': 'full_ft',\n",
        "        'epoch': epoch,\n",
        "        'NC1': graphs.Sw_invSb[-1],\n",
        "        'NC2': graphs.norm_M_CoV[-1],\n",
        "        'NC3': graphs.W_M_dist[-1],\n",
        "        'NC4': graphs.NCC_mismatch[-1],\n",
        "        'weight_update_rank': weight_delta_rank,\n",
        "        'class_mean_change': torch.norm(current_class_means - original_class_means).item(),\n",
        "    })\n",
        "\n",
        "pd.DataFrame(full_results).to_csv('full_ft_results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXS38Z1ACKA6N90MVPxLrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}