{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V6JtNCJOzDu"
      },
      "source": [
        "# Experiment 3: LoRA and Frozen Backbone Probes\n",
        "- Take a pretrained classifier (e.g., trained ResNet).\n",
        "- Fine-tune only the final layer using LoRA, varying the rank (e.g., 1–16).\n",
        "- Evaluate:\n",
        " - Whether NC geometry persists or evolves under low-rank adaptation.\n",
        " - If LoRA directions align with NC class mean directions (cosine similarity, projection overlap)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiJGkQOuVcut"
      },
      "source": [
        "### TODO\n",
        "1. validate data collection (cos_sim, proj_score calculation)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "data_path = os.environ.get(\"DATA_PATH\", None)\n",
        "\n",
        "# If not set, use Drive or local Colab storage\n",
        "if data_path is None:\n",
        "    try:\n",
        "        # Uncomment below to use Drive\n",
        "        drive.mount('/content/drive')\n",
        "        data_path = '/content/drive/MyDrive/Colab Notebooks/experiment_data/'\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to mount Drive: {e}\")\n",
        "        data_path = '/content/data/'\n",
        "        print(\"Falling back to local storage\")\n",
        "\n",
        "os.makedirs(data_path, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e87EzJMy0U1p",
        "outputId": "2daae96b-2b90-48ba-af5b-e606dc1343b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "620YzzMPOvDJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Reuse existing imports and configurations from Experiment 1+2\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "debug = True # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "lr                  = 0.01 # verify with ablation\n",
        "batch_size          = 128\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4 # too high?\n",
        "\n",
        "# analysis parameters\n",
        "epochs              = 20\n",
        "ranks               = [1,2,4,8,16]\n",
        "lora_alpha          = 16\n",
        "lora_dropout        = 0.05\n",
        "RANK_THRESHOLDS     = [0.95, 0.99]\n",
        "nc_expected         = False\n",
        "\"\"\"If NC geometry already evidenced (ex. pretraining exhibited NC), compute_alignment_metrics compares LoRA update directions\n",
        "with original_class_means. If NC is still developing (Experiment 3b), compute_alignment_metrics compares LoRA update directions\n",
        "with current_class_means.\n",
        "\n",
        "Experiment 3A: LoRA's effect on post-NC. Do enforced low-rank updates reinforce, undo, or non-effect NC geometry?\n",
        "Experiment 3B: LoRA's effect on NC development. Do enforced low-rank updates accelerate, block, or non-effect NC development?\n",
        "\n",
        "Be careful to load correct pretrained model for the experiment being conducted. \"\"\"\n",
        "pretrained_model_path = data_path + 'experiment3_pretrained_models/checkpoint.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "i4R4RmRuPfmY"
      },
      "outputs": [],
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader, epoch):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    all_features = []\n",
        "    class_features = [[] for _ in range(C)]\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Metrics']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # Collect all features for rank analysis\n",
        "            if computation == 'Mean':\n",
        "              all_features.append(h.detach())\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                # Collect class-specific features for SVD analysis\n",
        "                if computation == 'Mean':\n",
        "                    class_features[c].append(h_c.detach())\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Metrics':\n",
        "                    ## COV\n",
        "                    # update within-class cov\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "            M = torch.stack(mean).T\n",
        "            graphs.mean = mean\n",
        "            loss /= sum(N)\n",
        "\n",
        "            # Feature rank analysis\n",
        "            all_features_tensor = torch.cat(all_features, dim=0)\n",
        "\n",
        "            # Compute feature rank using *torch SVD*\n",
        "            with torch.no_grad():\n",
        "                _, S, _ = torch.linalg.svd(all_features_tensor, full_matrices=False)\n",
        "                S = S[:100]  # Only keep top 100 components\n",
        "\n",
        "            # Calculate effective rank\n",
        "            normalized_sv = S / torch.sum(S)\n",
        "            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "            effective_ranks = {}\n",
        "            for thresh in RANK_THRESHOLDS:\n",
        "                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item() # convert tensor to scalar\n",
        "            graphs.feature_rank.append(effective_ranks)\n",
        "            graphs.singular_values.append(S.cpu().numpy())\n",
        "\n",
        "            # Class means SVD\n",
        "            U_M, S_M, V_M = torch.svd(M, some=True)\n",
        "            graphs.mean_singular_values.append(S_M.cpu().numpy())\n",
        "\n",
        "            # Class-wise SVD analysis\n",
        "            class_sv_lists = []\n",
        "            for c in range(C):\n",
        "                if len(class_features[c]) > 0:\n",
        "                    class_feat = torch.cat(class_features[c], dim=0).to(device)\n",
        "                    # Center the features\n",
        "                    class_feat = class_feat - mean[c].unsqueeze(0)\n",
        "                    # Compute SVD\n",
        "                    try:\n",
        "                        _, S_c, _ = torch.svd(class_feat, some=True)\n",
        "                        class_sv_lists.append(S_c.cpu().numpy())\n",
        "                    except:\n",
        "                        # Handle potential numerical issues\n",
        "                        class_sv_lists.append(np.zeros(min(class_feat.shape)))\n",
        "\n",
        "            graphs.class_singular_values.append(class_sv_lists)\n",
        "        elif computation == 'Metrics':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm, with LoRA weights\n",
        "    if hasattr(model.fc, 'lora_A'):\n",
        "        lora_A = model.fc.lora_A['default'].weight\n",
        "        lora_B = model.fc.lora_B['default'].weight\n",
        "        W_effective = model.fc.weight + (lora_B @ lora_A) # W_effective replaces W\n",
        "    else:\n",
        "        W_effective = model.fc.weight\n",
        "\n",
        "    M_norms = torch.norm(M_, dim=0)\n",
        "    W_norms = torch.norm(W_effective.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.linalg.svd(Sb, full_matrices=False)\n",
        "    eigvec = eigvec[:, :C-1]\n",
        "    eigval = eigval[:C-1]\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W_effective.T / torch.norm(W_effective.T, 'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W_effective.T/W_norms))\n",
        "\n",
        "\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    self.mean         = []\n",
        "    self.feature_rank = [] # stores dict [{'0.95': rank1, '0.99': rank2}\n",
        "    self.singular_values = []\n",
        "    self.mean_singular_values = []\n",
        "    self.class_singular_values = []\n",
        "\n",
        "    # Experiment 3 data\n",
        "    self.cos_sim = []\n",
        "    self.proj_score = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2SHzUkNIQq32"
      },
      "outputs": [],
      "source": [
        "def prepare_lora_model(pretrained_model, rank=4):\n",
        "    config = LoraConfig(\n",
        "        r=rank,\n",
        "        lora_alpha=lora_alpha,\n",
        "        target_modules=[\"fc\"],  # Apply LoRA to final layer\n",
        "        lora_dropout=lora_dropout,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "    model = get_peft_model(pretrained_model, config)\n",
        "    print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def compute_alignment_metrics(model, graphs, class_means, rank):\n",
        "    \"\"\"Calculate alignment between LoRA directions and NC class means.\"\"\"\n",
        "    lora_A = model.base_model.model.fc.lora_A['default'].weight  # [rank, in_dim]\n",
        "    lora_B = model.base_model.model.fc.lora_B['default'].weight  # [out_dim, rank]\n",
        "    W_lora = lora_B @ lora_A  # Combined LoRA direction [out_dim, in_dim]\n",
        "\n",
        "    # Project original class means onto LoRA subspace\n",
        "    M = class_means.T.cpu().numpy()  # [in_dim, C]\n",
        "    U, _, _ = randomized_svd(W_lora.detach().cpu().numpy(), n_components=rank)\n",
        "\n",
        "    # Cosine similarity between LoRA directions and current class means\n",
        "    cos_sims = []\n",
        "    for c in range(M.shape[1]):\n",
        "        v = M[:, c]\n",
        "        for i in range(U.shape[1]):\n",
        "            u = U[:, i]\n",
        "            cos_sim = np.dot(u, v) / (np.linalg.norm(u)*np.linalg.norm(v)+1e-8)\n",
        "            cos_sims.append(cos_sim)\n",
        "\n",
        "    # Subspace projection score\n",
        "    proj = U.T @ M\n",
        "    proj_score = np.linalg.norm(proj)**2 / np.linalg.norm(M)**2\n",
        "\n",
        "    graphs.cos_sim.append(float(np.mean(cos_sims)))\n",
        "    graphs.proj_score.append(float(proj_score))\n",
        "\n",
        "    return float(np.mean(cos_sims)), float(proj_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4jZ0_e2fQ5bJ"
      },
      "outputs": [],
      "source": [
        "def train_lora(model, criterion, device, train_loader, optimizer, epoch, rank):\n",
        "    model.train()\n",
        "    # model.fc.original.requires_grad_(False)  # redundant as peft freezes weights automatically\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), desc=f'LoRA Rank {rank} Epoch {epoch}')\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({'Loss': loss.item()})\n",
        "        pbar.update(1)\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "            break\n",
        "    pbar.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yKTl8zXRCdf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f854c24-8793-4b05-8faa-343b7463b228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Colab Notebooks/experiment_data/experiment3_pretrained_models/checkpoint.pth, pretrained to 30 epochs.\n",
            "Experiment 3B: LoRA's effect on NC development.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 522\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:18<22:59,  3.74s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:20,  3.63s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 0/390 [00:04<?, ?it/s, Loss=1.52]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 1/390 [00:04<31:30,  4.86s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   0%|          | 1/390 [00:08<31:30,  4.86s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 2/390 [00:08<26:04,  4.03s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 2/390 [00:11<26:04,  4.03s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 3/390 [00:11<24:15,  3.76s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 3/390 [00:15<24:15,  3.76s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 4/390 [00:15<24:59,  3.88s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|          | 4/390 [00:20<24:59,  3.88s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|▏         | 5/390 [00:20<25:50,  4.03s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   1%|▏         | 5/390 [00:23<25:50,  4.03s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 6/390 [00:23<24:32,  3.83s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 6/390 [00:27<24:32,  3.83s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 7/390 [00:27<23:42,  3.71s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 7/390 [00:31<23:42,  3.71s/it, Loss=1.23]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 8/390 [00:31<25:34,  4.02s/it, Loss=1.23]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 8/390 [00:35<25:34,  4.02s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 9/390 [00:35<25:10,  3.97s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   2%|▏         | 9/390 [00:38<25:10,  3.97s/it, Loss=1.6]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 10/390 [00:38<24:03,  3.80s/it, Loss=1.6]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 10/390 [00:42<24:03,  3.80s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 11/390 [00:42<23:18,  3.69s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 11/390 [00:47<23:18,  3.69s/it, Loss=1.19]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 12/390 [00:47<25:43,  4.08s/it, Loss=1.19]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 12/390 [00:50<25:43,  4.08s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 13/390 [00:50<24:21,  3.88s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   3%|▎         | 13/390 [00:54<24:21,  3.88s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▎         | 14/390 [00:54<23:43,  3.79s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▎         | 14/390 [00:58<23:43,  3.79s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 15/390 [00:58<23:46,  3.81s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 15/390 [01:02<23:46,  3.81s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 16/390 [01:02<25:10,  4.04s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 16/390 [01:06<25:10,  4.04s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 17/390 [01:06<24:01,  3.86s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   4%|▍         | 17/390 [01:09<24:01,  3.86s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 18/390 [01:09<23:11,  3.74s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 18/390 [01:14<23:11,  3.74s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 19/390 [01:14<24:22,  3.94s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▍         | 19/390 [01:18<24:22,  3.94s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 20/390 [01:18<24:25,  3.96s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 20/390 [01:21<24:25,  3.96s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 21/390 [01:21<23:26,  3.81s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   5%|▌         | 21/390 [01:25<23:26,  3.81s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 1:   6%|▌         | 22/390 [01:25<23:43,  3.87s/it, Loss=1.25]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:18<22:51,  3.72s/it]\n",
            "Analysis Metrics\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:12,  3.61s/it]\n",
            "\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.23]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 1/390 [00:03<22:39,  3.49s/it, Loss=1.23]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   0%|          | 1/390 [00:08<22:39,  3.49s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 2/390 [00:08<28:09,  4.36s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 2/390 [00:11<28:09,  4.36s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 3/390 [00:11<25:25,  3.94s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 3/390 [00:15<25:25,  3.94s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 4/390 [00:15<24:09,  3.76s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|          | 4/390 [00:19<24:09,  3.76s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|▏         | 5/390 [00:19<23:58,  3.74s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   1%|▏         | 5/390 [00:23<23:58,  3.74s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 6/390 [00:23<25:56,  4.05s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 6/390 [00:27<25:56,  4.05s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 7/390 [00:27<24:38,  3.86s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 7/390 [00:30<24:38,  3.86s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 8/390 [00:30<23:46,  3.73s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 8/390 [00:34<23:46,  3.73s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 9/390 [00:34<24:50,  3.91s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   2%|▏         | 9/390 [00:39<24:50,  3.91s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 10/390 [00:39<25:09,  3.97s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 10/390 [00:42<25:09,  3.97s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 11/390 [00:42<24:04,  3.81s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 11/390 [00:46<24:04,  3.81s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 12/390 [00:46<23:22,  3.71s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 12/390 [00:50<23:22,  3.71s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 13/390 [00:50<25:44,  4.10s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   3%|▎         | 13/390 [00:54<25:44,  4.10s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▎         | 14/390 [00:54<24:36,  3.93s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▎         | 14/390 [00:57<24:36,  3.93s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 15/390 [00:57<23:38,  3.78s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 15/390 [01:01<23:38,  3.78s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 16/390 [01:01<23:00,  3.69s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 16/390 [01:06<23:00,  3.69s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 17/390 [01:06<25:16,  4.07s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   4%|▍         | 17/390 [01:09<25:16,  4.07s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 18/390 [01:09<24:08,  3.89s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 18/390 [01:13<24:08,  3.89s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 19/390 [01:13<23:13,  3.76s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▍         | 19/390 [01:17<23:13,  3.76s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 20/390 [01:17<23:35,  3.82s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 20/390 [01:21<23:35,  3.82s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 21/390 [01:21<24:33,  3.99s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   5%|▌         | 21/390 [01:25<24:33,  3.99s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 1 Epoch 2:   6%|▌         | 22/390 [01:25<23:44,  3.87s/it, Loss=1.46]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:19<23:15,  3.78s/it]\n",
            "Analysis Metrics\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:36,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment data saved for rank 1\n",
            "Trainable parameters: 1044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:29,  3.66s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:15,  3.62s/it]\n",
            "\n",
            "LoRA Rank 2 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   0%|          | 0/390 [00:04<?, ?it/s, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   0%|          | 1/390 [00:04<32:06,  4.95s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   0%|          | 1/390 [00:08<32:06,  4.95s/it, Loss=1.53]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 2/390 [00:08<26:22,  4.08s/it, Loss=1.53]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 2/390 [00:11<26:22,  4.08s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 3/390 [00:11<24:37,  3.82s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 3/390 [00:15<24:37,  3.82s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 4/390 [00:15<25:04,  3.90s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|          | 4/390 [00:20<25:04,  3.90s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|▏         | 5/390 [00:20<26:09,  4.08s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   1%|▏         | 5/390 [00:23<26:09,  4.08s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 6/390 [00:23<24:46,  3.87s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 6/390 [00:27<24:46,  3.87s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 7/390 [00:27<23:54,  3.74s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 7/390 [00:32<23:54,  3.74s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 8/390 [00:32<25:46,  4.05s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 8/390 [00:35<25:46,  4.05s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 9/390 [00:35<25:13,  3.97s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   2%|▏         | 9/390 [00:39<25:13,  3.97s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 10/390 [00:39<24:13,  3.82s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 10/390 [00:42<24:13,  3.82s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 11/390 [00:42<23:29,  3.72s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 11/390 [00:47<23:29,  3.72s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 12/390 [00:47<25:46,  4.09s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 12/390 [00:51<25:46,  4.09s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 13/390 [00:51<24:34,  3.91s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   3%|▎         | 13/390 [00:54<24:34,  3.91s/it, Loss=1.27]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▎         | 14/390 [00:54<23:42,  3.78s/it, Loss=1.27]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▎         | 14/390 [00:58<23:42,  3.78s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 15/390 [00:58<24:00,  3.84s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 15/390 [01:03<24:00,  3.84s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 16/390 [01:03<25:04,  4.02s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 16/390 [01:06<25:04,  4.02s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 17/390 [01:06<23:58,  3.86s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   4%|▍         | 17/390 [01:10<23:58,  3.86s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▍         | 18/390 [01:10<23:15,  3.75s/it, Loss=1.5]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▍         | 18/390 [01:14<23:15,  3.75s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▍         | 19/390 [01:14<24:48,  4.01s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▍         | 19/390 [01:18<24:48,  4.01s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▌         | 20/390 [01:18<24:26,  3.96s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▌         | 20/390 [01:22<24:26,  3.96s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▌         | 21/390 [01:22<23:28,  3.82s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   5%|▌         | 21/390 [01:25<23:28,  3.82s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 2 Epoch 1:   6%|▌         | 22/390 [01:25<23:49,  3.88s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:38,  3.68s/it]\n",
            "Analysis Metrics\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:27,  3.65s/it]\n",
            "\n",
            "LoRA Rank 2 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.16]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   0%|          | 1/390 [00:03<22:23,  3.45s/it, Loss=1.16]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   0%|          | 1/390 [00:07<22:23,  3.45s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 2/390 [00:07<23:20,  3.61s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 2/390 [00:11<23:20,  3.61s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 3/390 [00:11<26:23,  4.09s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 3/390 [00:15<26:23,  4.09s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 4/390 [00:15<24:46,  3.85s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|          | 4/390 [00:18<24:46,  3.85s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|▏         | 5/390 [00:18<23:39,  3.69s/it, Loss=1.56]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   1%|▏         | 5/390 [00:22<23:39,  3.69s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 6/390 [00:22<24:46,  3.87s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 6/390 [00:27<24:46,  3.87s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 7/390 [00:27<25:21,  3.97s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 7/390 [00:30<25:21,  3.97s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 8/390 [00:30<24:23,  3.83s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 8/390 [00:34<24:23,  3.83s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 9/390 [00:34<23:37,  3.72s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   2%|▏         | 9/390 [00:38<23:37,  3.72s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 10/390 [00:38<25:41,  4.06s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 10/390 [00:42<25:41,  4.06s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 11/390 [00:42<24:52,  3.94s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 11/390 [00:46<24:52,  3.94s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 12/390 [00:46<23:56,  3.80s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 12/390 [00:49<23:56,  3.80s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 13/390 [00:49<23:21,  3.72s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   3%|▎         | 13/390 [00:54<23:21,  3.72s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▎         | 14/390 [00:54<25:26,  4.06s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▎         | 14/390 [00:57<25:26,  4.06s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 15/390 [00:57<24:16,  3.88s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 15/390 [01:01<24:16,  3.88s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 16/390 [01:01<23:24,  3.76s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 16/390 [01:05<23:24,  3.76s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 17/390 [01:05<23:52,  3.84s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   4%|▍         | 17/390 [01:09<23:52,  3.84s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▍         | 18/390 [01:09<24:34,  3.96s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▍         | 18/390 [01:13<24:34,  3.96s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▍         | 19/390 [01:13<23:34,  3.81s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▍         | 19/390 [01:16<23:34,  3.81s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▌         | 20/390 [01:16<22:52,  3.71s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▌         | 20/390 [01:21<22:52,  3.71s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▌         | 21/390 [01:21<24:36,  4.00s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   5%|▌         | 21/390 [01:24<24:36,  4.00s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 2 Epoch 2:   6%|▌         | 22/390 [01:24<23:41,  3.86s/it, Loss=1.43]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:16,  3.62s/it]\n",
            "Analysis Metrics\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:11,  3.61s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment data saved for rank 2\n",
            "Trainable parameters: 2088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:11,  3.61s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:41,  3.69s/it]\n",
            "\n",
            "LoRA Rank 4 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.45]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   0%|          | 1/390 [00:03<22:42,  3.50s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   0%|          | 1/390 [00:06<22:42,  3.50s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 2/390 [00:06<22:33,  3.49s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 2/390 [00:11<22:33,  3.49s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 3/390 [00:11<26:37,  4.13s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 3/390 [00:15<26:37,  4.13s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 4/390 [00:15<25:10,  3.91s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|          | 4/390 [00:18<25:10,  3.91s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|▏         | 5/390 [00:18<24:03,  3.75s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   1%|▏         | 5/390 [00:22<24:03,  3.75s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 6/390 [00:22<23:41,  3.70s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 6/390 [00:27<23:41,  3.70s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 7/390 [00:27<25:52,  4.05s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 7/390 [00:30<25:52,  4.05s/it, Loss=1.28]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 8/390 [00:30<24:47,  3.89s/it, Loss=1.28]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 8/390 [00:34<24:47,  3.89s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 9/390 [00:34<23:59,  3.78s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   2%|▏         | 9/390 [00:38<23:59,  3.78s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 10/390 [00:38<24:51,  3.92s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 10/390 [00:42<24:51,  3.92s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 11/390 [00:42<25:13,  3.99s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 11/390 [00:46<25:13,  3.99s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 12/390 [00:46<24:09,  3.83s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 12/390 [00:49<24:09,  3.83s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 13/390 [00:49<23:24,  3.73s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   3%|▎         | 13/390 [00:54<23:24,  3.73s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▎         | 14/390 [00:54<25:26,  4.06s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▎         | 14/390 [00:58<25:26,  4.06s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 15/390 [00:58<24:27,  3.91s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 15/390 [01:01<24:27,  3.91s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 16/390 [01:01<23:39,  3.79s/it, Loss=1.57]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 16/390 [01:05<23:39,  3.79s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 17/390 [01:05<22:58,  3.70s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   4%|▍         | 17/390 [01:10<22:58,  3.70s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▍         | 18/390 [01:10<25:10,  4.06s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▍         | 18/390 [01:13<25:10,  4.06s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▍         | 19/390 [01:13<24:01,  3.88s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▍         | 19/390 [01:16<24:01,  3.88s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▌         | 20/390 [01:16<23:11,  3.76s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▌         | 20/390 [01:21<23:11,  3.76s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▌         | 21/390 [01:21<23:46,  3.87s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   5%|▌         | 21/390 [01:25<23:46,  3.87s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 4 Epoch 1:   6%|▌         | 22/390 [01:25<23:47,  3.88s/it, Loss=1.32]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:13,  3.61s/it]\n",
            "Analysis Metrics\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:12,  3.61s/it]\n",
            "\n",
            "LoRA Rank 4 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   0%|          | 0/390 [00:04<?, ?it/s, Loss=1.41]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   0%|          | 1/390 [00:04<31:38,  4.88s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   0%|          | 1/390 [00:08<31:38,  4.88s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 2/390 [00:08<26:13,  4.05s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 2/390 [00:11<26:13,  4.05s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 3/390 [00:11<24:22,  3.78s/it, Loss=1.5]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 3/390 [00:15<24:22,  3.78s/it, Loss=1.54]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 4/390 [00:15<24:53,  3.87s/it, Loss=1.54]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|          | 4/390 [00:20<24:53,  3.87s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|▏         | 5/390 [00:20<25:48,  4.02s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   1%|▏         | 5/390 [00:23<25:48,  4.02s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 6/390 [00:23<24:30,  3.83s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 6/390 [00:27<24:30,  3.83s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 7/390 [00:27<23:42,  3.72s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 7/390 [00:31<23:42,  3.72s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 8/390 [00:31<25:36,  4.02s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 8/390 [00:35<25:36,  4.02s/it, Loss=1.18]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 9/390 [00:35<25:11,  3.97s/it, Loss=1.18]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   2%|▏         | 9/390 [00:39<25:11,  3.97s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 10/390 [00:39<24:09,  3.82s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 10/390 [00:42<24:09,  3.82s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 11/390 [00:42<23:27,  3.71s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 11/390 [00:47<23:27,  3.71s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 12/390 [00:47<25:47,  4.09s/it, Loss=1.5]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 12/390 [00:50<25:47,  4.09s/it, Loss=1.22]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 13/390 [00:50<24:31,  3.90s/it, Loss=1.22]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   3%|▎         | 13/390 [00:54<24:31,  3.90s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▎         | 14/390 [00:54<23:37,  3.77s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▎         | 14/390 [00:58<23:37,  3.77s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 15/390 [00:58<23:58,  3.84s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 15/390 [01:02<23:58,  3.84s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 16/390 [01:02<24:47,  3.98s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 16/390 [01:06<24:47,  3.98s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 17/390 [01:06<23:53,  3.84s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   4%|▍         | 17/390 [01:09<23:53,  3.84s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▍         | 18/390 [01:09<23:07,  3.73s/it, Loss=1.26]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▍         | 18/390 [01:14<23:07,  3.73s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▍         | 19/390 [01:14<24:46,  4.01s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▍         | 19/390 [01:18<24:46,  4.01s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▌         | 20/390 [01:18<24:24,  3.96s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▌         | 20/390 [01:21<24:24,  3.96s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▌         | 21/390 [01:21<23:24,  3.81s/it, Loss=1.38]\u001b[A\n",
            "LoRA Rank 4 Epoch 2:   5%|▌         | 21/390 [01:25<23:24,  3.81s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 4 Epoch 2:   6%|▌         | 22/390 [01:25<23:44,  3.87s/it, Loss=1.5]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:29,  3.66s/it]\n",
            "Analysis Metrics\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:36,  3.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment data saved for rank 4\n",
            "Trainable parameters: 4176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<21:58,  3.57s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:18<23:01,  3.74s/it]\n",
            "\n",
            "LoRA Rank 8 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.49]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   0%|          | 1/390 [00:03<24:03,  3.71s/it, Loss=1.49]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   0%|          | 1/390 [00:07<24:03,  3.71s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 2/390 [00:07<22:51,  3.53s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 2/390 [00:10<22:51,  3.53s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 3/390 [00:10<22:34,  3.50s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 3/390 [00:15<22:34,  3.50s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 4/390 [00:15<26:09,  4.07s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|          | 4/390 [00:18<26:09,  4.07s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|▏         | 5/390 [00:18<24:43,  3.85s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   1%|▏         | 5/390 [00:22<24:43,  3.85s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 6/390 [00:22<23:49,  3.72s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 6/390 [00:26<23:49,  3.72s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 7/390 [00:26<24:14,  3.80s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 7/390 [00:30<24:14,  3.80s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 8/390 [00:30<25:25,  3.99s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 8/390 [00:34<25:25,  3.99s/it, Loss=1.49]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 9/390 [00:34<24:19,  3.83s/it, Loss=1.49]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   2%|▏         | 9/390 [00:37<24:19,  3.83s/it, Loss=1.27]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 10/390 [00:37<23:25,  3.70s/it, Loss=1.27]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 10/390 [00:42<23:25,  3.70s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 11/390 [00:42<25:05,  3.97s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 11/390 [00:46<25:05,  3.97s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 12/390 [00:46<24:50,  3.94s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 12/390 [00:49<24:50,  3.94s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 13/390 [00:49<23:52,  3.80s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   3%|▎         | 13/390 [00:53<23:52,  3.80s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▎         | 14/390 [00:53<23:10,  3.70s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▎         | 14/390 [00:57<23:10,  3.70s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 15/390 [00:57<25:20,  4.05s/it, Loss=1.58]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 15/390 [01:01<25:20,  4.05s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 16/390 [01:01<24:09,  3.88s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 16/390 [01:04<24:09,  3.88s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 17/390 [01:04<23:23,  3.76s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   4%|▍         | 17/390 [01:08<23:23,  3.76s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▍         | 18/390 [01:08<23:24,  3.78s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▍         | 18/390 [01:13<23:24,  3.78s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▍         | 19/390 [01:13<24:52,  4.02s/it, Loss=1.25]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▍         | 19/390 [01:16<24:52,  4.02s/it, Loss=1.53]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▌         | 20/390 [01:16<23:45,  3.85s/it, Loss=1.53]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▌         | 20/390 [01:20<23:45,  3.85s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▌         | 21/390 [01:20<23:00,  3.74s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   5%|▌         | 21/390 [01:24<23:00,  3.74s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 8 Epoch 1:   6%|▌         | 22/390 [01:24<23:36,  3.85s/it, Loss=1.43]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:02,  3.59s/it]\n",
            "Analysis Metrics\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:18<22:55,  3.73s/it]\n",
            "\n",
            "LoRA Rank 8 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   0%|          | 1/390 [00:03<22:07,  3.41s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   0%|          | 1/390 [00:08<22:07,  3.41s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 2/390 [00:08<28:01,  4.33s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 2/390 [00:11<28:01,  4.33s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 3/390 [00:11<25:23,  3.94s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 3/390 [00:15<25:23,  3.94s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 4/390 [00:15<23:59,  3.73s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|          | 4/390 [00:19<23:59,  3.73s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|▏         | 5/390 [00:19<24:02,  3.75s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   1%|▏         | 5/390 [00:23<24:02,  3.75s/it, Loss=1.59]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 6/390 [00:23<25:50,  4.04s/it, Loss=1.59]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 6/390 [00:27<25:50,  4.04s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 7/390 [00:27<24:34,  3.85s/it, Loss=1.46]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 7/390 [00:30<24:34,  3.85s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 8/390 [00:30<23:34,  3.70s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 8/390 [00:34<23:34,  3.70s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 9/390 [00:34<24:56,  3.93s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   2%|▏         | 9/390 [00:38<24:56,  3.93s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 10/390 [00:38<25:07,  3.97s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 10/390 [00:42<25:07,  3.97s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 11/390 [00:42<24:05,  3.81s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 11/390 [00:45<24:05,  3.81s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 12/390 [00:45<23:17,  3.70s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 12/390 [00:50<23:17,  3.70s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 13/390 [00:50<25:15,  4.02s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   3%|▎         | 13/390 [00:54<25:15,  4.02s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▎         | 14/390 [00:54<24:25,  3.90s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▎         | 14/390 [00:57<24:25,  3.90s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 15/390 [00:57<23:25,  3.75s/it, Loss=1.5]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 15/390 [01:01<23:25,  3.75s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 16/390 [01:01<22:58,  3.69s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 16/390 [01:06<22:58,  3.69s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 17/390 [01:06<25:15,  4.06s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   4%|▍         | 17/390 [01:09<25:15,  4.06s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▍         | 18/390 [01:09<24:06,  3.89s/it, Loss=1.36]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▍         | 18/390 [01:13<24:06,  3.89s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▍         | 19/390 [01:13<23:18,  3.77s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▍         | 19/390 [01:17<23:18,  3.77s/it, Loss=1.5] \u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▌         | 20/390 [01:17<23:56,  3.88s/it, Loss=1.5]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▌         | 20/390 [01:21<23:56,  3.88s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▌         | 21/390 [01:21<24:33,  3.99s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   5%|▌         | 21/390 [01:24<24:33,  3.99s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 8 Epoch 2:   6%|▌         | 22/390 [01:24<23:40,  3.86s/it, Loss=1.41]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:25,  3.65s/it]\n",
            "Analysis Metrics\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:16<22:31,  3.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment data saved for rank 8\n",
            "Trainable parameters: 8352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:11,  3.61s/it]\n",
            "Analysis Metrics\tEpoch: 0 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:46,  3.70s/it]\n",
            "\n",
            "LoRA Rank 16 Epoch 1:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   0%|          | 0/390 [00:04<?, ?it/s, Loss=1.35]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   0%|          | 1/390 [00:04<31:42,  4.89s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   0%|          | 1/390 [00:08<31:42,  4.89s/it, Loss=1.54]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 2/390 [00:08<26:45,  4.14s/it, Loss=1.54]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 2/390 [00:11<26:45,  4.14s/it, Loss=1.66]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 3/390 [00:11<24:46,  3.84s/it, Loss=1.66]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 3/390 [00:15<24:46,  3.84s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 4/390 [00:15<23:47,  3.70s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|          | 4/390 [00:20<23:47,  3.70s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|▏         | 5/390 [00:20<26:37,  4.15s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   1%|▏         | 5/390 [00:23<26:37,  4.15s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 6/390 [00:23<25:04,  3.92s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 6/390 [00:27<25:04,  3.92s/it, Loss=1.62]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 7/390 [00:27<24:03,  3.77s/it, Loss=1.62]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 7/390 [00:31<24:03,  3.77s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 8/390 [00:31<24:34,  3.86s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 8/390 [00:35<24:34,  3.86s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 9/390 [00:35<25:32,  4.02s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   2%|▏         | 9/390 [00:39<25:32,  4.02s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 10/390 [00:39<24:23,  3.85s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 10/390 [00:42<24:23,  3.85s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 11/390 [00:42<23:38,  3.74s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 11/390 [00:47<23:38,  3.74s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 12/390 [00:47<25:11,  4.00s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 12/390 [00:51<25:11,  4.00s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 13/390 [00:51<24:55,  3.97s/it, Loss=1.32]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   3%|▎         | 13/390 [00:54<24:55,  3.97s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▎         | 14/390 [00:54<23:55,  3.82s/it, Loss=1.39]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▎         | 14/390 [00:58<23:55,  3.82s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 15/390 [00:58<23:13,  3.72s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 15/390 [01:03<23:13,  3.72s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 16/390 [01:03<25:37,  4.11s/it, Loss=1.55]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 16/390 [01:06<25:37,  4.11s/it, Loss=1.3] \u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 17/390 [01:06<24:21,  3.92s/it, Loss=1.3]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   4%|▍         | 17/390 [01:10<24:21,  3.92s/it, Loss=1.68]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▍         | 18/390 [01:10<23:28,  3.79s/it, Loss=1.68]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▍         | 18/390 [01:13<23:28,  3.79s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▍         | 19/390 [01:13<23:29,  3.80s/it, Loss=1.51]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▍         | 19/390 [01:18<23:29,  3.80s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▌         | 20/390 [01:18<24:50,  4.03s/it, Loss=1.43]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▌         | 20/390 [01:22<24:50,  4.03s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▌         | 21/390 [01:22<23:44,  3.86s/it, Loss=1.37]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   5%|▌         | 21/390 [01:25<23:44,  3.86s/it, Loss=1.35]\u001b[A\n",
            "LoRA Rank 16 Epoch 1:   6%|▌         | 22/390 [01:25<23:49,  3.89s/it, Loss=1.35]\n",
            "Analysis Mean\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:05,  3.59s/it]\n",
            "Analysis Metrics\tEpoch: 1 [21/390 (5%)]:   5%|▌         | 21/390 [01:17<22:48,  3.71s/it]\n",
            "\n",
            "LoRA Rank 16 Epoch 2:   0%|          | 0/390 [00:00<?, ?it/s]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   0%|          | 0/390 [00:03<?, ?it/s, Loss=1.52]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   0%|          | 1/390 [00:03<22:16,  3.44s/it, Loss=1.52]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   0%|          | 1/390 [00:07<22:16,  3.44s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 2/390 [00:07<25:52,  4.00s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 2/390 [00:12<25:52,  4.00s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 3/390 [00:12<26:32,  4.11s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 3/390 [00:15<26:32,  4.11s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 4/390 [00:15<24:50,  3.86s/it, Loss=1.44]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|          | 4/390 [00:19<24:50,  3.86s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|▏         | 5/390 [00:19<23:50,  3.71s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   1%|▏         | 5/390 [00:24<23:50,  3.71s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 6/390 [00:24<26:33,  4.15s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 6/390 [00:27<26:33,  4.15s/it, Loss=1.49]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 7/390 [00:27<25:05,  3.93s/it, Loss=1.49]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 7/390 [00:30<25:05,  3.93s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 8/390 [00:30<24:04,  3.78s/it, Loss=1.31]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 8/390 [00:34<24:04,  3.78s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 9/390 [00:34<24:08,  3.80s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   2%|▏         | 9/390 [00:39<24:08,  3.80s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 10/390 [00:39<26:00,  4.11s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 10/390 [00:43<26:00,  4.11s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 11/390 [00:43<24:38,  3.90s/it, Loss=1.33]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 11/390 [00:46<24:38,  3.90s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 12/390 [00:46<23:40,  3.76s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 12/390 [00:50<23:40,  3.76s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 13/390 [00:50<24:34,  3.91s/it, Loss=1.34]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   3%|▎         | 13/390 [00:54<24:34,  3.91s/it, Loss=1.4] \u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▎         | 14/390 [00:54<24:51,  3.97s/it, Loss=1.4]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▎         | 14/390 [00:58<24:51,  3.97s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 15/390 [00:58<23:50,  3.81s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 15/390 [01:01<23:50,  3.81s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 16/390 [01:01<23:07,  3.71s/it, Loss=1.45]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 16/390 [01:06<23:07,  3.71s/it, Loss=1.28]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 17/390 [01:06<25:14,  4.06s/it, Loss=1.28]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   4%|▍         | 17/390 [01:10<25:14,  4.06s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▍         | 18/390 [01:10<24:18,  3.92s/it, Loss=1.42]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▍         | 18/390 [01:13<24:18,  3.92s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▍         | 19/390 [01:13<23:16,  3.76s/it, Loss=1.41]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▍         | 19/390 [01:17<23:16,  3.76s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▌         | 20/390 [01:17<22:46,  3.69s/it, Loss=1.47]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▌         | 20/390 [01:21<22:46,  3.69s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▌         | 21/390 [01:21<24:48,  4.03s/it, Loss=1.29]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   5%|▌         | 21/390 [01:25<24:48,  4.03s/it, Loss=1.48]\u001b[A\n",
            "LoRA Rank 16 Epoch 2:   6%|▌         | 22/390 [01:25<23:49,  3.88s/it, Loss=1.48]\n",
            "Analysis Mean\tEpoch: 2 [21/390 (5%)]:   5%|▌         | 21/390 [01:15<22:09,  3.60s/it]\n",
            "Analysis Metrics\tEpoch: 2 [4/390 (1%)]:   1%|          | 4/390 [00:14<22:27,  3.49s/it]"
          ]
        }
      ],
      "source": [
        "# ====================== EXECUTION ======================\n",
        "# pretrained model from Experiment 1+2\n",
        "checkpoint = torch.load(pretrained_model_path, weights_only=False)\n",
        "\n",
        "pretrain_epochs = checkpoint['epoch']\n",
        "print(f\"Loading model from {pretrained_model_path}, pretrained to {pretrain_epochs} epochs.\")\n",
        "if nc_expected:\n",
        "  print(\"Experiment 3A: LoRA's effect on post-NC.\")\n",
        "else:\n",
        "  print(\"Experiment 3B: LoRA's effect on NC development.\")\n",
        "\n",
        "# dataset, optimizer setup from Experiment 1+2\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "## Run Experiment 3\n",
        "original_class_means = None  # To store initial NC geometry\n",
        "for rank in ranks:\n",
        "\n",
        "    # Load fresh pretrained model for each run\n",
        "    pretrained_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "    pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    pretrained_model.maxpool = nn.Identity()\n",
        "    class features: # Register feature hook\n",
        "        pass\n",
        "    def hook(self, input, output):\n",
        "        features.value = input[0].clone()\n",
        "    pretrained_model.fc.register_forward_hook(hook)\n",
        "    pretrained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    pretrained_model.to(device)\n",
        "\n",
        "    # Apply LoRA\n",
        "    model = prepare_lora_model(pretrained_model, rank=rank)\n",
        "    # optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=lr,\n",
        "                          momentum=momentum,\n",
        "                          weight_decay=weight_decay) # SGD, not Adam, for consistency with Exp1+2\n",
        "\n",
        "    # Initial NC state from pretraining (Epoch 0)\n",
        "    graphs = Graphs()\n",
        "    analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch=0)\n",
        "    graphs.cos_sim.append(None) # append None for indexing\n",
        "    graphs.proj_score.append(None)\n",
        "    original_class_means = torch.stack(graphs.mean).T if original_class_means is None else original_class_means\n",
        "\n",
        "    # Experiment 3: Fine-tune pretrained model for 20 epochs under LoRA\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_lora(model, criterion, device, train_loader, optimizer, epoch, rank)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "        # Compute alignment metrics\n",
        "        if nc_expected: # Experiment 3A: compare LoRA updates to original class means\n",
        "            compute_alignment_metrics(model, graphs, original_class_means, rank)\n",
        "        else: # Experiment 3B: since NC hasn't developed, use current_class_means for cos_sim\n",
        "            current_class_means = torch.stack(graphs.mean).T\n",
        "            compute_alignment_metrics(model, graphs, current_class_means, rank)\n",
        "\n",
        "    # Data save\n",
        "    df = pd.DataFrame({\n",
        "            'rank': [rank] * (epochs + 1),\n",
        "            'epoch': list(range(0, epochs + 1)),\n",
        "            # 'relative_epoch': 350+, 300+, etc.\n",
        "            'Sw_invSb': graphs.Sw_invSb,\n",
        "            'W_M_dist': graphs.W_M_dist,\n",
        "            'NCC_mismatch': graphs.NCC_mismatch,\n",
        "            'norm_M_CoV': graphs.norm_M_CoV,\n",
        "            'norm_W_CoV': graphs.norm_W_CoV,\n",
        "            'cos_M': graphs.cos_M,\n",
        "            'cos_W': graphs.cos_W,\n",
        "            'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "            'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "            'cos_sim': graphs.cos_sim,\n",
        "            'proj_score': graphs.proj_score\n",
        "            # 'test_acc': graphs.accuracy[-1]  # Assuming test loader available\n",
        "    })\n",
        "\n",
        "    # Save results per rank\n",
        "    df.to_csv(data_path + f'lora_rank_{rank}_results__pretrain_{pretrain_epochs}.csv', index=False)\n",
        "    print(f\"Experiment data saved for rank {rank}\")\n",
        "\n",
        "    # # Save singular values (optional)\n",
        "    # for rank in ranks:\n",
        "    #     np.save(data_path + f'rank_{rank}_feature_svs.npy', graphs.singular_values)\n",
        "    #     np.save(data_path + f'rank_{rank}_mean_svs.npy', graphs.mean_singular_values)\n",
        "    #     np.save(data_path + f'rank_{rank}_class_svs.npy', graphs.class_singular_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDtYgy7VHUIp"
      },
      "outputs": [],
      "source": [
        "# Append all dfs to one big results_df for visualization?\n",
        "# do later... MPU.\n",
        "\n",
        "\n",
        "# # Visualization\n",
        "# df = pd.DataFrame(results)\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# for rank in df['rank'].unique():\n",
        "#     subset = df[df['rank'] == rank]\n",
        "#     plt.plot(subset['epoch'], subset['cos_sim'], label=f'Rank {rank}')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Mean Cosine Similarity')\n",
        "# plt.title('LoRA Direction Alignment with Class Means')\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n",
        "# plt.figure(figsize=(12, 4))\n",
        "# for rank in df['rank'].unique():\n",
        "#     subset = df[df['rank'] == rank]\n",
        "#     plt.plot(subset['epoch'], subset['NC1'], label=f'Rank {rank}')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('NC1 (Tr{Sw Sb^-1})')\n",
        "# plt.title('Neural Collapse Persistence During LoRA Tuning')\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 3b: LoRA Before NC\n",
        "Does the implementation of LoRA (restriction of parameter updates to low-rank) accelerate or disrupt NC behavior when implemented before collapse?"
      ],
      "metadata": {
        "id": "EnSfsYQ3esw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Built in to above code"
      ],
      "metadata": {
        "id": "1RRJXSw-fBhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkK4hWwxPohr"
      },
      "source": [
        "## Baseline: Full Fine-Tuning\n",
        "\n",
        "\n",
        "From Sonnet; not debugged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r9U7NEePyuB"
      },
      "outputs": [],
      "source": [
        "# After your LoRA experiments, add:\n",
        "\n",
        "# Full fine-tuning baseline\n",
        "full_model = models.resnet18(pretrained=False, num_classes=C)\n",
        "full_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "full_model.maxpool = nn.Identity()\n",
        "full_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "full_model.to(device)\n",
        "\n",
        "# Register hook\n",
        "full_model.fc.register_forward_hook(hook)\n",
        "\n",
        "# Save original weights for later comparison\n",
        "original_fc_weights = full_model.fc.weight.clone().detach()\n",
        "\n",
        "# For full fine-tuning, only train the classifier layer for fair comparison with LoRA\n",
        "for param in full_model.parameters():\n",
        "    param.requires_grad = False\n",
        "full_model.fc.weight.requires_grad = True\n",
        "full_model.fc.bias.requires_grad = True\n",
        "\n",
        "# Use same optimizer as LoRA experiments\n",
        "optimizer = optim.SGD(full_model.fc.parameters(), lr=0.01, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# Train and collect same metrics\n",
        "full_results = []\n",
        "graphs = Graphs()\n",
        "for epoch in range(1, 21):\n",
        "    train_full_ft(full_model, criterion, device, train_loader, optimizer, epoch)\n",
        "    analysis(graphs, full_model, criterion_summed, device, C, analysis_loader, epoch)\n",
        "\n",
        "    # Compute weight update and its rank\n",
        "    current_weights = full_model.fc.weight.clone().detach()\n",
        "    weight_delta = current_weights - original_fc_weights\n",
        "    weight_delta_rank = compute_effective_rank(weight_delta)\n",
        "\n",
        "    # Current class means\n",
        "    current_class_means = torch.stack(graphs.mean).T\n",
        "\n",
        "    # Store results with same metrics as LoRA\n",
        "    full_results.append({\n",
        "        'method': 'full_ft',\n",
        "        'epoch': epoch,\n",
        "        'NC1': graphs.Sw_invSb[-1],\n",
        "        'NC2': graphs.norm_M_CoV[-1],\n",
        "        'NC3': graphs.W_M_dist[-1],\n",
        "        'NC4': graphs.NCC_mismatch[-1],\n",
        "        'weight_update_rank': weight_delta_rank,\n",
        "        'class_mean_change': torch.norm(current_class_means - original_class_means).item(),\n",
        "    })\n",
        "\n",
        "pd.DataFrame(full_results).to_csv('full_ft_results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1WK7SDNzcrYOTaAfLBSAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}