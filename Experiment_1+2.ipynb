{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_1%2B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsSM3c2mub8l"
      },
      "source": [
        "# Experiment 1+2: Tracking Neural Collapse and Feature Rank (1); Measuring Parameter Update Rank (2)\n",
        "\n",
        "## Experiment 1\n",
        "Train deep networks (e.g., ResNet18 on CIFAR-10) with cross-entropy loss.\n",
        "At each epoch, compute:\n",
        "- NC metrics (Papyan et al.): NC1-NC4\n",
        "- Rank of penultimate-layer feature matrix\n",
        "- Spectrum of singular values (SVD) of class means and classwise features\n",
        "\n",
        "Goal: empirically track the onset and strength of neural collapse vs. feature-space dimensionality.\n",
        "\n",
        "## Experiment 2\n",
        "- Save weight updates over training (e.g., via finite differences or gradients).\n",
        "- Measure their empirical rank and SVD spectrum.\n",
        "- Check if late-stage training updates concentrate into a low-rank subspace, coinciding with the emergence of NC.\n",
        "\n",
        "Goal: evaluate whether global parameter (weight) updates show low-rank at the same time as neural collapse. Ensure that the low-rank parameter updates occur in the classifier and the layer before.\n",
        "\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "- Finite differences are tracked in UpdateTracker, reflecting momentum effects. Gradients are not tracked in this implementation.\n",
        "\n",
        "- Checkpointing saves model state, but does not save graphs or UpdateTracker (Experiment 1+2 data) objects due to storage concerns.\n",
        "\n",
        "- UpdateTracker uses sliding window approach for SVD calculations due to memory issues. Window size can be toggled with max_history parameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNKXMPjQuc0C",
        "outputId": "2ce48dd1-40d3-4b44-be13-935553e5c6e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/experiment_data/'\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "debug = True # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "# Optimization hyperparameters\n",
        "lr_decay            = 0.1\n",
        "lr                  = 0.1 # modify?\n",
        "\n",
        "epochs              = 350\n",
        "epochs_lr_decay     = [epochs//3, epochs*2//3]\n",
        "\n",
        "batch_size          = 128\n",
        "\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4\n",
        "\n",
        "# analysis parameters\n",
        "RANK_THRESHOLDS      = [0.95, 0.99]\n",
        "tracked_layers      = ['fc.weight', 'layer4.1.conv2.weight']\n",
        "\n",
        "analysis_epochs          = [1,   2,    4,   5,    7,   8,   10,   11,\n",
        "                       13,  14,  16,  17,  19,  20,  22,  24,  27,   29,\n",
        "                       32,  35,  38,  42,  45,  50,  54,  59,  65,  71,   77,\n",
        "                       85,  92,  101, 110, 121, 132, 144, 158, 172, 188,  206,\n",
        "                       225, 245, 268, 293, 320, 340, 341, 342, 343, 344, 345,\n",
        "                       346, 347, 348, 349, 350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "R1cW3l0Du0Nq"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
        "        if data.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        if str(criterion) == 'CrossEntropyLoss()':\n",
        "          loss = criterion(out, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = torch.mean((torch.argmax(out,dim=1)==target).float()).item()\n",
        "\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(\n",
        "            'Train\\t\\tEpoch: {} [{}/{} ({:.0f}%)] \\t'\n",
        "            'Batch Loss: {:.6f} \\t'\n",
        "            'Batch Accuracy: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx,\n",
        "                len(train_loader),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item(),\n",
        "                accuracy))\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "          break\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9tYX9Fr9u3Ki"
      },
      "outputs": [],
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    all_features = []\n",
        "    class_features = [[] for _ in range(C)]\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Metrics']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # Collect all features for rank analysis\n",
        "            if computation == 'Mean':\n",
        "              all_features.append(h.cpu().detach())\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                # Collect class-specific features for SVD analysis\n",
        "                if computation == 'Mean':\n",
        "                  class_features[c].append(h_c.cpu().detach())\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Metrics':\n",
        "                    ## COV\n",
        "                    # update within-class cov\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "            loss /= sum(N)\n",
        "\n",
        "            # Feature rank analysis\n",
        "            all_features_tensor = torch.cat(all_features, dim=0)\n",
        "\n",
        "            # Compute feature rank using *randomized* SVD\n",
        "            U, S, _ = randomized_svd(all_features_tensor.cpu().numpy(), n_components=100)\n",
        "            S = torch.from_numpy(S).to(device)\n",
        "\n",
        "            # Calculate effective rank\n",
        "            normalized_sv = S / torch.sum(S)\n",
        "            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "            effective_ranks = {}\n",
        "            for thresh in RANK_THRESHOLDS:\n",
        "                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item() # convert tensor to scalar\n",
        "            graphs.feature_rank.append(effective_ranks)\n",
        "            graphs.singular_values.append(S.cpu().numpy())\n",
        "\n",
        "            # Class means SVD\n",
        "            U_M, S_M, V_M = torch.svd(M, some=True)\n",
        "            graphs.mean_singular_values.append(S_M.cpu().numpy())\n",
        "\n",
        "            # Class-wise SVD analysis\n",
        "            class_sv_lists = []\n",
        "            for c in range(C):\n",
        "                if len(class_features[c]) > 0:\n",
        "                    class_feat = torch.cat(class_features[c], dim=0)\n",
        "                    # Center the features\n",
        "                    class_feat = class_feat - mean[c].unsqueeze(0)\n",
        "                    # Compute SVD\n",
        "                    try:\n",
        "                        _, S_c, _ = torch.svd(class_feat, some=True)\n",
        "                        class_sv_lists.append(S_c.cpu().numpy())\n",
        "                    except:\n",
        "                        # Handle potential numerical issues\n",
        "                        class_sv_lists.append(np.zeros(min(class_feat.shape)))\n",
        "\n",
        "            graphs.class_singular_values.append(class_sv_lists)\n",
        "        elif computation == 'Metrics':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    W  = classifier.weight\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.cpu().double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.svd_lowrank(Sb, q=C-1)\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "reGN3Qbo81pz"
      },
      "outputs": [],
      "source": [
        "class UpdateTracker:\n",
        "    def __init__(self, model, max_history=30, max_components=100):\n",
        "        # Configuration\n",
        "        self.max_components = max_components\n",
        "        self.max_history = max_history  # Only keep recent updates to save memory\n",
        "        self.tracked_layers = tracked_layers\n",
        "        self.first_update = True\n",
        "\n",
        "        # Global tracking (all layers)\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.global_deltas = []  # Will use a sliding window approach\n",
        "\n",
        "        # Layer-specific tracking\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "        self.layer_deltas = {n: [] for n in self.tracked_layers}\n",
        "\n",
        "    def compute_update_rank(self, model):\n",
        "        # Initialize return values\n",
        "        effective_ranks = {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "        effective_ranks_layer = {n: {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "                               for n in self.tracked_layers}\n",
        "        sv = np.full(self.max_components, np.nan)\n",
        "        layer_svs = {n: np.full(min(self.max_components, model.state_dict()[n].numel()), np.nan)\n",
        "                    for n in self.tracked_layers}\n",
        "\n",
        "        # Skip if first epoch\n",
        "        if self.first_update:\n",
        "            print(\"First epoch - initializing trackers (no updates yet)\")\n",
        "            self.first_update = False\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Global computation (all layers) ===\n",
        "        try:\n",
        "            # Compute current update\n",
        "            delta = []\n",
        "            for n, p in model.named_parameters():\n",
        "                param_delta = (p.detach() - self.prev_weights[n]).flatten().cpu().numpy()\n",
        "                delta.append(param_delta)\n",
        "            delta_vector = np.concatenate(delta)\n",
        "\n",
        "            # Check if update is too small\n",
        "            update_norm = np.linalg.norm(delta_vector)\n",
        "            if update_norm < 1e-10:\n",
        "                print(\"Warning: Update magnitude very small, skipping SVD\")\n",
        "                return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "            # Add to history (with memory management)\n",
        "            self.global_deltas.append(delta_vector)\n",
        "            if len(self.global_deltas) > self.max_history:\n",
        "                self.global_deltas.pop(0)  # Remove oldest update\n",
        "\n",
        "            # Perform SVD on recent history\n",
        "            delta_matrix = np.vstack(self.global_deltas)\n",
        "            n_components = min(delta_matrix.shape[0], self.max_components)\n",
        "\n",
        "            if delta_matrix.shape[0] > 1:\n",
        "                # Use randomized SVD for efficiency\n",
        "                U, S, _ = randomized_svd(delta_matrix, n_components=n_components)\n",
        "                explained_variance = np.cumsum(S**2) / np.sum(S**2)\n",
        "\n",
        "                # Calculate effective ranks\n",
        "                for thresh in RANK_THRESHOLDS:\n",
        "                    effective_ranks[str(thresh)] = np.sum(explained_variance < thresh) + 1\n",
        "                sv = S.copy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in global SVD computation: {e}\")\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Layer-specific computation ===\n",
        "        for layer_name in self.tracked_layers:\n",
        "            try:\n",
        "                # Compute current layer update\n",
        "                current_param = model.state_dict()[layer_name]\n",
        "                prev_param = self.layer_prev_weights[layer_name]\n",
        "                layer_delta = (current_param - prev_param).flatten().cpu().numpy().astype(np.float32)  # Use float32 to save memory\n",
        "\n",
        "                # Check if layer update is too small\n",
        "                layer_update_norm = np.linalg.norm(layer_delta)\n",
        "                if layer_update_norm < 1e-10:\n",
        "                    continue\n",
        "\n",
        "                # Add to layer history (with memory management)\n",
        "                self.layer_deltas[layer_name].append(layer_delta)\n",
        "                if len(self.layer_deltas[layer_name]) > self.max_history:\n",
        "                    self.layer_deltas[layer_name].pop(0)  # Remove oldest update\n",
        "\n",
        "                # Perform SVD on recent history for this layer\n",
        "                if len(self.layer_deltas[layer_name]) > 1:\n",
        "                    layer_delta_matrix = np.vstack(self.layer_deltas[layer_name])\n",
        "                    layer_n_components = min(layer_delta_matrix.shape[0], self.max_components)\n",
        "\n",
        "                    # Use randomized SVD for efficiency\n",
        "                    _, layer_S, _ = randomized_svd(layer_delta_matrix, n_components=layer_n_components)\n",
        "                    layer_explained_variance = np.cumsum(layer_S**2) / np.sum(layer_S**2)\n",
        "\n",
        "                    # Calculate effective ranks for this layer\n",
        "                    for thresh in RANK_THRESHOLDS:\n",
        "                        effective_ranks_layer[layer_name][str(thresh)] = np.sum(layer_explained_variance < thresh) + 1\n",
        "                    layer_svs[layer_name] = layer_S.copy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in layer-specific SVD for {layer_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Update previous weights for next iteration\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "\n",
        "        return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    # Experiment 1 data\n",
        "    self.feature_rank = [] # stores dict [{'0.95': rank1, '0.99': rank2}\n",
        "    self.singular_values = []\n",
        "    self.mean_singular_values = []\n",
        "    self.class_singular_values = []\n",
        "\n",
        "    # Global Experiment 2 data\n",
        "    self.update_ranks = []\n",
        "    self.update_spectra = []  # Global singular values -- None for most epochs\n",
        "\n",
        "    # Layer-specific Experiment 2 data\n",
        "    self.layer_singular_values = {n: [] for n in tracked_layers} #  -- None for most epochs\n",
        "    self.layer_update_ranks = {n: [] for n in tracked_layers}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "tsXs-Zw35TW2",
        "outputId": "d3342f32-4459-41f1-8432-7072354f7d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train\t\tEpoch: 1 [5/390 (1%)] \tBatch Loss: 7.182210 \tBatch Accuracy: 0.125000:   1%|▏         | 5/390 [00:51<1:06:19, 10.34s/it]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4dad382b6683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-fd8d4dbccbef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, device, num_classes, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=False, num_classes=C)\n",
        "model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)\n",
        "model.maxpool = nn.Identity()\n",
        "model = model.to(device)\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                              milestones=epochs_lr_decay,\n",
        "                                              gamma=lr_decay)\n",
        "tracker = UpdateTracker(model)\n",
        "graphs = Graphs()\n",
        "cur_epochs = []\n",
        "start_epoch = 1\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, 100):\n",
        "    train(model, criterion, device, C, train_loader, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "\n",
        "    # === Experiment 2 data collection ===\n",
        "    update_epochs = list(range(1, epoch + 1)) # used for graphs, data saving\n",
        "    global_rank, global_sv, layer_ranks, layer_svs = tracker.compute_update_rank(model)\n",
        "    graphs.update_ranks.append(global_rank)\n",
        "    for layer_name in tracked_layers:\n",
        "        graphs.layer_update_ranks[layer_name].append(layer_ranks[layer_name])\n",
        "\n",
        "    # store spectra only for analysis epochs\n",
        "    if epoch in analysis_epochs:\n",
        "        graphs.update_spectra.append(global_sv)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(layer_svs[layer_name])\n",
        "    else:\n",
        "        # Store None as placeholder to maintain index alignment with epoch numbers\n",
        "        graphs.update_spectra.append(None)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(None)\n",
        "            # graphs.layer_update_ranks[layer_name].append({'0.95': np.nan, '0.99': np.nan})\n",
        "\n",
        "\n",
        "    if epoch in analysis_epochs:\n",
        "        cur_epochs.append(epoch)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader)\n",
        "\n",
        "        # === Graphs ===\n",
        "        ## Experiment 0: NC Figures\n",
        "        plt.figure(1)\n",
        "        plt.semilogy(cur_epochs, graphs.reg_loss)\n",
        "        plt.legend(['Loss + Weight Decay'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Loss')\n",
        "\n",
        "        plt.figure(2)\n",
        "        plt.plot(cur_epochs, 100*(1 - np.array(graphs.accuracy)))\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Training Error (%)')\n",
        "        plt.title('Training Error')\n",
        "\n",
        "        plt.figure(3)\n",
        "        plt.semilogy(cur_epochs, graphs.Sw_invSb)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Tr{Sw Sb^-1}')\n",
        "        plt.title('NC1: Activation Collapse')\n",
        "\n",
        "        plt.figure(4)\n",
        "        plt.plot(cur_epochs, graphs.norm_M_CoV)\n",
        "        plt.plot(cur_epochs, graphs.norm_W_CoV)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Std/Avg of Norms')\n",
        "        plt.title('NC2: Equinorm')\n",
        "\n",
        "        plt.figure(5)\n",
        "        plt.plot(cur_epochs, graphs.cos_M)\n",
        "        plt.plot(cur_epochs, graphs.cos_W)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Avg|Cos + 1/(C-1)|')\n",
        "        plt.title('NC2: Maximal Equiangularity')\n",
        "\n",
        "        plt.figure(6)\n",
        "        plt.plot(cur_epochs,graphs.W_M_dist)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('||W^T - H||^2')\n",
        "        plt.title('NC3: Self Duality')\n",
        "\n",
        "        plt.figure(7)\n",
        "        plt.plot(cur_epochs,graphs.NCC_mismatch)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Proportion Mismatch from NCC')\n",
        "        plt.title('NC4: Convergence to NCC')\n",
        "\n",
        "        ## Experiment 1 Figures\n",
        "        plt.figure(8)\n",
        "        plt.plot(cur_epochs, [x['0.95'] for x in graphs.feature_rank], label='95% Threshold')\n",
        "        plt.plot(cur_epochs, [x['0.99'] for x in graphs.feature_rank], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.title('Feature Space Dimensionality')\n",
        "\n",
        "        plt.figure(9)\n",
        "        # Plot singular value spectra for selected epochs\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx < len(graphs.singular_values):\n",
        "                sv = graphs.singular_values[epoch_idx]\n",
        "                plt.semilogy(range(1, len(sv)+1), sv, label=f'Epoch {cur_epochs[epoch_idx]}')\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Singular Value')\n",
        "        plt.title('Feature Matrix Spectrum Evolution')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.figure(10)\n",
        "        # Plot class means singular values for the last epoch\n",
        "        if len(graphs.mean_singular_values) > 0:\n",
        "            sv_means = graphs.mean_singular_values[-1]\n",
        "            plt.bar(range(1, len(sv_means)+1), sv_means)\n",
        "            plt.xlabel('Index')\n",
        "            plt.ylabel('Singular Value')\n",
        "            plt.title('Class Means Singular Value Spectrum')\n",
        "\n",
        "        # Correlation between NC metrics and feature rank\n",
        "        plt.figure(11)\n",
        "        plt.scatter([x['0.95'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "            label='95% Threshold')\n",
        "        plt.scatter([x['0.99'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "                    label='99% Threshold', marker='x')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Feature Rank')\n",
        "        plt.ylabel('Tr{Sw Sb^-1} (NC1)')\n",
        "        plt.title('Neural Collapse vs. Feature Dimensionality')\n",
        "\n",
        "        plt.figure(12)\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last epoch\n",
        "        colors = ['b', 'g', 'r']\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx >= len(graphs.class_singular_values):\n",
        "                continue\n",
        "            all_class_svs = graphs.class_singular_values[epoch_idx]\n",
        "\n",
        "            # Aggregate top-5 SVs across classes\n",
        "            top5_svs = np.array([sv[:5] for sv in all_class_svs if len(sv) >= 5])\n",
        "            mean_top5 = np.mean(top5_svs, axis=0)\n",
        "            std_top5 = np.std(top5_svs, axis=0)\n",
        "\n",
        "            plt.errorbar(\n",
        "                range(1, 6), mean_top5, yerr=std_top5,\n",
        "                label=f'Epoch {cur_epochs[epoch_idx]}', color=colors[i], alpha=0.7\n",
        "            )\n",
        "\n",
        "        plt.xlabel('Singular Value Index (Top 5)')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.yscale('log')\n",
        "        plt.title('Experiment 1: Class-Wise Feature Singular Values (Mean ± Std)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        ## Experiment 2 Figures\n",
        "        plt.figure(13)\n",
        "        # Filter out NaNs from IPCA early epochs\n",
        "        valid_global_95 = [\n",
        "            (e, r['0.95']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.95' in r and not np.isnan(r['0.95'])\n",
        "        ]\n",
        "        valid_global_99 = [\n",
        "            (e, r['0.99']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.99' in r and not np.isnan(r['0.99'])\n",
        "        ]\n",
        "        plt.plot([e for e, _ in valid_global_95], [v for _, v in valid_global_95], label='95% Threshold')\n",
        "        plt.plot([e for e, _ in valid_global_99], [v for _, v in valid_global_99], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank of Parameter Updates')\n",
        "        plt.title('Experiment 2: Parameter Update Rank')\n",
        "\n",
        "        plt.figure(14)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs\n",
        "            valid_95 = [\n",
        "                (e, r['0.95']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.95' in r and not np.isnan(r['0.95'])\n",
        "            ]\n",
        "            valid_99 = [\n",
        "                (e, r['0.99']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.99' in r and not np.isnan(r['0.99'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_95], [v for _, v in valid_95],\n",
        "                    label=f'{layer_name} (95%)')\n",
        "            plt.plot([e for e, _ in valid_99], [v for _, v in valid_99],\n",
        "                    label=f'{layer_name} (99%)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks')\n",
        "\n",
        "        plt.figure(15)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter out None values and get only epochs in analysis_epochs\n",
        "            valid_svs = [\n",
        "                (e, sv) for e, sv in zip(update_epochs, graphs.layer_singular_values[layer_name])\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "\n",
        "            if valid_svs:\n",
        "                # If we have few valid epochs, show all of them\n",
        "                if len(valid_svs) <= 3:\n",
        "                    selected_indices = list(range(len(valid_svs)))\n",
        "                else:\n",
        "                    # Otherwise show first, middle, last\n",
        "                    selected_indices = [0, len(valid_svs)//2, -1]\n",
        "\n",
        "                for idx in selected_indices:\n",
        "                    e, sv = valid_svs[idx]\n",
        "                    plt.semilogy(\n",
        "                        range(1, len(sv)+1), sv,\n",
        "                        label=f'{layer_name} (Epoch {e})'\n",
        "                    )\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Update Singular Values')\n",
        "\n",
        "        plt.figure(16)\n",
        "        # For the cumulative variance plot, use the last available spectrum for each layer\n",
        "        for layer_name in tracked_layers:\n",
        "            # Get non-None values\n",
        "            valid_svs = [\n",
        "                sv for sv in graphs.layer_singular_values[layer_name]\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "\n",
        "            if valid_svs:\n",
        "                # Use the last available spectrum\n",
        "                sv = valid_svs[-1]\n",
        "                explained_variance = np.cumsum(sv**2) / np.sum(sv**2)\n",
        "                plt.plot(explained_variance, label=f'{layer_name}')\n",
        "\n",
        "        for thresh in RANK_THRESHOLDS:\n",
        "            plt.axhline(thresh, color='k' if thresh==0.99 else 'gray',\n",
        "                        linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Cumulative Explained Variance')\n",
        "        plt.legend()\n",
        "        plt.title('Cumulative Variance Explained by Layer Updates')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # === Data Save ===\n",
        "    if epoch in [1, 2, 3, 5, 10, 15, 30, 50, 100, 150, 200, 250, 300, epochs]:\n",
        "        # Data from analysis epochs\n",
        "        df = pd.DataFrame({\n",
        "            'epoch': cur_epochs,\n",
        "            'Sw_invSb': graphs.Sw_invSb,\n",
        "            'W_M_dist': graphs.W_M_dist,\n",
        "            'NCC_mismatch': graphs.NCC_mismatch,\n",
        "            'norm_M_CoV': graphs.norm_M_CoV,\n",
        "            'norm_W_CoV': graphs.norm_W_CoV,\n",
        "            'cos_M': graphs.cos_M,\n",
        "            'cos_W': graphs.cos_W,\n",
        "            # Experiment 1\n",
        "            'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "            'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "            # Experiment 2\n",
        "            'update_rank_95': [graphs.update_ranks[e-1]['0.95'] for e in cur_epochs],\n",
        "            'update_rank_99': [graphs.update_ranks[e-1]['0.99'] for e in cur_epochs],\n",
        "            'fc_update_rank_95': [graphs.layer_update_ranks['fc.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "            'fc_update_rank_99': [graphs.layer_update_ranks['fc.weight'][e-1]['0.99'] for e in cur_epochs],\n",
        "            'conv_update_rank_95': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "            'conv_update_rank_99': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.99'] for e in cur_epochs]\n",
        "        })\n",
        "        df.to_csv(data_path + 'experiment1_experiment2_data.csv', index=False)\n",
        "\n",
        "        # Save Experiment 1 singular values\n",
        "        np.save(data_path + 'feature_svs.npy', graphs.singular_values)\n",
        "        np.save(data_path + 'mean_svs.npy', graphs.mean_singular_values)\n",
        "        np.save(data_path + 'class_svs.npy', np.array(graphs.class_singular_values, dtype=object))\n",
        "\n",
        "        # Save Experiment 2 data for all epochs\n",
        "        df_exp2 = pd.DataFrame({\n",
        "            'epoch': update_epochs,\n",
        "            'update_rank_95': [x.get('0.95', np.nan) for x in graphs.update_ranks],\n",
        "            'update_rank_99': [x.get('0.99', np.nan) for x in graphs.update_ranks],\n",
        "            'fc_update_rank_95': [x.get('0.95', np.nan) for x in graphs.layer_update_ranks['fc.weight']],\n",
        "            'fc_update_rank_99': [x.get('0.99', np.nan) for x in graphs.layer_update_ranks['fc.weight']],\n",
        "            'conv_update_rank_95': [x.get('0.95', np.nan) for x in graphs.layer_update_ranks['layer4.1.conv2.weight']],\n",
        "            'conv_update_rank_99': [x.get('0.99', np.nan) for x in graphs.layer_update_ranks['layer4.1.conv2.weight']]\n",
        "        })\n",
        "        df_exp2.to_csv(data_path + 'experiment2_data.csv', index=False)\n",
        "\n",
        "        # Experiment 2 singular values (Note: saved as objects due to jagged arrays)\n",
        "        valid_update_spectra = [sv for sv in graphs.update_spectra if sv is not None]\n",
        "        np.save(data_path + 'global_update_spectra.npy', np.array(valid_update_spectra, dtype=object))\n",
        "\n",
        "        for layer_name in tracked_layers:\n",
        "            valid_layer_svs = [sv for sv in graphs.layer_singular_values[layer_name] if sv is not None]\n",
        "            np.save(data_path + f'{layer_name}_singular_values.npy', np.array(valid_layer_svs, dtype=object))\n",
        "\n",
        "        # Save figures\n",
        "        figures_to_save = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]  # All\n",
        "        for fig_num in figures_to_save:\n",
        "            plt.figure(fig_num)\n",
        "            plt.savefig(data_path + f'figure_{fig_num}_epoch_{epoch}.pdf', bbox_inches='tight', dpi=300)\n",
        "            plt.close(fig_num)\n",
        "\n",
        "        print(f\"Experiment data saved at epoch {epoch}\")\n",
        "\n",
        "        # memory usage check\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_info = process.memory_info()\n",
        "        print(f\"Epoch {epoch}: Memory usage: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
        "        print(f\"Tracker deltas: {len(tracker.global_deltas)} updates stored\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "            print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "        # save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            # 'graphs': graphs,\n",
        "            # 'tracker': tracker, # too large; checkpoint for Experiment 3\n",
        "            'cur_epochs': cur_epochs\n",
        "        }\n",
        "        torch.save(checkpoint, data_path + 'checkpoint.pth')\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "        gc.collect()\n",
        "        # torch.cuda.empty_cache() # if using GPU\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLayepRjgk8QtFIGBAaZeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}