{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_1%2B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsSM3c2mub8l"
      },
      "source": [
        "# Experiment 1+2: Tracking Neural Collapse and Feature Rank (1); Measuring Parameter Update Rank (2)\n",
        "\n",
        "## Experiment 1\n",
        "Train deep networks (e.g., ResNet18 on CIFAR-10) with cross-entropy loss.\n",
        "At each epoch, compute:\n",
        "- NC metrics (Papyan et al.): NC1-NC4\n",
        "- Rank of penultimate-layer feature matrix\n",
        "- Spectrum of singular values (SVD) of class means and classwise features\n",
        "\n",
        "Goal: empirically track the onset and strength of neural collapse vs. feature-space dimensionality.\n",
        "\n",
        "## Experiment 2\n",
        "- Save weight updates over training (e.g., via finite differences or gradients).\n",
        "- Measure their empirical rank and SVD spectrum.\n",
        "- Check if late-stage training updates concentrate into a low-rank subspace, coinciding with the emergence of NC.\n",
        "\n",
        "Goal: evaluate whether global parameter (weight) updates show low-rank at the same time as neural collapse. Ensure that the low-rank parameter updates occur in the classifier and the layer before.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzmtsmJtkp4S"
      },
      "source": [
        "NOTE: Finite differences are tracked in UpdateTracker, reflecting momentum effects. Gradients were NOT tracked separately in this iteration.\n",
        "\n",
        "NOTE: Checkpointing only works for Experiment 1. Tracker object stores global deltas for parameter update rank analysis (~15 GB by Epoch 350), which is unreasonable to checkpoint.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Todo:\n",
        "1. Full dry run\n",
        "-> prune epoch_list\n",
        "\n",
        "2. Data checks\n",
        "-> I am collecting all the necessary data? I am collecting the data *correctly* (calculating what I say I'm calculating)\n",
        "\n",
        "-> I am building the necessary graphs?\n",
        "\n",
        "-> I am ready to process the data? contingency plan once results are in\n",
        "\n",
        "3. Data saving\n",
        "-> Parameter update rank tracking looks off. Why?\n",
        "\"Likely Cause: The UpdateTracker computes ranks using randomized_svd with n_components=100, but for early epochs:\n",
        "Parameter updates are small/noisy, leading to unstable SVD.\n",
        "n_components may be too high relative to the actual rank, causing numerical artifacts.\"\n",
        "Also because using modified epochs (only 20 batches). *Test with debug=False run. *\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4. Implement randomizedSVD, sliding-window UpdateTracker.\n",
        "-> can save all deltas for key epochs\n",
        "\n",
        "5/1: Validating with dry run.\n",
        "\n",
        "\n",
        "Goals:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNKXMPjQuc0C",
        "outputId": "8d2eaf45-ee9e-472d-ab52-0bf31d54e3c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "overwrite_checkpoint = True\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "debug = True # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "# Optimization hyperparameters\n",
        "lr_decay            = 0.1\n",
        "lr                  = 0.1 # modify?\n",
        "\n",
        "epochs              = 350\n",
        "epochs_lr_decay     = [epochs//3, epochs*2//3]\n",
        "\n",
        "batch_size          = 128\n",
        "\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4\n",
        "\n",
        "# analysis parameters\n",
        "RANK_THRESHOLDS      = [0.95, 0.99]\n",
        "tracked_layers      = ['fc.weight', 'layer4.1.conv2.weight']\n",
        "\n",
        "epoch_list          = [1,   2,    4,   5,    7,   8,   10,   11,\n",
        "                       13,  14,  16,  17,  19,  20,  22,  24,  27,   29,\n",
        "                       32,  35,  38,  42,  45,  50,  54,  59,  65,  71,   77,\n",
        "                       85,  92,  101, 110, 121, 132, 144, 158, 172, 188,  206,\n",
        "                       225, 245, 268, 293, 320, 340, 341, 342, 343, 344, 345,\n",
        "                       346, 347, 348, 349, 350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R1cW3l0Du0Nq"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
        "        if data.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        if str(criterion) == 'CrossEntropyLoss()':\n",
        "          loss = criterion(out, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = torch.mean((torch.argmax(out,dim=1)==target).float()).item()\n",
        "\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(\n",
        "            'Train\\t\\tEpoch: {} [{}/{} ({:.0f}%)] \\t'\n",
        "            'Batch Loss: {:.6f} \\t'\n",
        "            'Batch Accuracy: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx,\n",
        "                len(train_loader),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item(),\n",
        "                accuracy))\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "          break\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9tYX9Fr9u3Ki"
      },
      "outputs": [],
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    all_features = []\n",
        "    class_features = [[] for _ in range(C)]\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Metrics']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "\n",
        "            # Collect all features for rank analysis\n",
        "            if computation == 'Mean':\n",
        "              all_features.append(h.cpu().detach())\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                # Collect class-specific features for SVD analysis\n",
        "                if computation == 'Mean':\n",
        "                  class_features[c].append(h_c.cpu().detach())\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Metrics':\n",
        "                    ## COV\n",
        "                    # update within-class cov\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "            loss /= sum(N)\n",
        "\n",
        "            # Feature rank analysis\n",
        "            all_features_tensor = torch.cat(all_features, dim=0)\n",
        "\n",
        "            # Compute feature rank using *randomized* SVD\n",
        "            U, S, _ = randomized_svd(all_features_tensor.cpu().numpy(), n_components=100)\n",
        "            S = torch.from_numpy(S).to(device)\n",
        "\n",
        "            # Calculate effective rank\n",
        "            normalized_sv = S / torch.sum(S)\n",
        "            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "            effective_ranks = {}\n",
        "            for thresh in RANK_THRESHOLDS:\n",
        "                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item() # convert tensor to scalar\n",
        "            graphs.feature_rank.append(effective_ranks)\n",
        "            graphs.singular_values.append(S.cpu().numpy())\n",
        "\n",
        "            # Class means SVD\n",
        "            U_M, S_M, V_M = torch.svd(M, some=True)\n",
        "            graphs.mean_singular_values.append(S_M.cpu().numpy())\n",
        "\n",
        "            # Class-wise SVD analysis\n",
        "            class_sv_lists = []\n",
        "            for c in range(C):\n",
        "                if len(class_features[c]) > 0:\n",
        "                    class_feat = torch.cat(class_features[c], dim=0)\n",
        "                    # Center the features\n",
        "                    class_feat = class_feat - mean[c].unsqueeze(0)\n",
        "                    # Compute SVD\n",
        "                    try:\n",
        "                        _, S_c, _ = torch.svd(class_feat, some=True)\n",
        "                        class_sv_lists.append(S_c.cpu().numpy())\n",
        "                    except:\n",
        "                        # Handle potential numerical issues\n",
        "                        class_sv_lists.append(np.zeros(min(class_feat.shape)))\n",
        "\n",
        "            graphs.class_singular_values.append(class_sv_lists)\n",
        "        elif computation == 'Metrics':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    W  = classifier.weight\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.cpu().double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.svd_lowrank(Sb, q=C-1)\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V):\n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "reGN3Qbo81pz"
      },
      "outputs": [],
      "source": [
        "class UpdateTracker:\n",
        "    def __init__(self, model, max_history=10, max_components=100):\n",
        "        # Configuration\n",
        "        self.max_components = max_components\n",
        "        self.max_history = max_history  # Only keep recent updates to save memory\n",
        "        self.tracked_layers = tracked_layers\n",
        "        self.first_update = True\n",
        "\n",
        "        # Global tracking (all layers)\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.global_deltas = []  # Will use a sliding window approach\n",
        "\n",
        "        # Layer-specific tracking\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "        self.layer_deltas = {n: [] for n in self.tracked_layers}\n",
        "\n",
        "    def compute_update_rank(self, model):\n",
        "        # Initialize return values\n",
        "        effective_ranks = {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "        effective_ranks_layer = {n: {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "                               for n in self.tracked_layers}\n",
        "        sv = np.full(self.max_components, np.nan)\n",
        "        layer_svs = {n: np.full(min(self.max_components, model.state_dict()[n].numel()), np.nan)\n",
        "                    for n in self.tracked_layers}\n",
        "\n",
        "        # Skip if first epoch\n",
        "        if self.first_update:\n",
        "            print(\"First epoch - initializing trackers (no updates yet)\")\n",
        "            self.first_update = False\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Global computation (all layers) ===\n",
        "        try:\n",
        "            # Compute current update\n",
        "            delta = []\n",
        "            for n, p in model.named_parameters():\n",
        "                param_delta = (p.detach() - self.prev_weights[n]).flatten().cpu().numpy()\n",
        "                delta.append(param_delta)\n",
        "            delta_vector = np.concatenate(delta)\n",
        "\n",
        "            # Check if update is too small\n",
        "            update_norm = np.linalg.norm(delta_vector)\n",
        "            if update_norm < 1e-10:\n",
        "                print(\"Warning: Update magnitude very small, skipping SVD\")\n",
        "                return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "            # Add to history (with memory management)\n",
        "            self.global_deltas.append(delta_vector)\n",
        "            if len(self.global_deltas) > self.max_history:\n",
        "                self.global_deltas.pop(0)  # Remove oldest update\n",
        "\n",
        "            # Perform SVD on recent history\n",
        "            delta_matrix = np.vstack(self.global_deltas)\n",
        "            n_components = min(delta_matrix.shape[0], self.max_components)\n",
        "\n",
        "            if delta_matrix.shape[0] > 1:\n",
        "                # Use randomized SVD for efficiency\n",
        "                U, S, _ = randomized_svd(delta_matrix, n_components=n_components)\n",
        "                explained_variance = np.cumsum(S**2) / np.sum(S**2)\n",
        "\n",
        "                # Calculate effective ranks\n",
        "                for thresh in RANK_THRESHOLDS:\n",
        "                    effective_ranks[str(thresh)] = np.sum(explained_variance < thresh) + 1\n",
        "                sv = S.copy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in global SVD computation: {e}\")\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Layer-specific computation ===\n",
        "        for layer_name in self.tracked_layers:\n",
        "            try:\n",
        "                # Compute current layer update\n",
        "                current_param = model.state_dict()[layer_name]\n",
        "                prev_param = self.layer_prev_weights[layer_name]\n",
        "                layer_delta = (current_param - prev_param).flatten().cpu().numpy().astype(np.float32)  # Use float32 to save memory\n",
        "\n",
        "                # Check if layer update is too small\n",
        "                layer_update_norm = np.linalg.norm(layer_delta)\n",
        "                if layer_update_norm < 1e-10:\n",
        "                    continue\n",
        "\n",
        "                # Add to layer history (with memory management)\n",
        "                self.layer_deltas[layer_name].append(layer_delta)\n",
        "                if len(self.layer_deltas[layer_name]) > self.max_history:\n",
        "                    self.layer_deltas[layer_name].pop(0)  # Remove oldest update\n",
        "\n",
        "                # Perform SVD on recent history for this layer\n",
        "                if len(self.layer_deltas[layer_name]) > 1:\n",
        "                    layer_delta_matrix = np.vstack(self.layer_deltas[layer_name])\n",
        "                    layer_n_components = min(layer_delta_matrix.shape[0], self.max_components)\n",
        "\n",
        "                    # Use randomized SVD for efficiency\n",
        "                    _, layer_S, _ = randomized_svd(layer_delta_matrix, n_components=layer_n_components)\n",
        "                    layer_explained_variance = np.cumsum(layer_S**2) / np.sum(layer_S**2)\n",
        "\n",
        "                    # Calculate effective ranks for this layer\n",
        "                    for thresh in RANK_THRESHOLDS:\n",
        "                        effective_ranks_layer[layer_name][str(thresh)] = np.sum(layer_explained_variance < thresh) + 1\n",
        "                    layer_svs[layer_name] = layer_S.copy()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in layer-specific SVD for {layer_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Update previous weights for next iteration\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "\n",
        "        return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    # Experiment 1 data\n",
        "    self.feature_rank = [] # stores dict [{'0.95': rank1, '0.99': rank2}\n",
        "    self.singular_values = []\n",
        "    self.mean_singular_values = []\n",
        "    self.class_singular_values = []\n",
        "\n",
        "    # Global Experiment 2 data\n",
        "    self.update_ranks = []\n",
        "    self.update_spectra = []  # Global singular values -- None for most epochs\n",
        "\n",
        "    # Layer-specific Experiment 2 data\n",
        "    self.layer_singular_values = {n: [] for n in tracked_layers} #  -- None for most epochs\n",
        "    self.layer_update_ranks = {n: [] for n in tracked_layers}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tsXs-Zw35TW2",
        "outputId": "a58f9640-0b06-4c24-eda4-5f3d1ae69689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train\t\tEpoch: 1 [4/390 (1%)] \tBatch Loss: 5.870763 \tBatch Accuracy: 0.085938:   1%|          | 4/390 [00:48<1:14:43, 11.62s/it]"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=False, num_classes=C)\n",
        "model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)\n",
        "model.maxpool = nn.Identity()\n",
        "model = model.to(device)\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                              milestones=epochs_lr_decay,\n",
        "                                              gamma=lr_decay)\n",
        "tracker = UpdateTracker(model)\n",
        "graphs = Graphs()\n",
        "cur_epochs = []\n",
        "start_epoch = 1\n",
        "checkpoint_path = '/content/drive/MyDrive/checkpoint.pth'\n",
        "\n",
        "if not overwrite_checkpoint:\n",
        "  try:\n",
        "      checkpoint = torch.load(checkpoint_path, weights_only=False) # note: only use trusted source (checkpoint from MyDrive)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "      start_epoch = checkpoint['epoch'] + 1\n",
        "      graphs = checkpoint['graphs']\n",
        "      cur_epochs = checkpoint['cur_epochs']\n",
        "      print(f\"Resuming from epoch {start_epoch}\")\n",
        "  except FileNotFoundError:\n",
        "      print(\"No checkpoint found. Starting from scratch.\")\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, 50):\n",
        "    train(model, criterion, device, C, train_loader, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    ## Experiment 2 data appending - can refactor bc repeated code\n",
        "    # Compute update ranks with IPCA\n",
        "    global_rank, global_sv, layer_ranks, layer_svs = tracker.compute_update_rank(model)\n",
        "    graphs.update_ranks.append(global_rank)\n",
        "    for layer_name in tracked_layers:\n",
        "        graphs.layer_update_ranks[layer_name].append(layer_ranks[layer_name])\n",
        "\n",
        "    # store spectra only for analysis epochs\n",
        "    if epoch in epoch_list:\n",
        "        graphs.update_spectra.append(global_sv)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(layer_svs[layer_name])\n",
        "    else:\n",
        "        # Store None as placeholder to maintain index alignment with epoch numbers\n",
        "        graphs.update_spectra.append(None)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(None)\n",
        "\n",
        "    ## memory usage check\n",
        "    import psutil\n",
        "    import os\n",
        "    if epoch % 10 == 0:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_info = process.memory_info()\n",
        "        print(f\"Epoch {epoch}: Memory usage: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
        "        print(f\"Tracker deltas: {len(tracker.global_deltas)} updates stored\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "            print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "    if epoch in epoch_list:\n",
        "        cur_epochs.append(epoch)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader)\n",
        "\n",
        "        ## Experiment 0: NC Figures\n",
        "        plt.figure(1)\n",
        "        plt.semilogy(cur_epochs, graphs.reg_loss)\n",
        "        plt.legend(['Loss + Weight Decay'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Loss')\n",
        "\n",
        "        plt.figure(2)\n",
        "        plt.plot(cur_epochs, 100*(1 - np.array(graphs.accuracy)))\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Training Error (%)')\n",
        "        plt.title('Training Error')\n",
        "\n",
        "        plt.figure(3)\n",
        "        plt.semilogy(cur_epochs, graphs.Sw_invSb)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Tr{Sw Sb^-1}')\n",
        "        plt.title('NC1: Activation Collapse')\n",
        "\n",
        "        plt.figure(4)\n",
        "        plt.plot(cur_epochs, graphs.norm_M_CoV)\n",
        "        plt.plot(cur_epochs, graphs.norm_W_CoV)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Std/Avg of Norms')\n",
        "        plt.title('NC2: Equinorm')\n",
        "\n",
        "        plt.figure(5)\n",
        "        plt.plot(cur_epochs, graphs.cos_M)\n",
        "        plt.plot(cur_epochs, graphs.cos_W)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Avg|Cos + 1/(C-1)|')\n",
        "        plt.title('NC2: Maximal Equiangularity')\n",
        "\n",
        "        plt.figure(6)\n",
        "        plt.plot(cur_epochs,graphs.W_M_dist)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('||W^T - H||^2')\n",
        "        plt.title('NC3: Self Duality')\n",
        "\n",
        "        plt.figure(7)\n",
        "        plt.plot(cur_epochs,graphs.NCC_mismatch)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Proportion Mismatch from NCC')\n",
        "        plt.title('NC4: Convergence to NCC')\n",
        "\n",
        "\n",
        "        ## Experiment 1 Figures\n",
        "        plt.figure(8)\n",
        "        plt.plot(cur_epochs, [x['0.95'] for x in graphs.feature_rank], label='95% Threshold')\n",
        "        plt.plot(cur_epochs, [x['0.99'] for x in graphs.feature_rank], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.title('Feature Space Dimensionality')\n",
        "\n",
        "        plt.figure(9)\n",
        "        # Plot singular value spectra for selected epochs\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx < len(graphs.singular_values):\n",
        "                sv = graphs.singular_values[epoch_idx]\n",
        "                plt.semilogy(range(1, len(sv)+1), sv, label=f'Epoch {cur_epochs[epoch_idx]}')\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Singular Value')\n",
        "        plt.title('Feature Matrix Spectrum Evolution')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.figure(10)\n",
        "        # Plot class means singular values for the last epoch\n",
        "        if len(graphs.mean_singular_values) > 0:\n",
        "            sv_means = graphs.mean_singular_values[-1]\n",
        "            plt.bar(range(1, len(sv_means)+1), sv_means)\n",
        "            plt.xlabel('Index')\n",
        "            plt.ylabel('Singular Value')\n",
        "            plt.title('Class Means Singular Value Spectrum')\n",
        "\n",
        "        # Correlation between NC metrics and feature rank\n",
        "        plt.figure(11)\n",
        "        plt.scatter([x['0.95'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "            label='95% Threshold')\n",
        "        plt.scatter([x['0.99'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "                    label='99% Threshold', marker='x')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Feature Rank')\n",
        "        plt.ylabel('Tr{Sw Sb^-1} (NC1)')\n",
        "        plt.title('Neural Collapse vs. Feature Dimensionality')\n",
        "\n",
        "        plt.figure(12)\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last epoch\n",
        "        colors = ['b', 'g', 'r']\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx >= len(graphs.class_singular_values):\n",
        "                continue\n",
        "            all_class_svs = graphs.class_singular_values[epoch_idx]\n",
        "\n",
        "            # Aggregate top-5 SVs across classes\n",
        "            top5_svs = np.array([sv[:5] for sv in all_class_svs if len(sv) >= 5])\n",
        "            mean_top5 = np.mean(top5_svs, axis=0)\n",
        "            std_top5 = np.std(top5_svs, axis=0)\n",
        "\n",
        "            plt.errorbar(\n",
        "                range(1, 6), mean_top5, yerr=std_top5,\n",
        "                label=f'Epoch {cur_epochs[epoch_idx]}', color=colors[i], alpha=0.7\n",
        "            )\n",
        "\n",
        "        plt.xlabel('Singular Value Index (Top 5)')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.yscale('log')\n",
        "        plt.title('Experiment 1: Class-Wise Feature Singular Values (Mean ± Std)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        ## Experiment 2 Figures\n",
        "        # Update rank\n",
        "        update_epochs = list(range(1, epoch + 1))\n",
        "\n",
        "        plt.figure(13)\n",
        "        # Filter out NaNs from IPCA early epochs\n",
        "        valid_global_95 = [\n",
        "            (e, r['0.95']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.95' in r and not np.isnan(r['0.95'])\n",
        "        ]\n",
        "        valid_global_99 = [\n",
        "            (e, r['0.99']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.99' in r and not np.isnan(r['0.99'])\n",
        "        ]\n",
        "        plt.plot([e for e, _ in valid_global_95], [v for _, v in valid_global_95], label='95% Threshold')\n",
        "        plt.plot([e for e, _ in valid_global_99], [v for _, v in valid_global_99], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank of Parameter Updates')\n",
        "        plt.title('Experiment 2: Parameter Update Rank')\n",
        "\n",
        "        # Layer-Specific Update Ranks\n",
        "        plt.figure(14)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs\n",
        "            valid_95 = [\n",
        "                (e, r['0.95']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.95' in r and not np.isnan(r['0.95'])\n",
        "            ]\n",
        "            valid_99 = [\n",
        "                (e, r['0.99']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.99' in r and not np.isnan(r['0.99'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_95], [v for _, v in valid_95],\n",
        "                    label=f'{layer_name} (95%)')\n",
        "            plt.plot([e for e, _ in valid_99], [v for _, v in valid_99],\n",
        "                    label=f'{layer_name} (99%)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks')\n",
        "\n",
        "        plt.figure(15)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter out None values and get only epochs in epoch_list\n",
        "            valid_svs = [\n",
        "                (e, sv) for e, sv in zip(update_epochs, graphs.layer_singular_values[layer_name])\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "\n",
        "            if valid_svs:\n",
        "                # If we have few valid epochs, show all of them\n",
        "                if len(valid_svs) <= 3:\n",
        "                    selected_indices = list(range(len(valid_svs)))\n",
        "                else:\n",
        "                    # Otherwise show first, middle, last\n",
        "                    selected_indices = [0, len(valid_svs)//2, -1]\n",
        "\n",
        "                for idx in selected_indices:\n",
        "                    e, sv = valid_svs[idx]\n",
        "                    plt.semilogy(\n",
        "                        range(1, len(sv)+1), sv,\n",
        "                        label=f'{layer_name} (Epoch {e})'\n",
        "                    )\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Update Singular Values')\n",
        "\n",
        "        plt.figure(16)\n",
        "        # For the cumulative variance plot, use the last available spectrum for each layer\n",
        "        for layer_name in tracked_layers:\n",
        "            # Get non-None values\n",
        "            valid_svs = [\n",
        "                sv for sv in graphs.layer_singular_values[layer_name]\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "\n",
        "            if valid_svs:\n",
        "                # Use the last available spectrum\n",
        "                sv = valid_svs[-1]\n",
        "                explained_variance = np.cumsum(sv**2) / np.sum(sv**2)\n",
        "                plt.plot(explained_variance, label=f'{layer_name}')\n",
        "\n",
        "        for thresh in RANK_THRESHOLDS:\n",
        "            plt.axhline(thresh, color='k' if thresh==0.99 else 'gray',\n",
        "                        linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Cumulative Explained Variance')\n",
        "        plt.legend()\n",
        "        plt.title('Cumulative Variance Explained by Layer Updates')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "        # save checkpoint\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            # 'graphs': graphs,\n",
        "            # 'tracker': tracker, # too large; checkpoint for Experiment 3\n",
        "            'cur_epochs': cur_epochs\n",
        "        }\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "        gc.collect()\n",
        "        # torch.cuda.empty_cache() # if using GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIFZJprvms59"
      },
      "source": [
        "## Storing data for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wl235LVmwGV"
      },
      "outputs": [],
      "source": [
        "# Data from analysis epochs\n",
        "df = pd.DataFrame({\n",
        "    'epoch': cur_epochs,\n",
        "    'Sw_invSb': graphs.Sw_invSb,\n",
        "    'W_M_dist': graphs.W_M_dist,\n",
        "    'NCC_mismatch': graphs.NCC_mismatch,\n",
        "    'norm_M_CoV': graphs.norm_M_CoV,\n",
        "    'norm_W_CoV': graphs.norm_W_CoV,\n",
        "    'cos_M': graphs.cos_M,\n",
        "    'cos_W': graphs.cos_W,\n",
        "    # Experiment 1\n",
        "    'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "    'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "    # Experiment 2\n",
        "    'update_rank_95': [graphs.update_ranks[e-1]['0.95'] for e in cur_epochs],\n",
        "    'update_rank_99': [graphs.update_ranks[e-1]['0.99'] for e in cur_epochs],\n",
        "    'fc_update_rank_95': [graphs.layer_update_ranks['fc.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "    'fc_update_rank_99': [graphs.layer_update_ranks['fc.weight'][e-1]['0.99'] for e in cur_epochs],\n",
        "    'conv_update_rank_95': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "    'conv_update_rank_99': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.99'] for e in cur_epochs]\n",
        "})\n",
        "df.to_csv('experiment1_experiment2_data.csv', index=False)\n",
        "\n",
        "# Save Experiment 1 singular values\n",
        "np.save('feature_svs.npy', graphs.singular_values)\n",
        "np.save('mean_svs.npy', graphs.mean_singular_values)\n",
        "np.save('class_svs.npy', np.array(graphs.class_singular_values, dtype=object)) # Save as dtype=object\n",
        "\n",
        "# Save Experiment 2 data for all epochs\n",
        "df_exp2 = pd.DataFrame({\n",
        "    'epoch': update_epochs,\n",
        "    'update_rank_95': [x['0.95'] for x in graphs.update_ranks],\n",
        "    'update_rank_99': [x['0.99'] for x in graphs.update_ranks],\n",
        "    'fc_update_rank_95': [x['0.95'] for x in graphs.layer_update_ranks['fc.weight']],\n",
        "    'fc_update_rank_99': [x['0.99'] for x in graphs.layer_update_ranks['fc.weight']],\n",
        "    'conv_update_rank_95': [x['0.95'] for x in graphs.layer_update_ranks['layer4.1.conv2.weight']],\n",
        "    'conv_update_rank_99': [x['0.99'] for x in graphs.layer_update_ranks['layer4.1.conv2.weight']]\n",
        "})\n",
        "df_exp2.to_csv('experiment2_data.csv', index=False)\n",
        "\n",
        "# Experiment 2 singular values (Note: saved as objects due to jagged arrays)\n",
        "valid_update_spectra = [sv for sv in graphs.update_spectra if sv is not None]\n",
        "np.save('global_update_spectra.npy', np.array(valid_update_spectra, dtype=object))\n",
        "\n",
        "for layer_name in tracked_layers:\n",
        "    valid_layer_svs = [sv for sv in graphs.layer_singular_values[layer_name] if sv is not None]\n",
        "    np.save(f'{layer_name}_singular_values.npy', np.array(valid_layer_svs, dtype=object))\n",
        "\n",
        "# Save figures\n",
        "figures_to_save = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]  # All\n",
        "for fig_num in figures_to_save:\n",
        "    plt.figure(fig_num)\n",
        "    plt.savefig(f'figure_{fig_num}_epoch_{epoch}.pdf', bbox_inches='tight', dpi=300)\n",
        "    plt.close(fig_num)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPnspmWEc7Y54dDWCboVPg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}