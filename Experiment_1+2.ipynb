{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethangearey/nc-lora/blob/main/Experiment_1%2B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsSM3c2mub8l"
      },
      "source": [
        "# Experiment 1+2: Tracking Neural Collapse and Feature Rank (1); Measuring Parameter Update Rank (2)\n",
        "\n",
        "## Experiment 1\n",
        "Train deep networks (e.g., ResNet18 on CIFAR-10) with cross-entropy loss.\n",
        "At each epoch, compute:\n",
        "- NC metrics (Papyan et al.): NC1-NC4\n",
        "- Rank of penultimate-layer feature matrix\n",
        "- Spectrum of singular values (SVD) of class means and classwise features\n",
        "\n",
        "Goal: empirically track the onset and strength of neural collapse vs. feature-space dimensionality.\n",
        "\n",
        "## Experiment 2\n",
        "- Save weight updates over training (e.g., via finite differences or gradients).\n",
        "- Measure their empirical rank and SVD spectrum.\n",
        "- Check if late-stage training updates concentrate into a low-rank subspace, coinciding with the emergence of NC.\n",
        "\n",
        "## Experiment 3\n",
        "- Calculate trainVariance (citation), ETF-structure score (NC2), feature rank, SVD spectra for 4 main ResNet blocks.\n",
        "\n",
        "Goal: Evaluate propogation of collapse signal through the layers of the network.\n",
        "\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "- Finite differences are tracked in UpdateTracker, reflecting SGD and momentum effects. Gradients are not tracked in this implementation.\n",
        "\n",
        "- UpdateTracker uses sliding window approach for SVD calculations. Window size can be toggled with max_history parameter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "data_path = os.environ.get(\"DATA_PATH\", None)\n",
        "\n",
        "# If not set, use Drive or local Colab storage\n",
        "if data_path is None:\n",
        "    try:\n",
        "        # Uncomment below to use Drive\n",
        "        drive.mount('/content/drive')\n",
        "        data_path = '/content/drive/MyDrive/Colab Notebooks/experiment_data/'\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to mount Drive: {e}\")\n",
        "        data_path = '/content/data/'\n",
        "        print(\"Falling back to local storage\")\n",
        "\n",
        "os.makedirs(data_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "29ie0YVzzTMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "450f96ee-06ae-47b9-f7ef-7e07d8d0eda0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YNKXMPjQuc0C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import gc\n",
        "import psutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "\n",
        "debug = False # Only runs 20 batches per epoch for debugging\n",
        "checkpoint = False # loads from data_path + 'checkpoint.pth'\n",
        "\n",
        "# Random seed\n",
        "seed                = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# CIFAR dataset parameters\n",
        "im_size             = 32\n",
        "padded_im_size      = 32\n",
        "input_ch            = 3\n",
        "C                   = 10\n",
        "\n",
        "# Optimization Criterion\n",
        "loss_name = 'CrossEntropyLoss'\n",
        "\n",
        "# Optimization hyperparameters\n",
        "lr_decay            = 0.1\n",
        "lr                  = 0.1\n",
        "\n",
        "epochs              = 350\n",
        "epochs_lr_decay   = [100, 300] # include epochs_lr_decay in analysis_epochs\n",
        "# epochs_lr_decay = [epochs//3, 2*epochs//3] # classic from Papyan et. al.\n",
        "\n",
        "\n",
        "batch_size          = 128\n",
        "\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4\n",
        "\n",
        "# analysis parameters\n",
        "RANK_THRESHOLDS     = [0.95, 0.99]\n",
        "tracked_layers      = ['fc.weight', 'layer4.1.conv2.weight', 'layer3.1.conv2.weight', 'layer2.1.conv2.weight', 'layer1.1.conv2.weight', 'conv1.weight']\n",
        "max_history         = 1\n",
        "\n",
        "analysis_epochs          = [1,   5,   10,   15,  20,  30,  40,  60,  80,\n",
        "                            100, 120, 140, 150, 160, 180, 200,\n",
        "                            220, 240, 250, 260, 280, 300,\n",
        "                            310, 320, 330, 340, 350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R1cW3l0Du0Nq"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "\n",
        "    pbar = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
        "        if data.shape[0] != batch_size:\n",
        "            continue\n",
        "\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        if str(criterion) == 'CrossEntropyLoss()':\n",
        "          loss = criterion(out, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = torch.mean((torch.argmax(out,dim=1)==target).float()).item()\n",
        "\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(\n",
        "            'Train\\t\\tEpoch: {} [{}/{} ({:.0f}%)] \\t'\n",
        "            'Batch Loss: {:.6f} \\t'\n",
        "            'Batch Accuracy: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx,\n",
        "                len(train_loader),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item(),\n",
        "                accuracy))\n",
        "\n",
        "        if debug and batch_idx > 20:\n",
        "          break\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9tYX9Fr9u3Ki"
      },
      "outputs": [],
      "source": [
        "# mutual coherence\n",
        "def coherence(V):\n",
        "    assert V.shape[1] == C, f\"got shape {V.shape}\"\n",
        "    G = V.T @ V\n",
        "    G += torch.ones((C,C),device=device) / (C-1)\n",
        "    G -= torch.diag(torch.diag(G))\n",
        "    return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader):\n",
        "    model.eval()\n",
        "\n",
        "    ### BEGIN LAYER BY LAYER TRACKING\n",
        "    layer_outputs = {}\n",
        "    train_variance_by_layer = {}\n",
        "    all_features_by_layer = {}\n",
        "\n",
        "    # Choose layers of interest\n",
        "    selected_layers = {\n",
        "        'layer1': model.layer1,\n",
        "        'layer2': model.layer2,\n",
        "        'layer3': model.layer3,\n",
        "        'layer4': model.layer4,\n",
        "        'avgpool': model.avgpool,\n",
        "    }\n",
        "\n",
        "    def make_hook(name):\n",
        "        def hook(module, input, output):\n",
        "            # Reshape if avgpool\n",
        "            if name == 'avgpool':\n",
        "                output = output.view(output.shape[0], -1)\n",
        "            layer_outputs[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    # Register hooks\n",
        "    hooks = [layer.register_forward_hook(make_hook(name)) for name, layer in selected_layers.items()]\n",
        "    ### END LAYER BY LAYER TRACKING\n",
        "\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    all_features = []\n",
        "    class_features = [[] for _ in range(C)]\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Metrics']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "            ### BEGIN LAYER TRACKING ###\n",
        "            if computation == 'Mean':\n",
        "                for layer_name, feat in layer_outputs.items():\n",
        "\n",
        "                    if layer_name not in train_variance_by_layer:\n",
        "                        train_variance_by_layer[layer_name] = [[] for _ in range(C)]\n",
        "\n",
        "                    layer_feat = feat.view(data.shape[0], -1)\n",
        "                    if layer_name not in all_features_by_layer:\n",
        "                        all_features_by_layer[layer_name] = []\n",
        "                    all_features_by_layer[layer_name].append(layer_feat.detach())\n",
        "\n",
        "                    for c in range(C):\n",
        "                        idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "                        if len(idxs) == 0:\n",
        "                            continue\n",
        "                        h_c = feat[idxs].view(len(idxs), -1)\n",
        "                        train_variance_by_layer[layer_name][c].append(h_c.cpu())\n",
        "\n",
        "              ### END LAYER TRACKING ###\n",
        "\n",
        "            # Collect all features for rank analysis\n",
        "            if computation == 'Mean':\n",
        "              all_features.append(h.detach())\n",
        "\n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                # Collect class-specific features for SVD analysis\n",
        "                if computation == 'Mean':\n",
        "                    class_features[c].append(h_c.detach())\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "\n",
        "                elif computation == 'Metrics':\n",
        "                    ## COV\n",
        "                    # update within-class cov\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "\n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "\n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "            M = torch.stack(mean).T\n",
        "            loss /= sum(N)\n",
        "\n",
        "            ### BEGIN LAYER TRACKING ###\n",
        "            graphs.train_variance_by_layer.append({})\n",
        "            graphs.feature_rank_by_layer.append({})\n",
        "            graphs.singular_values_by_layer.append({})\n",
        "            graphs.nc1_by_layer.append({})\n",
        "            graphs.nc2_by_layer.append({})\n",
        "\n",
        "\n",
        "            for layer_name, class_lists in train_variance_by_layer.items():\n",
        "                class_means = []\n",
        "                for c in range(C):\n",
        "                    if len(class_lists[c]) == 0:\n",
        "                        continue\n",
        "                    class_feats = torch.cat(class_lists[c], dim=0)\n",
        "                    class_mean = class_feats.mean(dim=0)\n",
        "                    class_means.append(class_mean)\n",
        "\n",
        "                class_means = torch.stack(class_means)  # shape (num_classes, feature_dim)\n",
        "                global_mean = class_means.mean(dim=0, keepdim=True)  # shape (1, feature_dim)\n",
        "\n",
        "                between_class_var = ((class_means - global_mean) ** 2).mean().item()\n",
        "\n",
        "                # Compute within-class variance\n",
        "                class_variances = []\n",
        "                for c in range(C):\n",
        "                    if len(class_lists[c]) == 0:\n",
        "                        continue\n",
        "                    class_feats = torch.cat(class_lists[c], dim=0)\n",
        "                    centered = class_feats - class_feats.mean(dim=0, keepdim=True)\n",
        "                    var = (centered ** 2).mean().item()\n",
        "                    class_variances.append(var)\n",
        "                within_class_var = np.mean(class_variances)\n",
        "\n",
        "                # Calculate the full Neural Collapse train variance metric\n",
        "                train_variance = within_class_var / between_class_var if between_class_var != 0 else None\n",
        "                graphs.train_variance_by_layer[-1][layer_name] = train_variance\n",
        "\n",
        "\n",
        "                # === Compute NC1 ===\n",
        "                # layer_M = class_means.T  # shape: [D, C]\n",
        "                # layer_muG = torch.mean(layer_M, dim=1, keepdim=True)\n",
        "                # layer_M_centered = layer_M - layer_muG\n",
        "                # Sb_layer = torch.matmul(layer_M_centered, layer_M_centered.T) / C  # [D, D]\n",
        "\n",
        "                # Sw_layer = torch.zeros_like(Sb_layer)\n",
        "                # for c in range(C):\n",
        "                #     if len(class_lists[c]) == 0:\n",
        "                #         continue\n",
        "                #     class_feats = torch.cat(class_lists[c], dim=0)\n",
        "                #     centered = class_feats - class_feats.mean(dim=0, keepdim=True)\n",
        "                #     cov = centered.T @ centered / class_feats.shape[0]\n",
        "                #     Sw_layer += cov\n",
        "                # Sw_layer /= C\n",
        "\n",
        "                # # Add jitter\n",
        "                # Sw_layer += 1e-8 * torch.eye(Sw_layer.shape[0], device=Sw_layer.device)\n",
        "                # Sb_layer += 1e-8 * torch.eye(Sb_layer.shape[0], device=Sb_layer.device)\n",
        "\n",
        "                # try:\n",
        "                #     Sb_inv = torch.linalg.pinv(Sb_layer)\n",
        "                #     nc1 = torch.trace(Sw_layer @ Sb_inv).item()\n",
        "                # except:\n",
        "                #     nc1 = float('nan')\n",
        "\n",
        "                # === Compute NC2 ===\n",
        "                normed_means = F.normalize(class_means, dim=1)\n",
        "                nc2 = coherence(normed_means.T.to(device))\n",
        "\n",
        "                # # Ensure lists are synced with epochs\n",
        "                # if len(graphs.nc1_by_layer) < len(graphs.train_variance_by_layer):\n",
        "                #     graphs.nc1_by_layer.append({})\n",
        "                #     graphs.nc2_by_layer.append({})\n",
        "\n",
        "\n",
        "                graphs.nc1_by_layer[-1][layer_name] = 0 # deprecate\n",
        "                graphs.nc2_by_layer[-1][layer_name] = nc2\n",
        "\n",
        "                # === Feature rank analysis by layer ===\n",
        "                if layer_name in all_features_by_layer and len(all_features_by_layer[layer_name]) > 0:\n",
        "                    all_features_tensor = torch.cat(all_features_by_layer[layer_name], dim=0)\n",
        "\n",
        "                    # Compute feature rank using SVD\n",
        "                    with torch.no_grad():\n",
        "                        try:\n",
        "                            _, S, _ = torch.linalg.svd(all_features_tensor, full_matrices=False)\n",
        "                            # Keep top singular values (adjust based on your needs)\n",
        "                            S = S[:min(100, len(S))]\n",
        "\n",
        "                            # Calculate effective rank\n",
        "                            normalized_sv = S / torch.sum(S)\n",
        "                            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "                            effective_ranks = {}\n",
        "                            for thresh in RANK_THRESHOLDS:\n",
        "                                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item()\n",
        "\n",
        "                            graphs.feature_rank_by_layer[-1][layer_name] = effective_ranks\n",
        "                            graphs.singular_values_by_layer[-1][layer_name] = S.cpu().numpy()\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"SVD failed for layer {layer_name}: {e}\")\n",
        "                            # Set default values if SVD fails\n",
        "                            graphs.feature_rank_by_layer[-1][layer_name] = {str(thresh): 0 for thresh in RANK_THRESHOLDS}\n",
        "                            graphs.singular_values_by_layer[-1][layer_name] = np.zeros(100)\n",
        "                else:\n",
        "                    # No features collected for this layer\n",
        "                    graphs.feature_rank_by_layer[-1][layer_name] = {str(thresh): 0 for thresh in RANK_THRESHOLDS}\n",
        "                    graphs.singular_values_by_layer[-1][layer_name] = np.zeros(100)\n",
        "\n",
        "            ### END LAYER TRACKING ###\n",
        "\n",
        "            # Feature rank analysis\n",
        "            all_features_tensor = torch.cat(all_features, dim=0)\n",
        "\n",
        "            # Compute feature rank using *torch SVD*\n",
        "            with torch.no_grad():\n",
        "                _, S, _ = torch.linalg.svd(all_features_tensor, full_matrices=False)\n",
        "                S = S[:100]  # Only keep top 100 components\n",
        "\n",
        "            # Calculate effective rank\n",
        "            normalized_sv = S / torch.sum(S)\n",
        "            cumulative_energy = torch.cumsum(normalized_sv, dim=0)\n",
        "            effective_ranks = {}\n",
        "            for thresh in RANK_THRESHOLDS:\n",
        "                effective_ranks[str(thresh)] = (torch.sum(cumulative_energy < thresh) + 1).item() # convert tensor to scalar\n",
        "            graphs.feature_rank.append(effective_ranks)\n",
        "            graphs.singular_values.append(S.cpu().numpy())\n",
        "\n",
        "            # Class means SVD\n",
        "            U_M, S_M, V_M = torch.svd(M, some=True)\n",
        "            graphs.mean_singular_values.append(S_M.cpu().numpy())\n",
        "\n",
        "            # Class-wise SVD analysis\n",
        "            class_sv_lists = []\n",
        "            for c in range(C):\n",
        "                if len(class_features[c]) > 0:\n",
        "                    class_feat = torch.cat(class_features[c], dim=0).to(device)\n",
        "                    # Center the features\n",
        "                    class_feat = class_feat - mean[c].unsqueeze(0)\n",
        "                    # Compute SVD\n",
        "                    try:\n",
        "                        _, S_c, _ = torch.svd(class_feat, some=True)\n",
        "                        class_sv_lists.append(S_c.cpu().numpy())\n",
        "                    except:\n",
        "                        # Handle potential numerical issues\n",
        "                        class_sv_lists.append(np.zeros(min(class_feat.shape)))\n",
        "\n",
        "            graphs.class_singular_values.append(class_sv_lists)\n",
        "        elif computation == 'Metrics':\n",
        "            Sw /= sum(N)\n",
        "\n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "\n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    W  = classifier.weight\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.double()\n",
        "    Sw += 1e-8 * torch.eye(Sw.shape[0], device=Sw.device) # add jitter for numerical sability\n",
        "    Sb = Sb.double()  # Extra precision for small eigenvalues; modified orig.\n",
        "    eigvec, eigval, _ = torch.linalg.svd(Sb, full_matrices=False)\n",
        "    eigvec = eigvec[:, :C-1]\n",
        "    eigval = eigval[:C-1]\n",
        "    inv_Sb = eigvec @ torch.diag(1/eigval) @ eigvec.T\n",
        "    graphs.Sw_invSb.append(torch.trace(Sw @ inv_Sb).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))\n",
        "\n",
        "    ## remove Layer tracking hooks:\n",
        "    for h in hooks:\n",
        "        h.remove()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "reGN3Qbo81pz"
      },
      "outputs": [],
      "source": [
        "class UpdateTracker:\n",
        "    def __init__(self, model, max_history=max_history, max_components=100):\n",
        "        # Configuration\n",
        "        self.max_components = max_components\n",
        "        self.max_history = max_history  # Only keep recent updates to save memory\n",
        "        self.tracked_layers = tracked_layers\n",
        "        self.first_update = True\n",
        "\n",
        "        # Global tracking (all layers)\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.global_deltas = []  # Will use a sliding window approach\n",
        "\n",
        "        # Layer-specific tracking\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "        self.layer_deltas = {n: [] for n in self.tracked_layers}\n",
        "\n",
        "    def compute_update_rank(self, model):\n",
        "        # Initialize return values\n",
        "        effective_ranks = {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "        effective_ranks_layer = {n: {str(t): np.nan for t in RANK_THRESHOLDS}\n",
        "                               for n in self.tracked_layers}\n",
        "        sv = np.full(self.max_components, np.nan)\n",
        "        layer_svs = {n: np.full(min(self.max_components, model.state_dict()[n].numel()), np.nan)\n",
        "                    for n in self.tracked_layers}\n",
        "\n",
        "        # Skip if first epoch\n",
        "        if self.first_update:\n",
        "            print(\"First epoch - initializing trackers (no updates yet)\")\n",
        "            self.first_update = False\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Global computation (all layers) ===\n",
        "        try:\n",
        "            # Compute current update\n",
        "            delta = []\n",
        "            for n, p in model.named_parameters():\n",
        "                param_delta = (p.detach() - self.prev_weights[n]).flatten()\n",
        "                delta.append(param_delta)\n",
        "            delta_vector = torch.cat(delta)\n",
        "\n",
        "            # Check if update is too small\n",
        "            update_norm = torch.norm(delta_vector).item()\n",
        "            if update_norm < 1e-10:\n",
        "                print(\"Warning: Update magnitude very small, skipping SVD\")\n",
        "                return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "            # Add to history (with memory management)\n",
        "            self.global_deltas.append(delta_vector)\n",
        "            if len(self.global_deltas) > self.max_history:\n",
        "                self.global_deltas.pop(0)  # Remove oldest update\n",
        "\n",
        "            # Perform torch SVD on recent history\n",
        "            global_deltas = [d.to(device) for d in self.global_deltas] # send to device (debug)\n",
        "            delta_matrix = torch.stack(self.global_deltas)\n",
        "\n",
        "            if delta_matrix.shape[0] > 1:\n",
        "                U, S, _ = torch.linalg.svd(delta_matrix, full_matrices=False)\n",
        "                S = S[:self.max_components]  # Truncate\n",
        "                explained_variance = torch.cumsum(S**2, dim=0) / torch.sum(S**2)\n",
        "\n",
        "                for thresh in RANK_THRESHOLDS:\n",
        "                    effective_rank = torch.sum(explained_variance < thresh).item() + 1\n",
        "                    effective_ranks[str(thresh)] = effective_rank\n",
        "                sv = S.cpu().numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in global SVD computation: {e}\")\n",
        "            return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "        # === Layer-specific computation ===\n",
        "        for layer_name in self.tracked_layers:\n",
        "            try:\n",
        "                # Compute current layer update\n",
        "                current_param = model.state_dict()[layer_name]\n",
        "                prev_param = self.layer_prev_weights[layer_name]\n",
        "                layer_delta = (current_param - prev_param).flatten()\n",
        "\n",
        "                # Check if layer update is too small\n",
        "                layer_update_norm = torch.norm(layer_delta).item()\n",
        "                if layer_update_norm < 1e-10:\n",
        "                    continue\n",
        "\n",
        "                # Add to layer history (with memory management)\n",
        "                self.layer_deltas[layer_name].append(layer_delta)\n",
        "                if len(self.layer_deltas[layer_name]) > self.max_history:\n",
        "                    self.layer_deltas[layer_name].pop(0)  # Remove oldest update\n",
        "\n",
        "                # Perform SVD on recent history for this layer\n",
        "                if len(self.layer_deltas[layer_name]) > 1:\n",
        "                    layer_deltas = [d.to(device) for d in self.layer_deltas[layer_name]] # send to device (debug)\n",
        "                    layer_delta_matrix = torch.stack(self.layer_deltas[layer_name])\n",
        "                    layer_n_components = min(layer_delta_matrix.shape[0], self.max_components)\n",
        "\n",
        "                    # Use randomized SVD for efficiency\n",
        "                    _, layer_S, _ = torch.linalg.svd(layer_delta_matrix, full_matrices=False)\n",
        "                    layer_S = layer_S[:self.max_components] # Truncate\n",
        "                    layer_explained_variance = torch.cumsum(layer_S**2, dim=0) / torch.sum(layer_S**2)\n",
        "\n",
        "                    # Calculate effective ranks for this layer\n",
        "                    for thresh in RANK_THRESHOLDS:\n",
        "                        effective_ranks_layer[layer_name][str(thresh)] = (\n",
        "                            torch.sum(layer_explained_variance < thresh).item() + 1\n",
        "                        )\n",
        "                    layer_svs[layer_name] = layer_S.cpu().numpy() # move to CPU for saving\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in layer-specific SVD for {layer_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Update previous weights for next iteration\n",
        "        self.prev_weights = {n: p.detach().clone() for n, p in model.named_parameters()}\n",
        "        self.layer_prev_weights = {n: model.state_dict()[n].detach().clone()\n",
        "                                  for n in self.tracked_layers}\n",
        "\n",
        "        return effective_ranks, sv, effective_ranks_layer, layer_svs\n",
        "\n",
        "class Graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "\n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    # Experiment 1 data\n",
        "    self.feature_rank = [] # stores dict [{'0.95': rank1, '0.99': rank2}\n",
        "    self.singular_values = []\n",
        "    self.mean_singular_values = []\n",
        "    self.class_singular_values = []\n",
        "\n",
        "    # Global Experiment 2 data\n",
        "    self.update_ranks = []\n",
        "    self.update_spectra = []  # Global singular values -- None for most epochs\n",
        "\n",
        "    # Layer-specific Experiment 2 data\n",
        "    self.layer_singular_values = {n: [] for n in tracked_layers} #  -- None for most epochs\n",
        "    self.layer_update_ranks = {n: [] for n in tracked_layers}\n",
        "\n",
        "\n",
        "    # Experiment 3\n",
        "    self.train_variance_by_layer = []\n",
        "\n",
        "    self.nc1_by_layer = []\n",
        "    self.nc2_by_layer = []\n",
        "\n",
        "    self.feature_rank_by_layer = []\n",
        "    self.singular_values_by_layer = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "tsXs-Zw35TW2",
        "outputId": "a79896d0-029e-4375-eac6-e503207739c2",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 170M/170M [00:01<00:00, 85.8MB/s]\n",
            "Train\t\tEpoch: 1 [390/390 (100%)] \tBatch Loss: 1.375375 \tBatch Accuracy: 0.507812: 100%|██████████| 390/390 [00:20<00:00, 18.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First epoch - initializing trackers (no updates yet)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Analysis Mean\tEpoch: 1 [390/390 (100%)]: 100%|██████████| 390/390 [00:36<00:00, 10.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVD failed for layer layer1: CUDA out of memory. Tried to allocate 9.29 GiB. GPU 0 has a total capacity of 39.56 GiB of which 1.22 GiB is free. Process 12188 has 38.33 GiB memory in use. Of the allocated memory 36.03 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 39.56 GiB of which 1.22 GiB is free. Process 12188 has 38.33 GiB memory in use. Of the allocated memory 36.03 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-40bbf792a6b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalysis_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mcur_epochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0manalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_summed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# === Graphs ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4d07de32c4db>\u001b[0m in \u001b[0;36manalysis\u001b[0;34m(graphs, model, criterion_summed, device, num_classes, loader)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;31m# === Feature rank analysis by layer ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_features_by_layer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features_by_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     \u001b[0mall_features_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features_by_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                     \u001b[0;31m# Compute feature rank using SVD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.09 GiB. GPU 0 has a total capacity of 39.56 GiB of which 1.22 GiB is free. Process 12188 has 38.33 GiB memory in use. Of the allocated memory 36.03 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "model = models.resnet18(pretrained=False, num_classes=C)\n",
        "model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)\n",
        "model.maxpool = nn.Identity()\n",
        "model.fc = nn.Linear(model.fc.in_features, C, bias=False)\n",
        "model = model.to(device)\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "                                ])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD( # Papyan et al. (2020) shows NC2 requires unregularized classifiers\n",
        "    [{'params': [p for n, p in model.named_parameters() if 'fc' not in n], 'weight_decay': weight_decay},\n",
        "     {'params': [p for n, p in model.named_parameters() if 'fc' in n], 'weight_decay': 0}],\n",
        "    lr=lr,\n",
        "    momentum=momentum\n",
        ")\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                              milestones=epochs_lr_decay,\n",
        "                                              gamma=lr_decay)\n",
        "\n",
        "if checkpoint:\n",
        "    print(\"Loading checkpoint...\")\n",
        "    checkpoint = torch.load(data_path + 'checkpoint.pth', map_location=device, weights_only=False)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
        "\n",
        "    graphs = checkpoint['graphs']\n",
        "    tracker = checkpoint['tracker']\n",
        "    cur_epochs = checkpoint['cur_epochs']\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "    # ## Debug: check, force tensors to GPU\n",
        "    # print(\"Checkpoint loaded, analyzing tensor devices...\")\n",
        "    # # Check devices in tracker\n",
        "    # if hasattr(tracker, 'global_deltas'):\n",
        "    #     devices = set()\n",
        "    #     for i, delta in enumerate(tracker.global_deltas):\n",
        "    #         devices.add(str(delta.device))\n",
        "    #         print(f\"Delta {i}: shape={delta.shape}, device={delta.device}, norm={torch.norm(delta).item()}\")\n",
        "    #     print(f\"Found tensors on devices: {devices}\")\n",
        "\n",
        "    # # Check current model device\n",
        "    # model_device = next(model.parameters()).device\n",
        "    # print(f\"Model is on device: {model_device}\")\n",
        "\n",
        "    # # Force move all tensors in tracker to model device\n",
        "    # device = next(model.parameters()).device\n",
        "    # print(f\"Moving all tensors to {device}\")\n",
        "\n",
        "    # # Move global deltas\n",
        "    # if hasattr(tracker, 'global_deltas'):\n",
        "    #     tracker.global_deltas = [delta.to(device) for delta in tracker.global_deltas]\n",
        "    #     print(f\"Moved {len(tracker.global_deltas)} global deltas to {device}\")\n",
        "\n",
        "    # # Move layer deltas\n",
        "    # if hasattr(tracker, 'layer_deltas'):\n",
        "    #     for layer_name in tracker.layer_deltas:\n",
        "    #         tracker.layer_deltas[layer_name] = [delta.to(device) for delta in tracker.layer_deltas[layer_name]]\n",
        "    #     print(f\"Moved layer deltas for {len(tracker.layer_deltas)} layers to {device}\")\n",
        "\n",
        "    # # Move previous weights\n",
        "    # if hasattr(tracker, 'prev_weights'):\n",
        "    #     tracker.prev_weights = {k: v.to(device) for k, v in tracker.prev_weights.items()}\n",
        "    #     print(\"Moved previous weights to {device}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    tracker = UpdateTracker(model)\n",
        "    graphs = Graphs()\n",
        "    cur_epochs = []\n",
        "    start_epoch = 1\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, epochs + 1):\n",
        "    train(model, criterion, device, C, train_loader, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "\n",
        "\n",
        "    # === Experiment 2 data collection ===\n",
        "    update_epochs = list(range(1, epoch + 1)) # used for graphs, data saving\n",
        "    global_rank, global_sv, layer_ranks, layer_svs = tracker.compute_update_rank(model)\n",
        "    graphs.update_ranks.append(global_rank)\n",
        "    for layer_name in tracked_layers:\n",
        "        graphs.layer_update_ranks[layer_name].append(layer_ranks[layer_name])\n",
        "\n",
        "    # store spectra only for analysis epochs\n",
        "    if epoch in analysis_epochs:\n",
        "        graphs.update_spectra.append(global_sv)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(layer_svs[layer_name])\n",
        "    else:\n",
        "        # Store None as placeholder to maintain index alignment with epoch numbers\n",
        "        graphs.update_spectra.append(None)\n",
        "        for layer_name in tracked_layers:\n",
        "            graphs.layer_singular_values[layer_name].append(None)\n",
        "            # graphs.layer_update_ranks[layer_name].append({'0.95': np.nan, '0.99': np.nan})\n",
        "\n",
        "\n",
        "    if epoch in analysis_epochs:\n",
        "        cur_epochs.append(epoch)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader)\n",
        "\n",
        "        # === Graphs ===\n",
        "        ## Experiment 0: NC Figures\n",
        "        plt.figure(1)\n",
        "        plt.semilogy(cur_epochs, graphs.reg_loss)\n",
        "        plt.legend(['Loss + Weight Decay'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Loss')\n",
        "\n",
        "        plt.figure(2)\n",
        "        plt.plot(cur_epochs, 100*(1 - np.array(graphs.accuracy)))\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Training Error (%)')\n",
        "        plt.title('Training Error')\n",
        "\n",
        "        plt.figure(3)\n",
        "        plt.semilogy(cur_epochs, graphs.Sw_invSb)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Tr{Sw Sb^-1}')\n",
        "        plt.title('NC1: Activation Collapse')\n",
        "\n",
        "        plt.figure(4)\n",
        "        plt.plot(cur_epochs, graphs.norm_M_CoV)\n",
        "        plt.plot(cur_epochs, graphs.norm_W_CoV)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Std/Avg of Norms')\n",
        "        plt.title('NC2: Equinorm')\n",
        "\n",
        "        plt.figure(5)\n",
        "        plt.plot(cur_epochs, graphs.cos_M)\n",
        "        plt.plot(cur_epochs, graphs.cos_W)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Avg|Cos + 1/(C-1)|')\n",
        "        plt.title('NC2: Maximal Equiangularity')\n",
        "\n",
        "        plt.figure(6)\n",
        "        plt.plot(cur_epochs,graphs.W_M_dist)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('||W^T - H||^2')\n",
        "        plt.title('NC3: Self Duality')\n",
        "\n",
        "        plt.figure(7)\n",
        "        plt.plot(cur_epochs,graphs.NCC_mismatch)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Proportion Mismatch from NCC')\n",
        "        plt.title('NC4: Convergence to NCC')\n",
        "\n",
        "        ## Experiment 1 Figures\n",
        "        plt.figure(8)\n",
        "        plt.plot(cur_epochs, [x['0.95'] for x in graphs.feature_rank], label='95% Threshold')\n",
        "        plt.plot(cur_epochs, [x['0.99'] for x in graphs.feature_rank], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.title('Feature Space Dimensionality')\n",
        "\n",
        "        plt.figure(9)\n",
        "        # Plot singular value spectra for selected epochs\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx < len(graphs.singular_values):\n",
        "                sv = graphs.singular_values[epoch_idx]\n",
        "                plt.semilogy(range(1, len(sv)+1), sv, label=f'Epoch {cur_epochs[epoch_idx]}')\n",
        "        plt.xlabel('Index')\n",
        "        plt.ylabel('Singular Value')\n",
        "        plt.title('Feature Matrix Spectrum Evolution')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.figure(10)\n",
        "        # Plot class means singular values for the last epoch\n",
        "        if len(graphs.mean_singular_values) > 0:\n",
        "            sv_means = graphs.mean_singular_values[-1]\n",
        "            plt.bar(range(1, len(sv_means)+1), sv_means)\n",
        "            plt.xlabel('Index')\n",
        "            plt.ylabel('Singular Value')\n",
        "            plt.title('Class Means Singular Value Spectrum')\n",
        "\n",
        "        # Correlation between NC metrics and feature rank\n",
        "        plt.figure(11)\n",
        "        plt.scatter([x['0.95'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "            label='95% Threshold')\n",
        "        plt.scatter([x['0.99'] for x in graphs.feature_rank], graphs.Sw_invSb,\n",
        "                    label='99% Threshold', marker='x')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Feature Rank')\n",
        "        plt.ylabel('Tr{Sw Sb^-1} (NC1)')\n",
        "        plt.title('Neural Collapse vs. Feature Dimensionality')\n",
        "\n",
        "        plt.figure(12)\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last epoch\n",
        "        colors = ['b', 'g', 'r']\n",
        "        for i, epoch_idx in enumerate(selected_epochs):\n",
        "            if epoch_idx >= len(graphs.class_singular_values):\n",
        "                continue\n",
        "            all_class_svs = graphs.class_singular_values[epoch_idx]\n",
        "\n",
        "            # Aggregate top-5 SVs across classes\n",
        "            top5_svs = np.array([sv[:5] for sv in all_class_svs if len(sv) >= 5])\n",
        "            mean_top5 = np.mean(top5_svs, axis=0)\n",
        "            std_top5 = np.std(top5_svs, axis=0)\n",
        "\n",
        "            plt.errorbar(\n",
        "                range(1, 6), mean_top5, yerr=std_top5,\n",
        "                label=f'Epoch {cur_epochs[epoch_idx]}', color=colors[i], alpha=0.7\n",
        "            )\n",
        "        plt.xlabel('Singular Value Index (Top 5)')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.yscale('log')\n",
        "        plt.title('Experiment 1: Class-Wise Feature Singular Values (Mean ± Std)')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "\n",
        "        ## Experiment 2 Figures\n",
        "        plt.figure(13)\n",
        "        # Filter out NaNs from early epochs\n",
        "        valid_global_95 = [\n",
        "            (e, r['0.95']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.95' in r and not np.isnan(r['0.95'])\n",
        "        ]\n",
        "        valid_global_99 = [\n",
        "            (e, r['0.99']) for e, r in zip(update_epochs, graphs.update_ranks)\n",
        "            if '0.99' in r and not np.isnan(r['0.99'])\n",
        "        ]\n",
        "        plt.plot([e for e, _ in valid_global_95], [v for _, v in valid_global_95], label='95% Threshold')\n",
        "        plt.plot([e for e, _ in valid_global_99], [v for _, v in valid_global_99], label='99% Threshold')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank of Parameter Updates')\n",
        "        plt.title('Experiment 2: Parameter Update Rank')\n",
        "\n",
        "      # SPLIT PLOT 14 INTO TWO SEPARATE PLOTS\n",
        "        plt.figure(14)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs for 95% threshold\n",
        "            valid_95 = [\n",
        "                (e, r['0.95']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.95' in r and not np.isnan(r['0.95'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_95], [v for _, v in valid_95], label=layer_name)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank (95% Threshold)')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks (95% Threshold)')\n",
        "\n",
        "        plt.figure(15)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs for 99% threshold\n",
        "            valid_99 = [\n",
        "                (e, r['0.99']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.99' in r and not np.isnan(r['0.99'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_99], [v for _, v in valid_99], label=layer_name)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank (99% Threshold)')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks (99% Threshold)')\n",
        "\n",
        "        # SPLIT PLOT 14 INTO TWO SEPARATE PLOTS\n",
        "        plt.figure(14)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs for 95% threshold\n",
        "            valid_95 = [\n",
        "                (e, r['0.95']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.95' in r and not np.isnan(r['0.95'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_95], [v for _, v in valid_95], label=f'{layer_name} (95%)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks (95% Threshold)')\n",
        "\n",
        "        plt.figure(15)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter NaNs for 99% threshold\n",
        "            valid_99 = [\n",
        "                (e, r['0.99']) for e, r in zip(update_epochs, graphs.layer_update_ranks[layer_name])\n",
        "                if '0.99' in r and not np.isnan(r['0.99'])\n",
        "            ]\n",
        "            plt.plot([e for e, _ in valid_99], [v for _, v in valid_99], label=f'{layer_name} (99%)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Rank')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Parameter Update Ranks (99% Threshold)')\n",
        "\n",
        "        plt.figure(16)\n",
        "        for layer_name in tracked_layers:\n",
        "            # Filter out None values and get only epochs in analysis_epochs\n",
        "            valid_svs = [\n",
        "                (e, sv) for e, sv in zip(update_epochs, graphs.layer_singular_values[layer_name])\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "            if valid_svs:\n",
        "                # If we have few valid epochs, show all of them\n",
        "                if len(valid_svs) <= 3:\n",
        "                    selected_indices = list(range(len(valid_svs)))\n",
        "                else:\n",
        "                    # Otherwise show first, middle, last\n",
        "                    selected_indices = [0, len(valid_svs)//2, -1]\n",
        "\n",
        "                for idx in selected_indices:\n",
        "                    e, sv = valid_svs[idx]\n",
        "                    plt.semilogy(\n",
        "                        range(1, len(sv)+1), sv,\n",
        "                        label=f'{layer_name} (Epoch {e})'\n",
        "                    )\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Magnitude')\n",
        "        plt.legend()\n",
        "        plt.title('Layer-Specific Update Singular Values')\n",
        "\n",
        "        plt.figure(17)\n",
        "        # For the cumulative variance plot, use the last available spectrum for each layer\n",
        "        for layer_name in tracked_layers:\n",
        "            # Get non-None values\n",
        "            valid_svs = [\n",
        "                sv for sv in graphs.layer_singular_values[layer_name]\n",
        "                if sv is not None and not np.isnan(sv).all()\n",
        "            ]\n",
        "            if valid_svs:\n",
        "                # Use the last available spectrum\n",
        "                sv = valid_svs[-1]\n",
        "                explained_variance = np.cumsum(sv**2) / np.sum(sv**2)\n",
        "                plt.plot(explained_variance, label=f'{layer_name}')\n",
        "        for thresh in RANK_THRESHOLDS:\n",
        "            plt.axhline(thresh, color='k' if thresh==0.99 else 'gray',\n",
        "                        linestyle='--', alpha=0.5)\n",
        "        plt.xlabel('Singular Value Index')\n",
        "        plt.ylabel('Cumulative Explained Variance')\n",
        "        plt.legend()\n",
        "        plt.title('Cumulative Variance Explained by Layer Updates')\n",
        "\n",
        "\n",
        "        # STANDARDIZED LAYER-WISE PLOTS\n",
        "        # Get the set of all layers from the first epoch\n",
        "        train_var_by_layer = graphs.train_variance_by_layer\n",
        "        layers = train_var_by_layer[0].keys()\n",
        "\n",
        "        # Initialize dictionary to collect values per layer\n",
        "        layer_to_values = {layer: [] for layer in layers}\n",
        "\n",
        "        # Fill the values over epochs\n",
        "        for epoch_data in train_var_by_layer:\n",
        "            for layer in layers:\n",
        "                layer_to_values[layer].append(epoch_data.get(layer, float('nan')))\n",
        "\n",
        "        # Plot Train Variance by Layer\n",
        "        plt.figure(18)\n",
        "        for layer, values in layer_to_values.items():\n",
        "            plt.plot(cur_epochs, values, label=layer)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Train Variance')\n",
        "        plt.legend()\n",
        "        plt.title('Train Variance per Layer over Epochs')\n",
        "\n",
        "        # NC1 by Layer\n",
        "        plt.figure(19)\n",
        "        for layer in layers:\n",
        "            values = [epoch_dict[layer] for epoch_dict in graphs.nc1_by_layer]\n",
        "            plt.plot(cur_epochs, values, label=layer)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('NC1')\n",
        "        plt.legend()\n",
        "        plt.title('NC1 over Epochs by Layer')\n",
        "\n",
        "        # NC2 by Layer\n",
        "        plt.figure(20)\n",
        "        for layer in layers:\n",
        "            values = [epoch_dict[layer] for epoch_dict in graphs.nc2_by_layer]\n",
        "            plt.plot(cur_epochs, values, label=layer)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('NC2')\n",
        "        plt.legend()\n",
        "        plt.title('NC2 over Epochs by Layer')\n",
        "\n",
        "        # Feature Rank by Layer (95% threshold)\n",
        "        plt.figure(21)\n",
        "        for layer in layers:\n",
        "            values_95 = [epoch_dict[layer]['0.95'] if layer in epoch_dict and '0.95' in epoch_dict[layer] else float('nan')\n",
        "                        for epoch_dict in graphs.feature_rank_by_layer]\n",
        "            plt.plot(cur_epochs, values_95, label=layer)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Feature Rank (95% Threshold)')\n",
        "        plt.legend()\n",
        "        plt.title('Feature Rank Evolution by Layer (95% Threshold)')\n",
        "\n",
        "        # Feature Rank by Layer (99% threshold)\n",
        "        plt.figure(22)\n",
        "        for layer in layers:\n",
        "            values_99 = [epoch_dict[layer]['0.99'] if layer in epoch_dict and '0.99' in epoch_dict[layer] else float('nan')\n",
        "                        for epoch_dict in graphs.feature_rank_by_layer]\n",
        "            plt.plot(cur_epochs, values_99, label=layer)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Effective Feature Rank (99% Threshold)')\n",
        "        plt.legend()\n",
        "        plt.title('Feature Rank Evolution by Layer (99% Threshold)')\n",
        "\n",
        "        # Layer-wise Feature Matrix Spectrum Evolution\n",
        "        selected_epochs = [0, len(cur_epochs)//2, -1]  # First, middle, last\n",
        "        colors = ['b', 'g', 'r']\n",
        "\n",
        "        # Get all layers from the first epoch\n",
        "        if len(graphs.singular_values_by_layer) > 0:\n",
        "            layers_sv = list(graphs.singular_values_by_layer[0].keys())\n",
        "\n",
        "            for layer_idx, layer in enumerate(layers_sv):\n",
        "                plt.figure(23 + layer_idx)  # Create separate figure for each layer\n",
        "\n",
        "                for i, epoch_idx in enumerate(selected_epochs):\n",
        "                    if epoch_idx < len(graphs.singular_values_by_layer):\n",
        "                        layer_data = graphs.singular_values_by_layer[epoch_idx]\n",
        "                        if layer in layer_data and layer_data[layer] is not None:\n",
        "                            sv = layer_data[layer]\n",
        "                            plt.semilogy(range(1, len(sv)+1), sv,\n",
        "                                       label=f'Epoch {cur_epochs[epoch_idx]}',\n",
        "                                       color=colors[i])\n",
        "\n",
        "                plt.xlabel('Singular Value Index')\n",
        "                plt.ylabel('Singular Value')\n",
        "                plt.legend()\n",
        "                plt.title(f'Feature Matrix Spectrum Evolution - {layer}')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # === Data Save ===\n",
        "    # def ensure_numpy(tensor_or_array):\n",
        "    #   \"\"\"Convert tensor to numpy array if it's a tensor, otherwise return as is.\"\"\"\n",
        "    #   if isinstance(tensor_or_array, torch.Tensor):\n",
        "    #       return tensor_or_array.detach().cpu().numpy()\n",
        "    #   return tensor_or_array\n",
        "\n",
        "    if epoch in [1, 2, 50, 100, 150, 200, 250, 300, epochs]:\n",
        "        # Data from analysis epochs\n",
        "        df = pd.DataFrame({\n",
        "            'epoch': cur_epochs,\n",
        "            'Sw_invSb': graphs.Sw_invSb,\n",
        "            'W_M_dist': graphs.W_M_dist,\n",
        "            'NCC_mismatch': graphs.NCC_mismatch,\n",
        "            'norm_M_CoV': graphs.norm_M_CoV,\n",
        "            'norm_W_CoV': graphs.norm_W_CoV,\n",
        "            'cos_M': graphs.cos_M,\n",
        "            'cos_W': graphs.cos_W,\n",
        "            # Experiment 1\n",
        "            'feature_rank_95': [x['0.95'] for x in graphs.feature_rank],\n",
        "            'feature_rank_99': [x['0.99'] for x in graphs.feature_rank],\n",
        "            # Experiment 2\n",
        "            'update_rank_95': [graphs.update_ranks[e-1]['0.95'] for e in cur_epochs],\n",
        "            'update_rank_99': [graphs.update_ranks[e-1]['0.99'] for e in cur_epochs],\n",
        "            'fc_update_rank_95': [graphs.layer_update_ranks['fc.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "            'fc_update_rank_99': [graphs.layer_update_ranks['fc.weight'][e-1]['0.99'] for e in cur_epochs],\n",
        "            'conv_update_rank_95': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.95'] for e in cur_epochs],\n",
        "            'conv_update_rank_99': [graphs.layer_update_ranks['layer4.1.conv2.weight'][e-1]['0.99'] for e in cur_epochs]\n",
        "        })\n",
        "        df.to_csv(data_path + 'experiment1_experiment2_data.csv', index=False)\n",
        "\n",
        "        # Save Experiment 1 singular values\n",
        "        np.save(data_path + 'feature_svs.npy', graphs.singular_values)\n",
        "        np.save(data_path + 'mean_svs.npy', graphs.mean_singular_values)\n",
        "        np.save(data_path + 'class_svs.npy', np.array(graphs.class_singular_values, dtype=object))\n",
        "\n",
        "        # Save Experiment 2 data for all epochs\n",
        "        df_exp2 = pd.DataFrame({\n",
        "            'epoch': update_epochs,\n",
        "            'update_rank_95': [x.get('0.95', np.nan) for x in graphs.update_ranks],\n",
        "            'update_rank_99': [x.get('0.99', np.nan) for x in graphs.update_ranks],\n",
        "            'fc_update_rank_95': [x.get('0.95', np.nan) for x in graphs.layer_update_ranks['fc.weight']],\n",
        "            'fc_update_rank_99': [x.get('0.99', np.nan) for x in graphs.layer_update_ranks['fc.weight']],\n",
        "            'conv_update_rank_95': [x.get('0.95', np.nan) for x in graphs.layer_update_ranks['layer4.1.conv2.weight']],\n",
        "            'conv_update_rank_99': [x.get('0.99', np.nan) for x in graphs.layer_update_ranks['layer4.1.conv2.weight']]\n",
        "        })\n",
        "        df_exp2.to_csv(data_path + 'experiment2_data.csv', index=False)\n",
        "\n",
        "        # Experiment 2 singular values (Note: saved as objects due to jagged arrays)\n",
        "        valid_update_spectra = [sv for sv in graphs.update_spectra if sv is not None]\n",
        "        np.save(data_path + 'global_update_spectra.npy', np.array(valid_update_spectra, dtype=object))\n",
        "\n",
        "        for layer_name in tracked_layers:\n",
        "            valid_layer_svs = [sv for sv in graphs.layer_singular_values[layer_name] if sv is not None]\n",
        "            np.save(data_path + f'{layer_name}_singular_values.npy', np.array(valid_layer_svs, dtype=object))\n",
        "\n",
        "        # # Save figures\n",
        "        # figures_to_save = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]  # All\n",
        "        # !mkdir -p ./temp_figures # Create a local directory\n",
        "        # import shutil\n",
        "        # for fig_num in figures_to_save:\n",
        "        #     fig = plt.figure(fig_num)\n",
        "        #     # Save to local storage first\n",
        "        #     local_path = f'./temp_figures/figure_{fig_num}_epoch_{epoch}.pdf'\n",
        "        #     fig.savefig(local_path, bbox_inches='tight', dpi=300)\n",
        "        #     plt.close(fig_num)\n",
        "        #     # Then copy to Google\n",
        "        #     drive_path = os.path.join(data_path, f'figure_{fig_num}_epoch_{epoch}.pdf')\n",
        "        #     shutil.copy2(local_path, drive_path)\n",
        "\n",
        "        print(f\"Experiment data saved at epoch {epoch}\")\n",
        "\n",
        "        # memory usage check\n",
        "        process = psutil.Process(os.getpid())\n",
        "        memory_info = process.memory_info()\n",
        "        print(f\"Epoch {epoch}: Memory usage: {memory_info.rss / (1024 * 1024):.2f} MB\")\n",
        "        print(f\"Tracker deltas: {len(tracker.global_deltas)} updates stored\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "            print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "        # save checkpoint\n",
        "        # Comment out graphs, tracker to reduce checkpoint size (~2.5GB -> 85 MB). Experiment will no longer be resumable from checkpoint.\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'lr_scheduler': lr_scheduler.state_dict(),\n",
        "            'graphs': graphs,\n",
        "            'tracker': tracker,\n",
        "            'cur_epochs': cur_epochs\n",
        "        }\n",
        "        try: # try Drive first\n",
        "            torch.save(checkpoint, data_path + 'checkpoint' + str(epoch) + '.pth')\n",
        "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to data_path: {e}\")\n",
        "            # Fall back to local storage\n",
        "            torch.save(checkpoint, '/content/checkpoint_local_copy.pth')\n",
        "            print(\"Checkpoint saved locally to Colab.\")\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}